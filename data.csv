,filename,title,incident_start_time,incident_detected_time,incident_end_time,severity_level,summary,incident_duration,time_to_detect,time_to_resolve,services,service_impact_type,components,component_root_cause,impact,user_impact_type,root_cause,category_root_cause,symptom,symptom_type
0,2022-05-05_Wikimedia_full_site_outage.wikitext,Outage Impacting All Wikis for Logged-In Users,2022-05-05 05:36:00,2022-05-05 05:39:18,2022-05-05 05:55:00,Sev(3),"A schema change affected mariadb's optimizer, causing a very frequent query to take significantly longer and resulting in an outage for all wikis for 20 minutes.",00:19:00,00:03:18,00:15:42,"Wikis,Wikis",MediaWiki Core Services,"mariadb optimizer,globalblocks table,centralauth database (s7)",Databases,"For 20 minutes, all wikis were unreachable for logged-in users and non-cached pages.",Complete Service Outage,"A schema change made mariadb's optimizer change its query plan, causing a frequent query to globalblocks table on the centralauth database to take 5 seconds instead of less than a second.","Database Issues, Configuration Errors",The issue was first detected by alerts on IRC and pages indicating service appservers failed probes and high POST latency on appservers.,"Appserver Issues, Service Probes and Health Checks Failures, Latency and Timeout Issues"
1,2022-11-22_wdqs_outage.wikitext,"Wikidata Query Service Outage - November 22, 2022",2022-11-22 15:15:00,2022-11-22 15:15:00,2022-11-22 15:34:00,Sev(2),"For at least 15 minutes, users of the Wikidata Query Service either could not connect or received extremely slow responses.",00:19:00,00:00:00,00:19:00,Wikidata Query Service,"Wikidata Query Service, Database and Data Analytics","public wdqs hosts,eqiad datacenter,pybal,public wdqs hosts",Other,"For at least 15 minutes, users of the wikidata query service experienced a lack of service and/or extremely slow responses.","Complete Service Outage, Elevated Response Times and Latencies",One bad query caused this outage. A single user or query should not be able to take out an entire datacenter.,"Database Issues, Configuration Errors","The issue was first detected by a PyBal alert for all public WDQS hosts in the Eqiad datacenter, indicating a health check failure on several servers.",Service Probes and Health Checks Failures
2,2021-02-05_wmflabs_certs_expired.wikitext,Certificates Expiration for Cloud VPS Web Proxies,,,,Sev(2),"Certificates for hostnames of Cloud VPS web proxies were allowed to expire, affecting access to certain services.",00:00:00,00:00:00,00:00:00,Cloud VPS,Cloud Services and Virtual Private Servers (VPS),"web proxies,hostname certificates","Web Proxies, Other","Users were unable to access services hosted on Cloud VPS web proxies, such as https://puppet-compiler.wmflabs.org.",Complete Service Outage,Failures in the mechanism or processes responsible for renewing SSL certificates for Cloud VPS web proxies.,TLS/SSL Issues,"The issue was first detected when users reported being unable to access specific services, and validation showed expired SSL certificates.","SSL Certificate Expirations, User Reported Problems"
3,2022-07-11_Shellbox_and_parsoid_saturation.wikitext,Mobileapps HTTP 503 Errors Incident,2022-07-11 13:14:00,2022-07-11 13:14:00,2022-07-11 14:26:00,Sev(3),"On July 11, 2022, the mobileapps service experienced HTTP 503 errors for 13 minutes due to background parsing associated with VisualEditor.",01:12:00,00:00:00,01:12:00,"mobileapps,VisualEditor,VisualEditor,shellbox",API and Application Servers,"MWExtensionDialog,Lilypond,Shellbox,grafana,grafana,grafana","Other, Other, Other, Load Balancers, Monitoring & Telemetry, Monitoring & Telemetry","For 13 minutes, mobileapps service was serving HTTP 503 errors to clients.","Partial Service Outage, Increased Error Rates, API Failures","Background parsing using VisualEditor's MWExtensionDialog with an insufficient debounce interval caused heavy shellouts to Lilypond through Shellbox, overwhelming the service.","Software Bugs, Capacity Overloads","The issue was first detected at 13:14 with a ProbeDown alert indicating Service shellbox:4008 had failed probes, leading to HTTP 503 errors for clients.",Service Probes and Health Checks Failures
4,2022-05-21_varnish_cache_busting.wikitext,Incident Involving CDN Service Disruption,2022-05-21 19:01:00,2022-05-21 19:01:00,2022-05-21 19:03:00,Sev(3),"A flood of API traffic from an AWS instance caused caching servers to be overloaded, resulting in a 2-minute outage for all wikis and services served by our CDN.",00:02:00,00:00:00,00:02:00,"services served by our CDN,services served by our CDN",Other,caching servers,Caching Servers,"For 2 minutes, all wikis and services served by our CDN were unavailable to all users.",Complete Service Outage,A flood of API traffic from an AWS instance caused caching servers to be overloaded.,"Unexpected Traffic Surges, Capacity Overloads","Services behind our caching layer were up, but not reachable during this time.","Cache Layer Anomalies, Network Connectivity Problems"
5,2020-12-02_netflow-hive-migration.wikitext,Netflow Data Deletion Incident,2020-12-02 18:00:00,2020-12-03 18:00:00,2020-12-03 18:30:00,Sev(3),"On Dec 2nd 2020, the Analytics team migrated the Hive's netflow data set from the `wmf` database to the `event` database, but missed whitelisting it in the data purging job, leading to the deletion of data older than 90 days.",24:30:00,24:00:00,00:30:00,"Hive,Druid",Database and Data Analytics,"netflow data set,data retention,data retention",Other,"About 1 year and 4 months of valuable netflow data from Hive was deleted. Most of it still exists in Druid, but with some missing fields.",Data Corruption or Loss,"The purging job configuration did not whitelist the newly migrated netflow data, leading to its deletion based on the default retention policy of 90 days.",Configuration Errors,"Missing data was first noticed by a developer on Dec 3rd, 2020 at 18:00 UTC while working on a related task, who then notified the Analytics team.",User Reported Problems
6,2023-02-11_logstash_latency.wikitext,Logstash Message Congestion Incident - February 2023,2023-02-11 10:44:00,2023-02-11 10:45:55,2023-02-11 12:56:00,Sev(3),"eqiad's Logstash experienced message congestion that exhausted the latency budget, consuming 160% of the quarterly budget for delayed messages.",02:12:00,00:01:55,02:10:05,"Logstash,Kafka","Monitoring and Logging, Database and Data Analytics","Uncategorized,Uncategorized,Uncategorized","Web Servers & Application Layers, Monitoring & Telemetry","Logstash messages were delayed for 2h14m. At peak (10:58), up to 45% of messages were delayed.",Elevated Response Times and Latencies,"A flood of exceptions logged from Parsoid created congestion on the Kafka queue, which Logstash could not consume quickly enough.","Unexpected Traffic Surges, Capacity Overloads",The issue was detected by an IRC alert at 10:45: 'Too many messages in kafka logging'.,"Service Probes and Health Checks Failures, High CPU & Resource Utilization"
7,2023-01-10_eqsin_network_outage.wikitext,Incident affecting users in Asia,2023-01-10 16:00:00,2023-01-10 16:37:00,2023-01-10 20:57:00,Sev(2),Users in Asia experienced limited access to Wikipedia pages due to an outage at the eqsin data center caused by overlapping maintenance and a long-term fiber cut.,04:57:00,00:37:00,04:20:00,Wikipedia,Other,"cr3-eqsin,cr3-eqsin,cr3-eqsin,doh5002,doh5002,prometheus5001,ncredir5001,doh5002,doh5002,doh5002,text-lb.eqsin.wikimedia.org,cr3-eqsin IPv6,cr3-eqsin IPv6,cr3-eqsin IPv6,cr3-eqsin IPv6,upload-lb.eqsin.wikimedia.org","Networking Equipment, Web Proxies, Monitoring & Telemetry, Load Balancers","Users in Asia were affected for approximately 11 to 41 minutes, being able to read only cached Wikipedia pages in eqsin.","Network Connectivity Problems, Cache Issues",A long fiber cut on one transport link and scheduled maintenance on the other caused a complete outage at the eqsin data center.,Network Failures,"Automated monitoring detected the issue. Alerts included BGP status critical on multiple eqsin hosts and 100% packet loss on several key systems starting from 16:37 UTC on January 10, 2022.",Network Connectivity Problems
8,2022-12-21_shellbox-syntaxhighlight.wikitext,Shellbox Syntaxhighlight Overload Incident,2022-12-21 09:00:00,2022-12-21 09:00:00,2022-12-21 09:10:00,Sev(2),"A sudden spike in requests to shellbox-syntaxhighlight overloaded the service, leading to slow response times and increased failures.",00:10:00,00:00:00,00:10:00,"syntaxhighlighting,CirrusSearch",Other,"shellbox-syntaxhighlight,jobrunners,api_appservers,CirrusSearch job topic eqiad.mediawiki.job.cirrusSearchLinksUpdate",Other,"For about 10 minutes, syntax highlighting was slow or returning errors.","Elevated Response Times and Latencies, Increased Error Rates","The majority of requests were originating from jobrunners and api_appservers, causing the overloaded shellbox-syntaxhighlight service.","Unexpected Traffic Surges, Capacity Overloads","Initially detected by human spotting alerts on IRC, closely followed by a page. Alerts included failing http-api probes and api_appservers running out of idle workers.","Appserver Issues, Service Probes and Health Checks Failures"
9,2022-04-26_cr2-eqord_down.wikitext,Telia Link Fiber Cut Impact on Eqord Connectivity,2023-04-26 05:13:00,2023-04-26 05:00:00,2023-04-26 07:27:00,Sev(2),"On the morning of April 26, a maintenance activity by Telia took down all remaining transports from Eqord, making the Eqord networking equipment completely unreachable.",02:14:00,23:47:00,02:27:00,"Eqord,Codfw,Ulsfo",Other,"Telia Link,Eqord Networking Equipment",Other,There was no end-user impact since Eqord is a network-only location with end-user traffic for Codfw and Ulsfo naturally routed directly instead of via Eqord.,Other,"A Telia maintenance activity, coupled with a pre-existing fiber cut, led to the complete unavailability of Eqord's networking equipment.","Network Failures, External Service Failures","The issue was first detected when ping commands to 208.115.136.238 failed with 100% packet loss, and Telia circuit failure logs indicated Ethernet MAC Remote and Local Fault Delta Events.",Network Connectivity Problems
10,2021-04-06_partial_rack_codfw.wikitext,Partial Service Outage Due to Faulty Network Switch in Codfw Cluster,2021-04-06 17:00:00,2021-04-06 17:01:22,2021-04-06 18:18:15,Sev(3),"Around 17:00 UTC, a faulty network switch caused partial failure of one rack in the Codfw cluster, leading to reduced redundancy or capacity for some services. The hosts were unreachable for about 1 hour.",01:18:15,00:01:22,01:16:53,"Elastic search,Swift media storage,Edge cache,DNS","Search and Indexing, Media Storage and File Handling, Content Delivery Network (CDN) and Edge Cache, Other","nodes,nodes,nodes,nodes","Databases, Web Servers & Application Layers, Caching Servers, Networking Equipment","Elastic search: reduced redundancy; Swift media storage: no user impact; Edge cache: one node depooled, minimal user impact; DNS: one node unavailable, minimal user impact.","Partial Service Outage, Cache Issues, Network Connectivity Problems",A faulty network switch caused partial failure of one rack in the Codfw cluster.,"Network Failures, Hardware Failures",The issue was first detected by alerts indicating that multiple hosts were down with 100% packet loss. initial alert: [17:01:22] <icinga-wm> PROBLEM - Host cp2036 is DOWN: PING CRITICAL - Packet loss = 100%,Network Connectivity Problems
11,2022-03-27_api.wikitext,Incident with MediaWiki API requests and PHP-FPM workers,2022-03-27 14:36:00,2022-03-27 14:36:00,2022-03-28 12:39:00,Sev(2),"A template change in itwiki triggered transclusion updates to many pages, leading to thousands of requests to the API cluster and resulting in higher levels of failed or slow MediaWiki API requests for about 4 hours over two days.",22:03:00,00:00:00,22:03:00,"Media Services,App Services,Change Management,API Services","API and Application Servers, Caching and Proxy Services","PHP-FPM workers,API cluster,Transclusion updates","Web Servers & Application Layers, Cluster Management","For about 4 hours, in three segments of 1-2 hours each over two days, there were higher levels of failed or slow MediaWiki API requests.","Elevated Response Times and Latencies, Increased Error Rates, API Failures","A template change on itwiki caused Changeprop to issue numerous requests to the API cluster to reparse transcluding pages, overwhelming the system.","Configuration Errors, Capacity Overloads","The issue was first detected through multiple Icinga alerts indicating 'Not enough idle PHP-FPM workers for Mediawiki api_appserver at eqiad', with related errors such as critical thresholds being breached.","Appserver Issues, High CPU & Resource Utilization"
12,2022-03-10_MediaWiki_availability.wikitext,MediaWiki availability on all wikis for logged-in users / uncached content,2022-03-10 08:26:11,2022-03-10 08:24:00,2022-03-10 08:46:03,Sev(2),MediaWiki was unavailable on all wikis for logged-in users and for anonymous users trying to edit or access uncached content for 12 minutes.,00:19:52,23:57:49,00:22:03,MediaWiki,MediaWiki Core Services,"db1099,s8 section,API Gateway,API Gateway","Databases, Web Servers & Application Layers, Web Proxies","All wikis were unreachable for logged-in users and for anonymous users trying to edit or access uncached content. The issue occurred in two spikes: from 08:24 to 08:30 UTC, and from 08:39 to 08:45 UTC approximately.",Complete Service Outage,"A replica database in the s8 section (Wikidata), db1099, was rebooted for maintenance and reintroduced into production slowly while a file transfer was occurring. This additional load caused the host to become slow to respond to queries, leading to a cascade effect on other databases in the same section, ultimately causing the exhaustion of workers at the application layer.",Capacity Overloads,"The issue was first detected as slow or unavailable access to uncached render pages and read-write actions, impacting the application servers (MediaWiki) as shown on the RED dashboard.","Latency and Timeout Issues, Appserver Issues"
13,2020-06-25_caching-sessions.wikitext,Incident Report - Wikimedia,2020-06-01 00:00:00,2020-06-01 00:10:00,2020-06-01 01:00:00,Sev(2),Incident involved issues with Varnish configuration affecting Wikimedia services.,01:00:00,00:10:00,00:50:00,,Other,,Other,Users experienced unexpected behavior and potential data inconsistencies due to improper caching settings.,"Cache Issues, Data Corruption or Loss",Misconfiguration in the Varnish caching backend related to the 'beresp.ttl=0' setting.,Configuration Errors,"The issue was first detected through user reports documented in Phabricator tasks T257522, T256395, and T255156.",User Reported Problems
14,2021-09-06_Wikifeeds.wikitext,MediaWiki API and Wikifeeds Service Outage,2021-09-04 02:40:00,2021-09-06 17:32:00,2021-09-07 07:13:00,Sev(3),"A MediaWiki API outage on 2021-09-04 led to increased HTTP 503 errors from the Wikifeeds service. The issue arose due to a significant spike in end-user traffic, which degraded the service's performance.",76:33:00,62:52:00,13:41:00,"MediaWiki API,MediaWiki API",API and Application Servers,"Kubernetes pods,tls-proxy container,Restbase","Containers & Kubernetes, Web Proxies, Caching Servers","For 3 days, the Wikifeeds API failed about 1% of its requests (approximately 5 out of every 500 per second).",API Failures,"A significant increase in end-user traffic to the Wikifeeds service, exacerbated by an insufficient capacity to handle the load, led to the service returning numerous HTTP 503 and 504 errors.","Capacity Overloads, Unexpected Traffic Surges","The issue was first detected by a service-checker icinga alert indicating HTTP 504 errors for the Wikifeeds service, which triggered intermittently over the weekend and was noticed by an SRE on the following Monday.","Latency and Timeout Issues, Service Probes and Health Checks Failures"
15,2021-06-30_Ulsfo_traffic_surge.wikitext,Traffic Surge Impacted Network Port Capacity,2023-10-03 22:00:00,2023-10-03 22:00:00,2023-10-03 23:00:00,Sev(3),"A traffic surge caused the Ulsfo cluster's network port to reach its capacity, leading to a mitigated issue by 23:00 UTC.",01:00:00,00:00:00,01:00:00,San Francisco DC,Other,"Ulsfo cluster,Wikimedia DNS,Wikimedia DNS",Networking Equipment,"For up to an hour, a portion of requests may have received a slower response from regions served by the San Francisco DC, including New Zealand and parts of North America.",Elevated Response Times and Latencies,A traffic surge started to fill up the capacity of a network port in the Ulsfo cluster.,"Unexpected Traffic Surges, Capacity Overloads",The issue was first detected as a traffic surge affecting the network port capacity in the Ulsfo cluster after 22:00 UTC.,Network Connectivity Problems
16,2021-04-15_appserver_latency.wikitext,Increased Latency and Error Rates for MediaWiki Cache Misses,2023-10-05 07:31:00,2023-10-05 07:31:00,2023-10-05 08:16:00,Sev(2),"From 07:31 to 08:16 UTC, there was increased latency and error rates for some MediaWiki cache misses and authenticated requests, affecting API users and bots on commons.wikimedia.org.",00:45:00,00:00:00,00:45:00,"MediaWiki,API","MediaWiki Core Services, API and Application Servers","cache,databases,API webserver","Caching Servers, Databases, Web Servers & Application Layers","API request latency for some requests went up by 5 seconds, and error rates increased up to 25%. Regular users and other wikis were also affected later through cross-wiki features involving Commons.","Elevated Response Times and Latencies, Increased Error Rates",The issue was caused by a bot increasing load on the commonswiki databases and consuming API webserver resources.,"Unexpected Traffic Surges, Capacity Overloads",The issue was first detected through increased latency and error rates for MediaWiki cache misses and authenticated requests.,"Latency and Timeout Issues, Cache Layer Anomalies"
17,2019-09-10_toolforge-kubernetes.wikitext,Kubernetes API Server Failure Due to CA Certificate Mismatch,2023-05-01 18:30:00,2023-05-01 18:45:00,2023-05-02 01:43:00,Sev(2),"Kubernetes API server failed to start due to a CA certificate mismatch with the etcd servers, leading to the collapse of many Kubernetes managed services.",07:13:00,00:15:00,06:58:00,Toolforge Kubernetes API,Cloud Services and Virtual Private Servers (VPS),"Kubernetes API server,puppet,CA certificates,etcd servers,Kubernetes API server","Cluster Management, Code Deployment & Version Control, Other, Databases, Web Proxies","Toolforge users could not launch or manage tools using the Kubernetes backend. Kube2proxy failures took all Kubernetes web services offline, though individual containers remained unaffected.","Partial Service Outage, API Failures","A CA certificate mismatch between the Kubernetes API server and the etcd servers, precipitated by puppet changes and further complicated by a service restart.",Configuration Errors,The issue was first detected through an alert from icinga on the check against http://checker.tools.wmflabs.org/k8s/nodes/ready indicating that all k8s nodes were unhealthy.,"Service Probes and Health Checks Failures, Network Connectivity Problems"
18,2024-04-17_mw-on-k8s_eqiad_outage.wikitext,CoreDNS Overload Incident During MediaWiki Deployment,2024-04-17 09:18:00,2024-04-17 09:10:00,2024-04-17 09:44:00,Sev(2),"The deployment of the mw-mcrouter service caused a significant increase in DNS requests to CoreDNS, overwhelming the pods and leading to an outage for MediaWiki pods.",00:26:00,23:52:00,00:34:00,"mediawiki,coredns,HAProxy","MediaWiki Core Services, Caching and Proxy Services, Other","mw-on-k8s,mw-on-k8s,mw-on-k8s,mw-on-k8s,scap,logstash","Containers & Kubernetes, Web Proxies, Web Servers & Application Layers, Web Servers & Application Layers, Code Deployment & Version Control, Monitoring & Telemetry","Non 5XX requests (text+upload) dropped from 138K rps to 120K-130K rps for 20 minutes, causing MediaWiki pods to never become ready and rolling back the scap deployment.","Partial Service Outage, Increased Error Rates","The change increased the number of DNS requests towards CoreDNS from 40k req/s to 110k req/s, overwhelming the CoreDNS pods and causing them to crash continuously due to insufficient memory.",Capacity Overloads,"The issue was first detected by antoine who observed a higher level of MediaWiki related events in Logstash, followed by alerts for PHP-FPM workers and high latency.","Appserver Issues, Latency and Timeout Issues, High CPU & Resource Utilization"
19,2022-09-15_sessionstore_quorum_issues.wikitext,Kask Split Brain Issue with SessionStore Cassandra Node,2022-09-15 12:25:00,2022-09-15 12:31:00,2022-09-15 12:38:00,Sev(2),"During routine maintenance on a sessionstore Cassandra node, the Kask service developed a split brain issue, leading to disruptions in session-related functions on the wikis.",00:13:00,00:06:00,00:07:00,"central login,central login",MediaWiki Core Services,"SessionStore Cassandra node,Kask service",Databases,Roughly 125 login errors per minute and 215 session loss errors.,Increased Error Rates,"An outstanding bug in gocql caused Kask to have an inconsistent view of cluster health, leading to quorum failures.",Software Bugs,"The issue was first human-detected and announced by Tamzin in #wikimedia-operations, followed by critical alerts from icinga-wm indicating sessionstore and MediaWiki errors.","Database Errors, Service Probes and Health Checks Failures, User Reported Problems"
20,2020-02-04_app_server_latency.wikitext,WANObjectCache Degradation and Increased CPU Load Impact on Babel,2020-02-04 16:01:00,2020-02-04 16:03:31,2020-02-04 16:12:00,Sev(4),"A drop in WANObjectCache hit ratio for Babel keys caused increased cache misses, leading to high CPU load on app servers and delayed other traffic from 16:03 to 16:12 UTC on 2020-02-04.",00:11:00,00:02:31,00:08:29,"MediaWiki,WANObjectCache,Babel,Kartotherian,Varnish","MediaWiki Core Services, Caching and Proxy Services, Other, Other, Caching and Proxy Services","memcached,API server,API server","Caching Servers, Web Servers & Application Layers","About 12 million varnish-text requests were dropped or delayed in a ten-minute period, spanning all five caching data centers; this represents about 20% of traffic in that window.","Partial Service Outage, Elevated Response Times and Latencies, Cache Issues","Increased cache misses led to more API requests over HTTPS, causing high CPU load on app servers and delaying other traffic.","Capacity Overloads, Unexpected Traffic Surges","Detected by Icinga CRITICAL alert for 'phpfpm_up reduced availability' at 16:03 UTC, followed by a series of paging alerts related to socket timeouts.","Latency and Timeout Issues, Service Probes and Health Checks Failures"
21,2022-02-01_ulsfo_network.wikitext,Connectivity Loss Due to Firewall Change at ulsfo POP,2022-01-31 16:05:00,2022-01-31 16:06:00,2022-01-31 16:08:00,Sev(2),A firewall change caused the ulsfo POP to lose connectivity to other POPs and core sites for 3 minutes.,00:03:00,00:01:00,00:02:00,"contribution service,page display service",Other,"cr3-ulsfo,cr3-ulsfo,cr3-ulsfo,OSPF sessions,BGP peers",Networking Equipment,"For 3 minutes, clients served by the ulsfo POP were not able to contribute or display un-cached pages, resulting in 8000 errors per minute (HTTP 5xx).","Partial Service Outage, Increased Error Rates, Network Connectivity Problems","The firewall change incorrectly restricted BFD to BGP peers, causing OSPF sessions to be torn down.",Configuration Errors,"Icinga alerted about connectivity issues to ulsfo at 16:06, indicating the outage.",Network Connectivity Problems
22,2022-08-10_cassandra_disk_space.wikitext,Cassandra Hosts Disk Space Incident,2022-08-10 12:55:00,2022-08-10 12:55:00,2022-08-10 18:22:00,Sev(2),A planned administrative shutdown of some redundant Cassandra hosts led to other hosts running out of disk space due to accumulating log files. The issue was resolved by bringing the hosts back online ahead of schedule.,05:27:00,00:00:00,05:27:00,Cassandra,Database and Data Analytics,"Hinted-handoff,Logs,Auxiliary data volume",Other,"During planned downtime, other hosts ran out of space due to accumulating logs. No external impact.",Other,"The Hinted-handoff writes resulted in unexpectedly high utilization of storage volumes on hosts located in the eqiad datacenter, due to limited disk space allocated for handling these logs.","Capacity Overloads, Configuration Errors",Free space became critically low on a volume housing auxiliary data on many Cassandra hosts. This was detected during the planned power maintenance event in the Codfw datacenter.,Other
23,2021-04-03_mailman2_attachments.wikitext,Incident Report: Exposure of Mailing List Attachments,2021-04-03 15:18:00,2021-04-04 11:56:00,2021-04-13 17:00:00,Sev(3),A vulnerability in Mailman2 allowed for public access to attachments from non-archived mailing lists due to lack of proper web authentication and flawed archiving behavior. This issue was mitigated by purging old attachments and migrating to Mailman3.,241:42:00,20:38:00,221:04:00,"Mailman2,Mailman2",Other,"Backup Configuration,Backup Configuration,Backup Configuration,Web Authentication",Other,Any person who sent an email to a list that was not supposed to be archived could have had their message contents publicly accessible and leaked.,Security Incidents,Mailman2 lacks proper web authentication and retained attachments of non-archived lists due to its design to support digest functionality.,"Security Misconfigurations, Software Bugs",The issue was discovered by Amir while preparing for the Mailman3 migration and was brought to attention due to the presence of public-facing URLs for these attachments.,Security Vulnerabilities
24,2022-12-09_api_appserver_worker_starvation.wikitext,Increased mediawiki logging led to eventgate-analytics congestion,2022-12-09 15:04:13,2022-12-09 15:04:13,2022-12-09 16:36:17,Sev(2),"Increased MediaWiki logging led to eventgate-analytics congestion, starving the api_appservers of idle workers.",01:32:04,00:00:00,01:32:04,"MediaWiki,eventgate,api_appserver,eventgate","MediaWiki Core Services, Database and Data Analytics, API and Application Servers, Database and Data Analytics","PHP-FPM workers,PHP-FPM workers,logstash,envoy telemetry,helmfile deployment,helmfile deployment","Monitoring & Telemetry, Web Servers & Application Layers, Monitoring & Telemetry, Monitoring & Telemetry, Containers & Kubernetes, Code Deployment & Version Control",No user-facing impact.,Other,"Increased MediaWiki logging caused eventgate-analytics congestion, which resulted in the starvation of idle PHP-FPM workers in api_appservers.",Capacity Overloads,Detected through monitoring of api_appserver idle starvation with alerts from icinga-wm indicating that some MediaWiki servers were running out of idle PHP-FPM workers.,"Appserver Issues, High CPU & Resource Utilization"
25,2023-01-09_API_errors_-_db1143.wikitext,Elevated 5xx and API errors on appservers,2023-01-09 21:04:00,2023-01-09 21:02:00,2023-01-09 21:14:00,Sev(2),"An incident occurred on January 9, 2023, causing elevated 5xx errors and API issues on Wikidata for approximately 10 minutes.",00:10:00,23:58:00,00:12:00,API appservers,API and Application Servers,"API errors,API errors",Web Servers & Application Layers,Users experienced elevated 5xx errors and were unable to get suggestions on Wikidata.,"Partial Service Outage, Increased Error Rates","A database server in cluster S4 was overloaded due to a large number of queries, which led to elevated 5xx errors and API issues.",Capacity Overloads,"The issue was first detected through alerts on IRC and VictorOps, notifying of 'High average GET latency on API appservers' and other related alerts.",Latency and Timeout Issues
26,2019-12-10_updateCategoryCounts.wikitext,Common Wiki Database Replicas Lag Incident,2019-12-10 22:26:00,2019-12-10 22:32:36,2019-12-10 22:39:00,Sev(2),"On 2019-12-10, mainly between 22:26 and 22:39 UTC, Common wiki database replicas lagged behind, causing slowdown, returning stale results, errors, and incorrect category counts.",00:13:00,00:06:36,00:06:24,"Commons,testwiki,group 0,group 0,group 0",MediaWiki Core Services,"General:Services,Category Services,Special Services","Databases, Web Servers & Application Layers",Users experienced issues on Commons with articles being removed from Category:11 for a few minutes. Other wikis encountered incorrect category counts during specified periods.,Data Corruption or Loss,"An incorrect SQL query missing quotes around the number 11 led to a slow query that did not use an index, updating all categories starting with 11 instead of just one.","Database Issues, Configuration Errors","The issue was first detected on IRC, where a user reported DB lock errors for Special:UploadWizard on Commons. There is an existing alert for lag, but it only triggers after 300 seconds for 10 checks.","Database Errors, User Reported Problems"
27,2022-09-09_Elastic_Autocomplete_Missing.wikitext,Wikipedia Autocomplete Search Results Issue,2022-09-09 00:18:00,2022-09-09 01:00:00,2022-09-09 08:10:00,Sev(2),"For 4-8 hours, Wikipedia autocomplete search results were often weird and unhelpful due to changes related to the Elasticsearch 7 upgrade.",07:52:00,00:42:00,07:10:00,Wikipedia,Other,"Search Platform,Search Platform,Search Platform",Other,Users experienced weird and unhelpful search results when using Wikipedia's autocomplete feature.,"Partial Service Outage, Cache Issues","An upgrade from Elasticsearch 6 to Elasticsearch 7 caused the autocomplete caches to fail, which wasn't detected due to insufficient monitoring.","Software Bugs, Configuration Errors","The issue was first detected when Wikipedia users reported 'weird and unhelpful search results,' specifically with regards to autocomplete, on the Wikipedia:Village pump (technical) page.","User Reported Problems, Cache Layer Anomalies"
28,2021-01-16_appserver_latency.wikitext,Incident Report - T272215,2021-01-23 18:22:00,2021-01-23 18:34:00,2021-01-23 19:45:00,Sev(2),Restricted details available at the specified link.,01:23:00,00:12:00,01:11:00,,Other,,Other,Details restricted. See https://phabricator.wikimedia.org/T272215,Other,Details restricted. See https://phabricator.wikimedia.org/T272215,Other,Details restricted. See https://phabricator.wikimedia.org/T272215,Other
29,2021-09-01_partial_parsoid_outage.wikitext,Parse2007 Server Fatal Error Incident,2021-09-01 02:05:00,2021-09-01 03:54:00,2021-09-01 12:20:00,Sev(3),"At 02:05 UTC, the parse2007 server in Codfw started to respond with fatal errors, likely due to a php-opcache corruption. The server was restarted at 11:20 UTC, which resolved the issue.",10:15:00,01:49:00,08:26:00,"MediaWiki,MediaWiki","MediaWiki Core Services, API and Application Servers","parse2007 server,php-fpm,php-fpm",Web Servers & Application Layers,"For 9 hours, 10% of submissions to Parsoid to parse or save wiki pages were failing on all wikis.","Partial Service Outage, Increased Error Rates, API Failures","Likely php-opcache corruption, given that the source code in question had not changed recently and is not known to be defined or referenced in an unusual manner.",Software Bugs,"Detected through logs indicating 'parse2007: Cannot declare class XWikimediaDebug in XWikimediaDebug.php', affecting about 10% of POST requests. Additionally, detected by human reporting an error.","User Reported Problems, Appserver Issues"
30,2022-12-12_wdqs_codfw_brief_outage.wikitext,WDQS Service Interruption Due to Thread Pool Exhaustion,2022-12-12 20:14:17,2022-12-12 20:13:18,2022-12-12 20:30:00,Sev(2),"A large influx in requests led to excessive thread pool usage from codfw blazegraph backends, which triggered a known bug causing deadlock for about 15 minutes.",00:15:43,23:59:01,00:16:42,"wdqs-ssl:443,PyBal,wdqs-ssl:443","Wikidata Query Service, Caching and Proxy Services","lvs2009,wdqs2001.codfw.wmnet,wdqs2001.codfw.wmnet,wdqs2001.codfw.wmnet,wdqs2001.codfw.wmnet","Load Balancers, Databases","For about 15 minutes, wdqs queries routed to codfw failed.","Partial Service Outage, Network Connectivity Problems",Excessive thread pool usage from codfw blazegraph backends triggering a known bug in Blazegraph causing improper thread management and deadlock.,Software Bugs,"The issue was first detected by pybal monitoring of WDQS, with alerts such as 'Service wdqs-ssl:443 has failed probes' and 'PROBLEM - PyBal backends health check on lvs2009 is CRITICAL: PYBAL CRITICAL - CRITICAL - wdqs-heavy-queries_8888'.",Service Probes and Health Checks Failures
31,2022-05-25_de.wikipedia.org.wikitext,Increased Load on DB Server Causing 503 Responses,2023-02-20 20:04:00,2023-02-20 20:04:00,2023-02-20 20:14:00,Sev(2),"An increase in POST requests to de.wikipedia.org caused increased load on one of the database servers, resulting in slower responses or errors for some logged-in users and non-cached pages for 6 minutes.",00:10:00,00:00:00,00:10:00,"text-https,text-https,text-https",Content Delivery Network (CDN) and Edge Cache,"DB servers,php-fpm workers,appservers","Databases, Web Servers & Application Layers, Web Servers & Application Layers","For 6 minutes, a portion of logged-in users and non-cached pages experienced slower response times or errors due to the increased load on one of the databases.","Partial Service Outage, Elevated Response Times and Latencies, Increased Error Rates, Storage and Database Failures",An increase in POST requests to de.wikipedia.org caused a spike in database queries to s5 that overloaded php-fpm workers.,"Unexpected Traffic Surges, Capacity Overloads",The issue was detected by automated monitoring alerts including 'Service text-https:443 has failed probes' and 'FrontendUnavailable: HAProxy (cache_text) has reduced HTTP availability'.,"Service Probes and Health Checks Failures, Network Connectivity Problems, Cache Layer Anomalies"
32,2022-06-30_asw-a4-codfw_accidental_power_cycle.wikitext,Network Connectivity Loss in A4-codfw Rack,2022-06-30 15:23:00,2022-06-30 15:23:00,2022-06-30 15:41:00,Sev(3),Network connectivity for the A4 codfw server rack went down due to full power loss of its switch. The issue was resolved in approximately 18 minutes with minimal impact on users.,00:18:00,00:00:00,00:18:00,Network services (A4-codfw rack),Other,"A4-codfw server rack,Network switch",Networking Equipment,"For approximately 18 minutes, servers in the A4-codfw rack lost network connectivity. Little to no external impact as affected services were either inactive in Codfw or had local redundancy.",Network Connectivity Problems,Full power loss of the A4 codfw server rack switch due to improper connection of the secondary power cord.,Hardware Failures,Lots of alert spam indicating network connectivity issues in the A4-codfw rack.,Network Connectivity Problems
33,2020-07-05_eqsin-router-crash.wikitext,Primary Hard Drive Failure on cr3-eqsin Router,2020-07-05 11:22:00,2020-07-05 11:22:00,2020-07-05 11:27:00,Sev(3),"On Sunday 5th at 11:22UTC, the primary hard drive of cr3-eqsin (one of the two Singapore POP routers) crashed, causing the router to reboot into its second disk with a factory default configuration. Everything failed over cleanly to the redundant router.",00:05:00,00:00:00,00:05:00,Network Connectivity,Other,"Primary Hard Drive,Primary Hard Drive","Networking Equipment, Other",We lost at max ~15000 requests/s in a 7min window.,"Partial Service Outage, Increased Error Rates",The primary hard drive of cr3-eqsin crashed.,Hardware Failures,"Host cr3-eqsin went DOWN with PING CRITICAL - Packet loss = 100%, triggering a paging alert.",Network Connectivity Problems
34,2020-05-22_thumbnails.wikitext,Memcached Configuration Issue in Wikimedia Commons,2020-05-22 00:28:00,2020-05-22 16:39:00,2020-05-22 22:24:00,Sev(3),"A change to the Memcached configuration in Wikimedia Commons led to a split-brain scenario due to incorrect classification of cache keys, causing issues with media uploads.",21:56:00,16:11:00,05:45:00,"Wikipedia,Wikipedia,Wikipedia,Wikipedia",Other,"Memcached,FileRepo,FileRepo","Caching Servers, Web Servers & Application Layers","About 10% of media uploads hit a race condition causing them to wrongly return a cached impression of the file not existing and thus unable to be inserted into articles. No data was lost. All affected files eventually became accessible either by themselves after a cache churn, or when our fix was deployed.","Cache Issues, Partial Service Outage","Cache keys relating to thumbnail metadata were incorrectly marked as 'local' instead of 'global', leading to inconsistent cache key distributions across Commons and Wikipedia.",Configuration Errors,"The issue was first detected when a user reported on the Commons Village Pump that some recently uploaded images couldn't be viewed from Wikipedia. Shortly after, tasks were created in Phabricator to track the problem. No automated alerts were triggered.",User Reported Problems
35,2022-10-04_jupyterhub-conda_outage.wikitext,Incident Report - Draft,2022-10-17 00:00:00,2022-10-17 00:00:00,2022-10-17 00:06:00,Sev(2),An incident occurred causing an outage that started at 00:00 UTC and ended at 00:06 UTC. The post-outage cleanup was finished by 00:15 UTC.,00:06:00,00:00:00,00:06:00,,Other,,Other,"The outage impacted users between 00:00 and 00:06 UTC. During this time, services were unavailable or partially degraded, leading to a poor user experience.","Complete Service Outage, Partial Service Outage",The specific root cause is not detailed in the provided document draft.,Other,Detection of the issue is not specified in the provided document draft.,Other
36,2022-05-26_Database_hardware_failure.wikitext,Incident T309286: Internal Service Outage Due to Faulty Memory Stick,2022-05-26 09:38:00,2022-05-26 09:38:00,2022-05-26 09:50:00,Sev(2),"For approximately 12 minutes on May 26, 2022, some internal services, including Bacula and Etherpad, were unavailable or operated at reduced capacity due to a reboot of db1128, the primary host of the m1 database section.",00:12:00,00:00:00,00:12:00,"Bacula,Etherpad",Other,"m1 database section,m1 database section",Databases,"For 12 minutes, internal services hosted on the m1 database (e.g., Etherpad) were unavailable or at reduced capacity.","Partial Service Outage, Storage and Database Failures","A faulty memory stick led to a reboot of db1128, which was the primary host of the m1 database.",Hardware Failures,The issue was first detected when users noticed unavailability or reduced capacity of services such as Bacula and Etherpad.,User Reported Problems
37,2022-07-13_brief_outbound_bandwidth_spike_eqsin.wikitext,Brief Bandwidth Spike at Eqsin Data Center,2022-07-13 13:42:00,2022-07-13 13:42:00,2022-07-13 14:09:00,Sev(2),Brief outbound bandwidth spike for upload in Eqsin data center (Singapore). Recovered by itself.,00:27:00,00:00:00,00:27:00,Image Serving,Media Storage and File Handling,Eqsin Data Center (Singapore),Other,"For 20 minutes, there was a small increase in error responses for images served from the Eqsin data center (Singapore).",Increased Error Rates,Outbound bandwidth spike for upload.,Unexpected Traffic Surges,Increase in error responses for images served from the Eqsin data center detected.,"Appserver Issues, Network Connectivity Problems, Cache Layer Anomalies"
38,2019-09-16_kubernetes-dns.wikitext,Partial Failures in Citoid and Cxserver Services,2019-09-16 13:43:00,2019-09-16 13:43:00,2019-09-16 14:29:00,Sev(2),Two services (citoid and cxserver) in eqiad experienced partial failures with citoid not augmenting results and cxserver not returning translations.,00:46:00,00:00:00,00:46:00,"citoid,cxserver",API and Application Servers,"calico-policy-controller,CoreDNS,helmfile","Containers & Kubernetes, Networking Equipment, Cluster Management",Citoid did not augment results with Zotero data for 46 minutes. cxserver did not return translations for use in ContentTranslation in eqiad for 35 minutes.,API Failures,Calico network policy rules were not correctly applied in eqiad due to a misconfiguration in the calico-policy-controller referencing the codfw datacenter.,Configuration Errors,"The issue was detected by both human observation and automated monitoring via Icinga alerts. Grafana dashboards indicated a partial outage for Citoid, and cxserver also triggered alerts.","Service Probes and Health Checks Failures, User Reported Problems"
39,2023-02-07_mediawiki.page-undelete_event_stream.wikitext,Incident on MediaWiki EventBus Extension Impacting WDQS and WCQS,2022-10-31 00:00:00,2023-02-07 00:00:00,2023-02-07 00:00:00,Sev(2),"On 2022-10-31, a change to the EventBus extension unregistered the hook handler for the mediawiki.page-undelete stream, causing an outage that was not noticed for over 3 months.",2376:00:00,2376:00:00,00:00:00,"WDQS,WCQS,event.mediawiki_page_undelete table in Hive,stream.wikimedia.org","Wikidata Query Service, Wikidata Query Service, Database and Data Analytics, API and Application Servers","EventBus extension,EventBus extension,EventBus extension",Other,The incident primarily affected WDQS and WCQS with inconsistencies in their downstream datastores. External consumers like the Internet Archive might have missed page undelete events. The Event.mediawiki_page_undelete table in Hive was empty for the affected period.,"Data Corruption or Loss, Other",The developer and reviewers did not catch an accidental change to extension.json that unregistered the EventBusHooks:onPageUndelete hook handler.,Configuration Errors,"A user reported inconsistencies in WDQS results on 2023-02-07, which led to the discovery of the issue.",User Reported Problems
40,2023-09-20_mw-page-content-change-enrich.wikitext,Network Timeout Issue Affecting mw-page-content-change-enrich,2023-09-19 15:00:00,2023-09-19 15:00:00,2023-09-20 12:00:00,Sev(2),The mw-page-content-change-enrich (flink) app failed to start in eqiad (passive DC) due to a network timeout with a dependent service (thanos-swift).,21:00:00,00:00:00,21:00:00,"mw-page-content-change-enrich,thanos-swift","Database and Data Analytics, Media Storage and File Handling","eqiad (passive DC),k8s operator,network egress rules","Databases, Containers & Kubernetes, Networking Equipment",There was no impact on the application SLO. This incident manifested on the passive DC deployment. The active DC deployment had no SLI degradation. Clients were not affected.,Other,"Incorrect egress rules caused a network timeout with the dependent service swift-thanos, resulting in the k8s operator's failure to start the mw-page-content-change-enrich app in the passive DC.","Configuration Errors, External Service Failures",Alerts were fired based on Prometheus metrics.,Other
41,2024-06-12_WMCS_toolforge_k8s_control_plane.wikitext,Kyverno Policy Overload Impacts Toolforge Kubernetes,2024-06-12 16:35:00,2024-06-12 16:40:00,2024-06-12 17:13:00,Sev(4),System-wide renaming of Kyverno policies overloaded Toolforge Kubernetes control nodes rendering API unresponsive. Lack of working Kubernetes API prevented rollback.,00:38:00,00:05:00,00:33:00,Toolforge,Cloud Services and Virtual Private Servers (VPS),"Kyverno policies,Kubernetes control nodes,Kubernetes control nodes,Registry admission webhook,k8s control nodes (tools-k8s-control-[7,8,9]),coredns","Containers & Kubernetes, Cluster Management","All Toolforge users were impacted. While running webservices/jobs were somewhat alive and reachable, no jobs or webservices could be created, or operated in any way.","Partial Service Outage, API Failures",System-wide renaming of Kyverno policies combined with lifted resource limits on Kyverno overloaded the k8s API server.,"Configuration Errors, Capacity Overloads",Detected by Arturo when the k8s API became unresponsive during ongoing deployment work. Some alerts followed but only after the incident was open and fixes in progress.,"Network Connectivity Problems, Service Probes and Health Checks Failures"
42,2022-12-13_sessionstore.wikitext,Sessionstore Outage - 2022-12-13,2022-12-13 12:15:00,2022-12-13 12:15:00,2022-12-13 12:24:00,Sev(2),"A wrong configuration change caused sessionstore pods to be unschedulable in our WikiKube cluster, resulting in failed edits across all projects for 9 minutes.",00:09:00,00:00:00,00:09:00,"WikiKube Cluster,All projects",Other,"sessionstore,sessionstore,Kubernetes Nodes","Databases, Containers & Kubernetes",All users were unable to edit for a period of 9 minutes.,Complete Service Outage,A configuration change for MatchNodeSelector caused sessionstore pods to be unschedulable due to node affinity for specific rack rows.,Configuration Errors,Automated monitoring detected the issue with an alert for failed probes: (ProbeDown) firing: Service sessionstore:8081 has failed probes (http_sessionstore_ip4) #page.,Service Probes and Health Checks Failures
43,2020-02-04_maps.wikitext,Maps Servers Saturation Incident,2020-03-03 16:03:00,2020-03-03 16:19:00,2020-03-03 21:20:00,Sev(3),"Maps servers were fully saturated on CPU, causing increased user-experienced latency and request error rates. To mitigate, traffic from non-Wikimedia sites was blocked.",05:17:00,00:16:00,05:01:00,"Maps,Wikimedia wikis and projects",Other,"Maps servers,Mediawiki API,Maps servers,CDN","Web Servers & Application Layers, Web Servers & Application Layers, Web Servers & Application Layers, Caching Servers","All Maps users, including those of Wikimedia wikis and projects, experienced elevated latency and error rates. Non-Wikimedia users were unable to load tiles not cached by CDN starting at 21:00 and ongoing as of the report date. Approximately 1.44 million errors were served during the outage.","Elevated Response Times and Latencies, Increased Error Rates, Cache Issues","Long-running lack of staffing on the software and its infrastructure, leading to a lack of familiarity with the software and the infrastructure by the SRE team and others involved in operations.",Other,Elevated latency and errors were experienced by users before automated LVS/PyBal alerts detected the issue.,"Latency and Timeout Issues, User Reported Problems, Service Probes and Health Checks Failures"
44,2022-11-17_Gerrit_3.5_upgrade.wikitext,Gerrit Partition Full After Upgrade,2022-11-17 09:00:00,2022-11-17 09:16:00,2022-11-17 11:45:00,Sev(2),"An upgrade of Gerrit from 3.4.8 to 3.5.4 caused the root partition to fill up, making Gerrit unavailable for almost three hours.",02:45:00,00:16:00,02:29:00,Gerrit,Other,"root partition,indexes,gerrit_file_diff.h2.db",Other,Gerrit was unavailable for almost three hours.,Complete Service Outage,"The root partition filled up after the Gerrit upgrade, preventing Gerrit from writing to its indexes.",Capacity Overloads,"The issue was first detected when users were unable to write comments or cast votes, leading to a 500 Internal Server Error modal window. Icinga alerts were not triggered as the monitoring was disabled due to the maintenance window.","Database Errors, User Reported Problems, Other"
45,2023-04-17 eqiad LVS.wikitext,eqiad/LVS Incident,2023-05-25 14:04:00,2023-05-25 14:04:00,2023-05-25 14:25:00,Sev(2),"During a scap deploy, one of the LVS servers in eqiad was taken down, causing multiple servers to be depooled and making eqiad unable to serve traffic, leading to an outage.",00:21:00,00:00:00,00:21:00,Wikimedia wikis,MediaWiki Core Services,"LVS servers,LVS servers,appservers,parsoid-php,api-https,jobrunners,videoscalers","Load Balancers, Load Balancers, Web Servers & Application Layers, Web Servers & Application Layers, Web Servers & Application Layers, Web Servers & Application Layers, Web Servers & Application Layers","For approximately 15-20 minutes, logged-in users connecting to Wikimedia wikis through the Ashburn datacenter and editors in general may have received 503 errors.","Partial Service Outage, Increased Error Rates","An LVS server (lvs1019) in eqiad was taken down during a scap deploy, resulting in multiple servers being depooled and eqiad being unable to serve traffic.",Configuration Errors,"The issue was detected immediately by SRE doing maintenance work, with multiple alerts and pages following promptly. Initial alert: PyBal backends health check on lvs1019 was CRITICAL.",Service Probes and Health Checks Failures
46,2019-12-04_MediaWiki.wikitext,Roll out of wmf.8 to group1 broke the world,2023-04-05 23:30:00,2023-04-05 23:30:00,2023-04-05 23:38:00,Sev(4),Roll out of wmf.8 to group1 caused significant issues leading to a major outage.,00:08:00,00:00:00,00:08:00,"logstash,logstash,Icinga",Monitoring and Logging,"Parsoid,Parsoid,database,group1 wikis,group1 wikis","Web Servers & Application Layers, Web Servers & Application Layers, Databases, Web Servers & Application Layers, Web Servers & Application Layers",Production group1 and group2 wikis became noticeably sluggish and eventually stopped working entirely.,Complete Service Outage,The incident was likely caused by issues that were obscured by a focus on Parsoid errors during the initial deploy.,"Software Bugs, Configuration Errors",Initial indicators of the issue were picked up in logstash and via logspam-watch on mwlog1001. A large number of Icinga alerts followed.,Service Probes and Health Checks Failures
47,2022-10-17_mx_and_vrts.wikitext,Delayed Email Delivery Due to Spam and Resource Exhaustion,2022-10-17 20:32:09,2022-10-17 20:32:09,2022-10-17 23:07:09,Sev(3),"A wave of spam email caused memory issues on the VRT machine (otrs1001), delaying mail delivery and triggering alerts.",02:35:00,00:00:00,02:35:00,"email delivery,VRT",Other,"otrs1001 (VRT machine),mx1001 (mail server),clamav-daemon",Other,"Delayed mail delivery, with users of VRT and general email recipients receiving mail delayed and spam email.","Elevated Response Times and Latencies, Increased Error Rates","Excessive number of Perl processes spawned due to spam email, consuming all RAM on the virtual machine, causing the OOM-killer to terminate the clamav-daemon, halting mail delivery.","Capacity Overloads, Security Misconfigurations, Unexpected Traffic Surges","Incident detected when on-call SRE got paged by Splunk-On-Call (VictorOps) with incident name 'Critical: [FIRING:1] MXQueueHigh misc (node ops page prometheus sre)' and incident ID VictorOps 3094, indicating MX host mx1001 had many queued messages.",High CPU & Resource Utilization
48,2019-09-20_d2_switch_failure.wikitext,Connectivity Loss Due to Switch Failure in Eqiad Rack D2,2023-10-20 23:57:00,2023-10-20 23:57:00,2023-10-21 02:15:00,Sev(3),"At 23:56 UTC on Friday 20th, the top of rack switch asw2-d2-eqiad went down causing all servers in rack D2 to go offline and brief connectivity loss for all row D servers. Some services remained degraded but eventually recovered.",02:18:00,00:00:00,02:18:00,"AQS,mobileapps,restbase,MW APIs,Logstash,Cloud NFS,Toolforge Kubernetes,Toolforge Kubernetes","API and Application Servers, API and Application Servers, Caching and Proxy Services, API and Application Servers, Monitoring and Logging, Media Storage and File Handling, Cloud Services and Virtual Private Servers (VPS), Cloud Services and Virtual Private Servers (VPS)","Top-of-Rack Switch,Virtual-chassis,Network Infrastructure,Row D,an-worker,an-worker,an-worker,an-worker,backup,cloudstore,cloudstore,cp,cp,flerovium,kafka,labstore,ms-be,ms-be,dbproxy,dbproxy,dbproxy","Networking Equipment, Other, Networking Equipment, Web Servers & Application Layers, Web Servers & Application Layers, Web Servers & Application Layers, Web Servers & Application Layers, Web Servers & Application Layers, Other, Web Servers & Application Layers, Web Servers & Application Layers, Caching Servers, Caching Servers, Other, Other, Web Servers & Application Layers, Web Servers & Application Layers, Databases, Databases, Databases","14 Hosts in D2 lost connectivity. Row D hosts experienced packet loss briefly. Some services such as AQS and mobileapps were impacted long after the initial blip. Logstash fell behind and threw many alerts. dbproxy1016 and dbproxy1017 failed health checks and switched to secondary choices. Cloud NFS clients lost access to specific services, and the maps Cloud VPS project lost NFS access until manual failover.","Network Connectivity Problems, Partial Service Outage, Elevated Response Times and Latencies","The immediate cause was a failure of the top of rack switch asw2-d2-eqiad, but the exact reason for the switch failure could not be determined.",Hardware Failures,"The first alert came from Icinga showing commons was down at 23:57 UTC, followed by an alert storm. The switch failure became apparent only after correlating alerts from Icinga and Netbox.",Network Connectivity Problems
49,2020-09-07_mobilefrontend-sec.wikitext,Stored XSS Security Issue on Talk Pages in MobileFrontend,,,,Sev(2),A stored XSS security issue was reported affecting talk pages in MobileFrontend and was patched the same day.,00:00:00,00:00:00,00:00:00,MobileFrontend,Other,talk pages,Other,Users accessing talk pages in MobileFrontend could have been exposed to malicious scripts due to the stored XSS issue.,Security Incidents,Stored XSS vulnerability in the talk pages of MobileFrontend.,Security Misconfigurations,The issue was reported and detected as a security vulnerability related to XSS in talk pages.,Security Vulnerabilities
50,2021-04-20_MediaWiki_API_slowdown.wikitext,Incident Report: Uncached Wiki Requests Instability,2023-09-01 07:31:00,2023-09-01 07:31:00,2023-09-01 08:16:00,Sev(2),"From approximately 7:31 to 8:16 UTC, there was instability serving Wiki uncached content, affecting authenticated users, bots, and POST requests. Most wikis experienced increased latencies and high levels of errors due to resource contention.",00:45:00,00:00:00,00:45:00,"Wiki uncached content,Authenticated users,Bots services,POST requests",Other,"service-wiki,service-servers,service-servers","Databases, Web Servers & Application Layers","Users experienced increased latencies (up to 5+ seconds) and a high level of errors, with approximately 1/4th of requests lost.","Elevated Response Times and Latencies, Increased Error Rates, Data Corruption or Loss","Excessive request rates from a bot caused overload on commonswiki requests, which blocked resources on most API and database servers, creating contention on other wikis.","Capacity Overloads, Unexpected Traffic Surges",The issue was first detected by increased latencies and high levels of errors within the system.,"Latency and Timeout Issues, High CPU & Resource Utilization"
51,2023-05-28_wikireplicas_lag.wikitext,Wikireplicas Data Unavailability Incident,2023-05-25 05:11:00,2023-05-25 05:11:16,2023-06-01 08:47:00,Sev(3),Wikireplicas experienced issues with outdated data and were unavailable for around one week due to a MariaDB replication bug.,171:36:00,00:00:16,171:35:44,"WMCS services,Database Replication,Database Replication","Cloud Services and Virtual Private Servers (VPS), Database and Data Analytics","MariaDB,db1154,db1154,clouddb1021,dbproxy1018,dbproxy1018","Databases, Web Proxies","Wikireplicas had outdated data and were unavailable for around one week, affecting tools most likely experienced intermittent unavailability or outdated data. Users complained about the lack of access to meta_p and heartbeat_p databases and slow query performance.","Partial Service Outage, Elevated Response Times and Latencies, Increased Error Rates, Storage and Database Failures",Replication of multiple sections broke most probably due to a bug in MariaDB version 10.4.29.,"Database Issues, Software Bugs","Replication lag was detected first by Icinga alerts, indicating critical replication issues on MariaDB replicas (s1 on db1154 and s2 on db1155).","Database Errors, Service Probes and Health Checks Failures"
52,2022-06-01_Lost_index_in_cloudelastic.wikitext,Cloudelastic Elasticsearch Data Loss Incident,2022-05-31 13:00:00,2022-05-31 13:00:00,2022-07-12 14:00:00,Sev(2),"During a reimage operation, the cloudelastic Elasticsearch cluster lost a shard and went into red status, indicating data loss. Restoration from production snapshots failed consistently, delaying the restoration by a month until it was completed on 12 July.",1009:00:00,00:00:00,1009:00:00,Cloudelastic,"Cloud Services and Virtual Private Servers (VPS), Search and Indexing","Elasticsearch cluster,Elasticsearch cluster,data restoration process",Other,"For 41 days, Cloudelastic was missing search results about files from commons.wikimedia.org.","Partial Service Outage, Data Corruption or Loss",The cloudelastic Elasticsearch cluster lost a shard during a reimage operation and restoration from production snapshots failed consistently.,"Database Issues, Software Bugs","The issue was first detected when the cloudelastic Elasticsearch cluster went into red status, indicating data loss.",Database Errors
53,2023-06-18_search_broken_on_wikidata_and_commons.wikitext,Search Broken on Wikidata and Commons - June 2023,2023-06-17 11:30:00,2023-06-17 22:07:00,2023-06-18 10:02:00,Sev(3),A reindexing task for Elasticsearch indices on Wikidata and Commons resulted in search failures due to a non-existent analyzer reference.,22:32:00,10:37:00,11:55:00,"Wikidata,Commons",Other,"Elasticsearch,Elasticsearch,Wikibase",Databases,Users could no longer perform searches on Wikidata and Commons during the outage period.,Partial Service Outage,The 'token_count_router' query was referencing a 'text_search' analyzer that had been de-duplicated and was no longer available.,Configuration Errors,The issue was first detected by users who reported the problem. It was further identified via increased CirrusSearch failures and confirmed with error messages indicating 'Unknown analyzer [text_search]'.,"User Reported Problems, Database Errors"
54,2019-08-07_s8-cawiki-errors.wikitext,DatabaseTermIdsResolver Misconfiguration in Wikidata,2019-08-07 17:26:00,2019-08-07 17:33:00,2019-08-07 19:17:00,Sev(2),"The table 'wb_terms' in 'wikidatawiki' used for label and description lookup of entities was replaced by 'TermStore'. A class called DatabaseTermIdsResolver picked the right host but used the wrong database name, leading to wrong database access.",01:51:00,00:07:00,01:44:00,"wiki,wiki,wiki",Other,"DatabaseTermIdsResolver,MediaWiki configuration,DatabaseTermIdsResolver",Other,"Some pages on 'cawiki' returned a 500 error due to failing to connect to the database. It potentially impacted all non-wikidata wikis, but 'cawiki' and 'ruwiki' use wikidata extensively enough for this to cause issues.","Partial Service Outage, Storage and Database Failures","DatabaseTermIdsResolver used the wrong database name ('cawiki' instead of 'wikidatawiki'), causing the code to try to get information from the wrong database.",Configuration Errors,We got the alert on IRC: PROBLEM - MediaWiki exceptions and fatals per minute on graphite1004 is CRITICAL: CRITICAL: 90.00% of data above the critical threshold [50.0],"Appserver Issues, User Reported Problems"
55,2022-10-06_eqiad_row_D_networking.wikitext,Partial Connectivity Outage in eqiad Row D,2022-10-07 14:50:00,2022-10-07 14:53:00,2022-10-07 14:52:00,Sev(2),"For 2 minutes, eqiad row D experienced a partial connectivity outage, impacting all types of clients. The automatic rollback restored full connectivity before manual recovery was needed.",00:02:00,00:03:00,23:59:00,"api-https,appserver,appserver,haproxy","API and Application Servers, Caching and Proxy Services","cr1-eqiad,cr1-eqiad,bast1003,dbproxy1016,dbproxy1016","Networking Equipment, Web Proxies","For 2 minutes eqiad row D suffered a partial connectivity outage, causing traffic through cr1-eqiad to be blackholed. This impacted all types of clients.","Partial Service Outage, Network Connectivity Problems",The configuration change on the cr1 discarded traffic towards the switch for unknown reasons.,Configuration Errors,"The issue was first detected when Ayounsi lost connectivity to cr1-eqiad and bast1003, followed by multiple alerts including high latency and service failures.","Network Connectivity Problems, Latency and Timeout Issues, Service Probes and Health Checks Failures"
56,2023-05-19 videoscaler jobrunner.wikitext,Incident on 2023-05-19 Involving Video Scaling and Jobrunner,2023-05-19 19:04:00,2023-05-19 19:24:00,2023-05-19 20:53:00,Sev(2),"A large video-scaling job caused server mw1469 to max out its capacity with ffmpeg processes, leading to flapping alerts for both jobrunner and videoscaler services.",01:49:00,00:20:00,01:29:00,"jobrunner,videoscaler","MediaWiki Core Services, Media Storage and File Handling","mw1469,mw1469,ffmpeg","Web Servers & Application Layers, Web Servers & Application Layers, Other",The user who uploaded those videos had to wait a bit longer to get different formats. Possibly other users waited a bit longer for other jobs.,Elevated Response Times and Latencies,A large video-scaling job made server mw1469 so busy that its capacity was maxed out by ffmpeg processes.,Capacity Overloads,"First detected by Icinga reporting via icinga-wm on IRC, followed by a page via Alertmanager.",Service Probes and Health Checks Failures
57,2024-06-10_puppet_volatile_data_broken_sync.wikitext,Puppet Rsync Client Hanging Incident,2024-04-27 00:00:00,2024-04-27 00:00:00,2024-04-27 00:06:00,Sev(2),"Between 2024-04-27 00:00 and 2024-04-27 00:08 UTC, the rsync clients of most puppetmasters/puppetservers to sync data from puppetmaster1001 hung indefinitely. On 2024-06-10, they were killed and restarted manually.",00:06:00,00:00:00,00:06:00,"puppet,CheckUser,Analytics",Other,"puppetmaster1001,puppetmaster1001,GeoLite2 files,puppetmaster1001",Other,"New data from puppet 'volatile' was only rarely/intermittently synced to much of the fleet during this window. The problem was particularly bad in codfw, where all 3 puppetservers had failed to rsync data for the entire duration. The impact is uncertain because, aside from one service, no one noticed any issues.","Partial Service Outage, Data Corruption or Loss","Infinite timeouts were allowed, causing rsync clients to hang indefinitely. Monitoring and logging were disabled, preventing alerting/notification on sync failures. The issue arose from a deadlock situation in the select() call for the rsync client sockets.","Configuration Errors, Software Bugs","Manual detection by Kosta Harlan, who noticed the absence of new GeoLite2 files while working on a task. The missing files were confirmed by cdanis during an investigation initiated from an inquiry on the #wikimedia-sre-foundations IRC channel.",Other
58,2020-02-06_mediawiki.wikitext,Widespread service timeouts following group2 wikis update,2020-02-06 20:22:00,2020-02-06 20:24:00,2020-02-06 20:52:00,Sev(4),Widespread service timeouts starting immediately after moving group2 wikis to 1.35.0-wmf.18. Reverting to 1.35.0-wmf.16 immediately brought most services back to life.,00:30:00,00:02:00,00:28:00,"Other,Other,Other","MediaWiki Core Services, Monitoring and Logging, API and Application Servers","wikifeeds,Apache HTTP on mw servers,eqiad appservers,Apache HTTP on mw servers,eqiad appservers","Web Servers & Application Layers, Caching Servers, Web Servers & Application Layers, Web Servers & Application Layers, Databases","All sites were unresponsive or at least partially offline for ~8 minutes. (Except for cache hits, which would be 'popular' pages for non-logged-in users.)","Complete Service Outage, Partial Service Outage",A typo in a config setting for Wikibase.,Configuration Errors,~100% of icinga alerts fired simultaneously.,"Service Probes and Health Checks Failures, Network Connectivity Problems"
59,2021-11-02_Cloud_VPS_networking.wikitext,2021-11-02 Cloud VPS networking,2021-11-02 11:35:00,,2021-11-02 13:20:00,Sev(2),"For about 1 hour and 40 minutes, Toolforge services and VMs in Cloud VPS may have experienced connectivity issues.",01:45:00,00:00:00,00:00:00,"Toolforge,Cloud VPS",Cloud Services and Virtual Private Servers (VPS),"cloudnet,cloudnet,Toolforge NFS in Kubernetes,LDAP connections,cloudnet",Other,"For about 1 hour and 40 minutes, Toolforge services and VMs in Cloud VPS may have experienced connectivity issues.",Network Connectivity Problems,The issue was caused by a kernel upgrade for several Cloud VPS network components that affected ingress traffic to the network edge for cloud VMs.,Configuration Errors,"Problems were initially detected with Toolforge NFS in Kubernetes and later with LDAP connections. Eventually, it was determined to be a problem with all ingress traffic to the network edge for cloud VMs.",Network Connectivity Problems
60,2020-08-12_appservers_oom.wikitext,Incident Report for T260329,,,,Sev(2),Restricted. Details at https://phabricator.wikimedia.org/T260329.,00:00:00,00:00:00,00:00:00,Restricted,Other,,Other,,Other,,Other,,Other
61,2022-05-20_Database_slow.wikitext,High Latency and Unavailability due to Application Server Worker Thread Exhaustion,2022-05-20 09:35:00,2022-05-20 09:35:00,2022-05-20 09:35:00,Sev(3),"On 2022-05-14 and 2022-05-20, there were incidents of high latency and unavailability due to application server worker thread exhaustion caused by slow database responses.",00:00:00,00:00:00,00:00:00,uncached traffic,Caching and Proxy Services,"application server,database","Web Servers & Application Layers, Databases",Two occurrences of impact on uncached traffic resulting in high latency and unavailability.,"Partial Service Outage, Elevated Response Times and Latencies, Cache Issues",MariaDB 10.6 performance regression under load.,"Database Issues, Software Bugs","Detected through 3 and 5-minute impacts on uncached traffic with high latency and unavailability, first noticed on 2022-05-14 at 8:18 UTC and again on 2022-05-20 at 09:35 UTC.","Latency and Timeout Issues, Cache Layer Anomalies"
62,2020-09-02_wdqs-outage.wikitext,Wikidata Query Service Outage Incident 20200902,2020-09-02 20:34:00,2020-09-02 20:42:31,2020-09-02 22:27:00,Sev(2),"Active Wikidata Query Service instances were down due to Blazegraph being 'locked up', causing queries to time out and exceed nginx's timeout window.",01:53:00,00:08:31,01:44:29,Wikidata Query Service,Wikidata Query Service,"Blazegraph,nginx","Databases, Web Servers & Application Layers","Intermittent service disruption occurred between 20:34 and 22:27 UTC, with about 75% of the time in a state of total outage.","Complete Service Outage, Partial Service Outage","Blazegraph received malformed queries, which triggered deadlocks leading to service unavailability.",Software Bugs,"Issue was first detected when a critical alert from PyBal backends health check was issued at 20:42 UTC, indicating multiple servers being marked down.","Service Probes and Health Checks Failures, Network Connectivity Problems"
63,2022-12-09_MediaWiki_REST_API.wikitext,REST API Content Versioning Issue on MediaWiki,2022-12-06 12:27:18,2022-12-08 23:30:00,2022-12-09 02:39:00,Sev(2),"A change in the MediaWiki REST API caused requests for old revisions to serve the current revision, detected due to visual diffs indicating no changes.",62:11:42,59:02:42,03:09:00,"MediaWiki,VisualEditor",MediaWiki Core Services,"MediaWiki REST API,MediaWiki REST API,MediaWiki REST API",Web Servers & Application Layers,"Visual diffs showed no changes (common feature, approx. 100,000 users of the beta feature on enwiki), and editing old revisions using the VisualEditor did not work (rare occurrence).",Partial Service Outage,A refactoring in the MediaWiki REST API caused it to incorrectly serve the current revision when old revisions were requested.,Software Bugs,The issue was first detected manually by observing that visual diffs indicated no changes had occurred.,User Reported Problems
64,2020-10-30_fr.wikipedia.org-slow-main-page.wikitext,"French Wikipedia Main Page Slow Loading Incident - October 30, 2020",2020-10-30 12:22:00,2020-10-30 17:40:00,2020-10-30 18:56:00,Sev(2),"The main page of the French Wikipedia was loading significantly slower than usual on October 30th, 2020, with load times of up to 20 seconds for logged-in users.",06:34:00,05:18:00,01:16:00,Wikipedia,Other,"FeaturedFeeds extension,Memcached",Other,Logged-in users of the French Wikipedia experienced slow load times of several seconds to up to 20 seconds. Logged-out users were not affected due to their pages being served from cache.,"Elevated Response Times and Latencies, Cache Issues","The FeaturedFeeds extension was generating and parsing the entire feed on every main page load because the feed size exceeded the memcached limit, resulting in 'ITEM_TOO_BIG' errors.","Capacity Overloads, Software Bugs",The issue was first detected when users started reporting the slow load times on the French Wikipedia Bistro page and a ticket was created on Phabricator.,"User Reported Problems, Latency and Timeout Issues"
65,2020-02-25_mediawiki_interface_language.wikitext,Configuration Change Caused UI Language Issue,2020-02-24 00:53:00,2020-02-24 08:41:00,2020-02-24 14:35:00,Sev(2),Logged-out users saw the user interface in their browser language rather than the default wiki content language due to a configuration change causing the $wgULSLanguageDetection setting to be unset.,13:42:00,07:48:00,05:54:00,"UniversalLanguageSelector,appserver","Other, API and Application Servers","$wgULSLanguageDetection,InitialiseSettings.php,InitialiseSettings.php",Other,"Anonymous users were affected, registered users were not. Appserver load increased due to cache splitting by Accept-Language, and average response time also went up considerably.","Partial Service Outage, Elevated Response Times and Latencies, Cache Issues",A configuration change inadvertently causing the $wgULSLanguageDetection setting to be unset and falling back to the default in the UniversalLanguageSelector extension.,Configuration Errors,Reported by users on Phabricator (T246071) and other tasks. There were no alerts.,User Reported Problems
66,2022-07-03_shellbox_request_spike.wikitext,Shellbox Service Incident Report,2022-07-03 11:17:00,2022-07-03 11:17:00,2022-07-03 11:33:00,Sev(2),"An increase in Score requests overwhelmed the Shellbox service, causing slow or unavailable edits and previews for pages with musical notes for 16 minutes.",00:16:00,00:00:00,00:16:00,"Shellbox,Parsoid",MediaWiki Core Services,"Score extension,k8s pods","Containers & Kubernetes, Other","For 16 minutes, edits and previews for pages with Score musical notes were too slow or unavailable.","Partial Service Outage, Elevated Response Times and Latencies",An increase in Score requests (musical note rendering) from Parsoid overwhelmed the Shellbox service.,"Capacity Overloads, Unexpected Traffic Surges","The issue was first detected from Grafana and Logstash dashboards showing a significant increase in request times and errors, including HTTP 503 and HTTP 504.","Latency and Timeout Issues, Network Connectivity Problems"
67,2023-07-19_enrich_job_outage.wikitext,Incident involving mw-page-content-change-enrich application outage on 2023-07-19,2023-07-19 18:50:30,2023-07-19 20:10:38,2023-07-19 20:30:00,Sev(2),"An enriched message exceeding Kafka's max.request.size caused the mw-page-content-change-enrich application's Kafka producer to crash, resulting in a Flink Taskmanager shutdown.",01:39:30,01:20:08,00:19:22,mw-page-content-change-enrich application,MediaWiki Core Services,"Flink Taskmanager,Kafka producer",Other,The mw-page-content-change-enrich application (eqiad) has not been producing enriched events during the outage.,Partial Service Outage,An enriched message exceeding Kafka's max.request.size caused the application's Kafka producer to crash.,Configuration Errors,GModena and TChin reacted to alerts triggered by degrading Service Level Indicators (SLIs).,"Latency and Timeout Issues, User Reported Problems"
68,2020-05-01_vc-link-failure.wikitext,Virtual Chassis Link Failure Between asw2-d1-eqiad and asw2-d8-eqiad,2023-10-01 05:21:00,2023-10-01 05:21:00,2023-10-02 07:08:00,Sev(2),"The virtual chassis link between asw2-d1-eqiad and asw2-d8-eqiad failed in two steps, causing packet loss for hosts on D1 and connectivity issues between MediaWiki appservers and memcache servers.",25:47:00,00:00:00,25:47:00,"appservers,servers,servers","API and Application Servers, Caching and Proxy Services","asw2-d1-eqiad,asw2-d1-eqiad,mw1356-1362","Networking Equipment, Web Servers & Application Layers","Little to no effect on traffic, error rates, and latencies for anonymous users. Logged in users experienced a ~1% error rate increase (with a short spike to ~7.5%) and increased tail latency (around 100%-150% increase).","Partial Service Outage, Elevated Response Times and Latencies, Increased Error Rates","Packet loss through Virtual Chassis Fabric, particularly between asw2-d1-eqiad and asw2-d8-eqiad.",Network Failures,"Detected by critical alerts from icinga-wm for MediaWiki exceptions and fatals per minute, and MediaWiki memcached error rate, followed by critical alerts for PHP7 rendering with socket timeouts.","Appserver Issues, Latency and Timeout Issues, Cache Layer Anomalies"
69,2023-05-23_wdqs_CODFW_5xx_errors.wikitext,WDQS Service Outage in CODFW Datacenter,2023-05-23 10:48:00,2023-05-23 10:47:00,2023-05-23 19:24:00,Sev(3),An expensive query repeatedly sent by an external user caused errors and timeouts for a subset of users accessing the WDQS service from the CODFW datacenter. A requestctl rule was implemented to mitigate the issue.,08:36:00,23:59:00,08:37:00,WDQS service,Wikidata Query Service,"PyBal backends,lvs2010,wdqs-heavy-queries_8888,wdqs-heavy-queries_8888","Load Balancers, Load Balancers, Web Servers & Application Layers, Databases",A subset of users accessing the WDQS service from our CODFW datacenter received errors and timeouts.,"Partial Service Outage, Increased Error Rates, Network Connectivity Problems",An expensive query sent over and over again by an external user(s) caused the issue.,Unexpected Traffic Surges,"First Icinga alerts fired at 10:47 UTC, indicating 'PyBal backends health check on lvs2010 is CRITICAL: PYBAL CRITICAL - CRITICAL - wdqs-heavy-queries_8888: Servers wdqs2009.codfw.wmnet are marked down but pooled'.",Service Probes and Health Checks Failures
70,2021-07-14_eventgate-analytics_latency_spike_caused_MW_app_server_overload.wikitext,EventGate Deployment Caused MediaWiki API Request Failures,2021-07-14 16:07:12,2021-07-14 16:07:12,2021-07-14 16:17:18,Sev(2),"During the deployment of updates to EventGate for Prometheus support, a spike in request latency from MediaWiki to eventgate-analytics caused some MediaWiki API requests to fail, leading to an auto-rollback of the deployment in codfw.",00:10:06,00:00:00,00:10:06,"MediaWiki API,EventGate,EventGate","API and Application Servers, Monitoring and Logging","php worker,node-rdkafka-prometheus,service-template-node,service-template-node,helm,node-rdkafka-prometheus","Web Servers & Application Layers, Monitoring & Telemetry, Containers & Kubernetes, Web Servers & Application Layers, Cluster Management, Monitoring & Telemetry","For ~10 minutes, MediaWiki API clients experienced request failures.",API Failures,A bug in the service-runner Prometheus integration caused the nodejs worker process to die when 'prom-client' was required within a worker.,Software Bugs,"Request latency spikes from MediaWiki to eventgate-analytics were detected, filling PHP worker slots and causing MediaWiki API request failures. Helm auto-rolled back the deployment after detecting issues with the eventgate-analytics deploy to codfw.","Latency and Timeout Issues, High CPU & Resource Utilization, Service Probes and Health Checks Failures"
71,2024-04-28_WMCS_Toolforge_Redis_refusing_connections.wikitext,Toolforge Redis Connection Incident,2024-04-28 13:39:00,2024-04-28 13:39:00,2024-04-28 14:36:00,Sev(2),Toolforge Redis refused new connections because there were too many active connections. This happened intermittently for 1 hour until the service was restarted manually.,00:57:00,00:00:00,00:57:00,Toolforge,Cloud Services and Virtual Private Servers (VPS),"tools-redis-6,tools-redis-6,tools-redis-6",Caching Servers,Toolforge tools using Redis suffered intermittent connection errors for the duration of the incident.,"Partial Service Outage, Network Connectivity Problems",Toolforge Redis server reached the maximum number of client connections.,Capacity Overloads,Toolschecker detected a problem with Redis and Icinga fired an alert. This was routed to Alertmanager and to VictorOps. VictorOps sent a page to the on-call engineer.,Cache Layer Anomalies
72,2021-02-01_swift-codfw.wikitext,Icinga Check Timeout Incident for `ms-fe.svc.codfw.wmnet`,2021-02-11 11:41:00,2021-02-11 11:41:00,2021-02-11 11:44:00,Sev(2),"Today, at 11:41, the icinga check for `ms-fe.svc.codfw.wmnet` timed out, resulting in a page alert which recovered three minutes later.",00:03:00,00:00:00,00:03:00,"swift-https,swift-https,swift-https","Media Storage and File Handling, Caching and Proxy Services","ms-fe.svc.codfw.wmnet,ms-fe.svc.codfw.wmnet,ms-fe.svc.codfw.wmnet,Swift backend",Caching Servers,"Users in codfw/ulsfo/eqsin experienced approximately 15 minutes of higher latency, possibly including timeouts, for hit-local and miss requests (10-25% of site requests, depending on the site).",Elevated Response Times and Latencies,"The slowness was induced by an earlier swift rebalance and the current method of performing such rebalances, which are generally noisy and impactful to the cluster.",Other,The issue was detected through an icinga alert for LVS swift-https codfw port 443/tcp being critical due to a socket timeout. Recovery was noticed when the alert status returned to OK.,"Latency and Timeout Issues, Service Probes and Health Checks Failures"
73,2021-03-30_Jobqueue_overload.wikitext,Incident Report on Server-Side Upload Process Impact,2023-10-01 08:00:00,2023-10-01 08:05:00,2023-10-01 09:30:00,Sev(3),"An upload of 65 video 4k files via the server-side upload process caused high CPU/socket timeout errors on jobrunners, resulting in an increased job backlog and unavailability on several mw-related servers.",01:30:00,00:05:00,01:25:00,"mw-related servers,videoscalers,mw-related servers","MediaWiki Core Services, Media Storage and File Handling","CPU,CPU,server-side upload process,server-side upload process,server-side upload process",Other,"Users experienced job backlog and unavailability on several mw-related servers, including job queue runners.",Partial Service Outage,"A combination of the files being 4k, requiring many downscales, and the fast connection from the local server caused excessive load on the jobqueue infrastructure.",Capacity Overloads,"The issue was first detected through high CPU usage and socket timeout errors on jobrunners, leading to increased job backlog and server unavailability.","High CPU & Resource Utilization, Latency and Timeout Issues"
74,2023-02-28_GitLab_data_loss.wikitext,"GitLab Production Data Loss Incident - February 28, 2022",2022-02-28 00:04:00,2022-02-28 02:03:00,2022-02-28 02:20:00,Sev(2),"GitLab was switched to the other data center as planned maintenance, and during the switchover, a backup was restored on the production host due to incorrect configuration, causing data loss for one and a half hours and reduced availability for 20 minutes.",02:16:00,01:59:00,00:17:00,GitLab,Other,"gitlab1004,gitlab1004,codfw","Code Deployment & Version Control, Other","Data loss on GitLab production host for one and a half hours, reduced availability for 20 minutes.","Data Corruption or Loss, Partial Service Outage","The daily restore job was not disabled on the new production host in codfw, resulting in a backup being restored on the production host.",Configuration Errors,"The issue was detected by automatic monitoring and task creation (probeDown) with details: alertname: ProbeDown, instance: gitlab2002:443, job: probes/custom, prometheus: ops, severity: task, site: codfw, source: prometheus, team: serviceops-collab.",Service Probes and Health Checks Failures
75,2024-02-28_etcd-mirror.wikitext,Etcd Replication Failure Incident,2024-02-28 01:39:00,2024-02-28 01:42:00,2024-02-28 02:52:00,Sev(2),"Etcd-mirror replication from eqiad to codfw failed due to an unusual sequence of data changes, causing the long-running watch to lose track of the up-to-date state of etcd in eqiad.",01:13:00,00:03:00,01:10:00,"conftool,etcd-mirror",Other,"etcd,etcd",Cluster Management,No user impact. Near miss for split brain config scenarios.,Other,"Etcd-mirror encountered an EtcdEventIndexCleared exception due to an unusually high number of non-conftool etcd events, which were triggered by Spicerack locks during a busy day for reimage cookbooks.","Capacity Overloads, Software Bugs",The issue was first detected by the 'EtcdReplicationDown' and 'SystemdUnitFailed' alerts firing.,"Service Probes and Health Checks Failures, Network Connectivity Problems"
76,2020-06-19_Logstash.wikitext,Logstash Index Staleness Incident,2023-10-30 00:00:00,2023-10-30 00:00:00,2023-10-30 00:06:00,Sev(2),Logstash indexes were temporarily stale and newer messages from Kafka were not available.,00:06:00,00:00:00,00:06:00,"Elasticsearch,Elasticsearch,Kafka","Monitoring and Logging, Other","Logstash Indexes,Kibana Monitoring Alerts",Monitoring & Telemetry,Monitoring alerts based on Logstash missed potential errors. Developers could not see the latest messages in Kibana and experienced reduced availability in querying older messages.,"Partial Service Outage, Elevated Response Times and Latencies, Increased Error Rates",TODO,Other,Not specified in the report.,Other
77,2022-03-06_wdqs-categories.wikitext,Wikidata Query Service API Outage Incident,2022-03-06 13:50:00,2022-03-06 16:52:00,2022-03-06 17:24:00,Sev(3),"The Wikidata Query Service (WDQS) experienced an overload causing it to sporadically block client traffic for 1.5 hours, affecting service availability.",03:34:00,03:02:00,00:32:00,Wikidata Query Service API,"Wikidata Query Service, API and Application Servers","wdqs::public,wdqs::public","Databases, Web Servers & Application Layers","For 1.5 hour, some requests to the public Wikidata Query Service API were sporadically blocked.","Partial Service Outage, Increased Error Rates, API Failures",The service got overloaded and started to block client traffic.,Capacity Overloads,Icinga sent some non-paging alerts at 13:50 indicating 'CRITICAL - wdqs-heavy-queries_8888'. Paging alerts started at 16:52 indicating a more severe outage.,Service Probes and Health Checks Failures
78,2021-04-26_thumbor.wikitext,Incident: Crash in Network Stack of ms-be1062,2023-10-27 09:21:00,2023-10-27 09:21:00,2023-10-27 09:38:00,Sev(2),"At 9:21, ms-be1062 crashed in its network stack, causing traffic blackholing and request timeouts to swift. This led to a resource starvation for thumbor cluster workers. Traffic was diverted to minimize impact, and rebooting ms-be1062 at 9:38 resolved the issue.",00:17:00,00:00:00,00:17:00,"thumbor,thumbor",Media Storage and File Handling,"network stack,ms-be1062,swift backend,pybal,network stack","Other, Web Proxies, Caching Servers, Cluster Management",Users experienced delayed thumbnail generation due to the resource starvation in the thumbor cluster workers waiting for swift responses.,"Elevated Response Times and Latencies, Storage and Database Failures","ms-be1062's network stack crash during a rebalance operation of the swift cluster, exacerbated by the demands placed on the swift backend in the current configuration.","Network Failures, Configuration Errors","Detected by traffic blackholing and request timeouts, with ms-be1062 not being depooled despite returning 503s, leading to resource starvation for thumbor cluster workers.","Appserver Issues, Latency and Timeout Issues, Network Connectivity Problems"
79,2021-07-16_asw-a2-codfw_network.wikitext,Switch Failure Impacting Codfw Services,2022-09-16 13:16:00,2022-09-16 13:16:00,2022-09-16 16:56:00,Sev(3),"asw-a2-codfw, the switch handling the network traffic of rack A2 on codfw became unresponsive, rendering 14 hosts unreachable and affecting additional load balancers and services dependent on them.",03:40:00,00:00:00,03:40:00,"Restbase,mobileapps,cxserver,Maps Tile Service,Wikidata Query Service,mw-api,mw-api,text cache","API and Application Servers, API and Application Servers, API and Application Servers, Content Delivery Network (CDN) and Edge Cache, Wikidata Query Service, API and Application Servers, API and Application Servers, Caching and Proxy Services","Rack A2 hosts,load balancers,switches,Rack A2 hosts,lvs2009,lvs2009,authdns2001","Load Balancers, Networking Equipment, Other, Load Balancers, Load Balancers, Other","For about 1 hour the Restbase, mobileapps, and cxserver services were serving errors. Reduced capacity of high-traffic1 load balancers and MediaWiki servers in Codfw.","Partial Service Outage, Increased Error Rates","One single switch failure led to a more significant outage because 3 load balancers including the backup one were dependent on a single network switch, and depool thresholds for several services were too restrictive.","Network Failures, Configuration Errors",Automated monitoring reported several hosts of rack A2 going down simultaneously. Icinga alerts included multiple hosts reporting PING CRITICAL with 100% packet loss.,"Network Connectivity Problems, Service Probes and Health Checks Failures"
80,2020-08-14_isp-unreachable.wikitext,"Users of Jio ISP (India, AS 55836) unable to reach Wikimedia sites",2020-08-19 13:45:00,2020-08-19 14:00:00,2020-08-19 15:30:00,Sev(2),"Users of Jio ISP (India, AS 55836) were unable to reach Wikimedia sites due to a misconfiguration in Jio's own RPKI records.",01:45:00,00:15:00,01:30:00,Wikimedia sites,Other,"RPKI records,network advertisements",Other,Users of Jio ISP in India were unable to reach Wikimedia sites.,Network Connectivity Problems,Misconfiguration in Jio's RPKI records.,Security Misconfigurations,"Users were unable to access Wikimedia sites, detected through user reports and monitoring alerts.","User Reported Problems, Service Probes and Health Checks Failures"
81,2024-06-11_WMCS_Ceph.wikitext,WMCS Cloud VPS Outage,2024-06-11 14:58:09,2024-06-11 15:01:50,2024-06-11 15:23:00,Sev(2),A faulty optic on one of the fiber links between WMCS racks caused a 25-minute outage of Cloud VPS and all hosted services due to packet loss in the Ceph storage cluster.,00:24:51,00:03:41,00:21:10,"Cloud VPS,Toolforge","Cloud Services and Virtual Private Servers (VPS), Other","Ceph storage cluster,fiber links,cloudsw1-d5-eqiad,cloudsw1-d5-eqiad",Other,For about 25 minutes Cloud VPS and all services hosted on it (incl. Toolforge) were completely inaccessible.,Complete Service Outage,"A faulty optic on one of the fiber links between WMCS racks caused packet loss between nodes in the Cloud VPS Ceph storage cluster, stalling writes and making VMs and services inaccessible.","Hardware Failures, Network Failures",The issue was first detected by an automated alert for CephSlowOps at 14:58:09 and a subsequent toolschecker NFS alert at 15:01:50 indicating HTTP 504 Gateway Time-out.,"Latency and Timeout Issues, Service Probes and Health Checks Failures"
82,2023-08-30_hadoop-yarn.wikitext,Log Aggregation Failure on Hadoop Worker Nodes,2023-08-29 11:02:00,2023-08-29 11:02:00,2023-08-30 16:22:00,Sev(3),Log aggregation was failing on recently re-imaged Hadoop worker nodes due to a wrong compression type in the log aggregator service.,29:20:00,00:00:00,29:20:00,"Airflow,Hadoop",Database and Data Analytics,"log aggregator service,Hadoop worker nodes,Airflow tasks",Other,"Partial log aggregation for yarn applications running on specific Hadoop worker nodes resulted in some non-idempotent Airflow tasks failing, despite Yarn applications being in a SUCCESS state.","Partial Service Outage, Data Corruption or Loss","The log aggregator service was configured with an incorrect value for the compression type, set to 'gzip' instead of 'gz'.",Configuration Errors,"The issue was first detected by multiple automated alerts from Airflow notifying that 'No logs found. Log aggregation may have not completed, or it may not be enabled.'",Service Probes and Health Checks Failures
83,2022-03-29_network.wikitext,Wikipedia Service Disruption Incident,incident_start_dt,incident_detected_dt,incident_end_dt,Sev(2),"For approximately 5 minutes, Wikipedia and other Wikimedia sites were slow or inaccessible for many users, mostly in Europe/Africa/Asia.",00:00:00,00:00:00,00:00:00,"Wikipedia,Wikipedia",Other,,Other,"Wikipedia and other Wikimedia sites were slow or inaccessible for many users, mostly in Europe/Africa/Asia.","Partial Service Outage, Network Connectivity Problems",,Other,,Other
84,2019-12-10_Cite.wikitext,In-progress Cite Extension Code Unleashes PHP Notice Logspam,2019-12-09 17:52:00,2019-12-09 17:52:00,2019-12-10 14:29:00,Sev(2),"In-progress Cite extension code caused a flood of 'PHP notice' logspam at approximately 3,000 errors per hour, burdening human log-watchers. Hotfixes reduced the logspam before the actual issue was identified, fixed, and backported.",20:37:00,00:00:00,20:37:00,1.35.0-wmf.8 train,MediaWiki Core Services,"Parser,Message class,Message class,Cite extension,API,Parser",Other,"The 1.35.0-wmf.8 train was delayed for several hours. Deployers spent extra time on hotfixes. References disappeared from <references /> sections on an unknown number of content pages, affecting pages that displayed an error message or used failing parser functions.","Partial Service Outage, Data Corruption or Loss","A combination of factors involving the Message class requesting its own Parser via PHP's clone command. This led to Cite extension's dynamic property $parser->extCite being cloned improperly, causing shared state between different Cite instances and resulting in errors when rendering <references /> sections.",Software Bugs,Brennen Bearnes noticed an increase in errors while monitoring logspam-watch during train deployment.,"Appserver Issues, User Reported Problems"
85,2022-11-03_conf_disk_space.wikitext,Conf1008 Disk Space Outage,2022-11-03 17:06:00,2022-11-03 17:06:00,2022-11-03 17:39:00,Sev(2),"A bug in the MediaWiki codebase caused an increase in connections to Confd hosts from Dumps systems, leading to a filled-up filesystem and impacting the confd service for ~33 minutes.",00:33:00,00:00:00,00:33:00,"etcd,etcd",Other,"MediaWiki core codebase,confd hosts,Dumps",Other,No user impact. Confd service failed for ~33 minutes.,Other,"A bug was introduced in MediaWiki core codebase causing configuration to be checked excessively for every row of a database query, leading to high volume of log events and disk space exhaustion.","Software Bugs, Capacity Overloads",The issue was first detected by an Icinga alert indicating critical disk space on conf1008: 'DISK CRITICAL - free space: / 2744 MB (3% inode=98%): /tmp 2744 MB (3% inode=98%): /var/tmp 2744 MB (3% inode=98%).',Other
86,2021-03-14_MediaWiki_API.wikitext,MediaWiki API Outage Due to Database Overload,2021-03-14 17:00:00,2021-03-14 17:00:00,2021-03-14 17:26:00,Sev(2),"On March 14, 2021, the MediaWiki API experienced an outage due to an overload of php-fpm processes from 17:00 to 17:26 UTC caused by queries against commons affecting database s4 on server db1144.",00:26:00,00:00:00,00:26:00,"API servers,API servers",API and Application Servers,"php-fpm processes,server db1144,server db1144,other MediaWiki features,other MediaWiki features,other MediaWiki features,other MediaWiki features","Databases, Web Servers & Application Layers",API users experienced an outage and were unable to access the MediaWiki API during the incident time period.,"Complete Service Outage, API Failures","Queries against commons overloaded database s4 on server db1144, which also serves other essential MediaWiki features.",Capacity Overloads,"The issue was first detected when the MediaWiki API ran out of php-fpm processes, leading to an API outage.",High CPU & Resource Utilization
87,2020-09-01_data-center-switchover.wikitext,Datacenter Switchover Incident Report,2020-09-01 14:02:00,2020-09-01 14:06:00,2020-09-01 14:07:00,Sev(2),"A planned datacenter switchover from eqiad to codfw was executed with some automation, leading to temporary performance issues and alert noise due to various service adjustments and underprovisioned resources.",00:05:00,00:04:00,00:01:00,"apertium,termbox,search,api-gateway,ores,sessionstore,eventgate-main,graphoid,eventstreams,wikifeeds,wdqs,parsoid,eventgate-logging-external,wdqs,echostore,mathoid,mobileapps,proton,restbase,kartotherian,recommendation-api,eventgate-analytics,restbase,citoid,schema,cxserver,eventgate-analytics,zotero,Thumbor,MediaWiki",Other,"sre.switchdc.services.00-reduce-ttl-and-sleep,Restbase,elasticsearch,Restbase,elasticsearch,Restbase,envoy,envoy","Databases, Web Proxies, Caching Servers, Load Balancers, Containers & Kubernetes, Monitoring & Telemetry, Code Deployment & Version Control, Networking Equipment, Cluster Management, Web Servers & Application Layers, Other

Networking Equipment, Web Servers & Application Layers, Caching Servers, Web Servers & Application Layers, Databases, Web Servers & Application Layers, Cluster Management, Web Proxies","Users experienced temporary downtime, increased save times, and elevated error rates across various services including search and maps.","Complete Service Outage, Elevated Response Times and Latencies, Increased Error Rates","Several services were not fully prepared for the switchover, including underprovisioning of resources in the codfw datacenter and failure to prewarm caches such as the Cirrus query cache.","Capacity Overloads, Configuration Errors","The issue was detected through multiple alerts such as purged cache alerts, elevated 500s in Varnish, and MariaDB read-only alerts. These were identified through Icinga monitoring and manual checks by SREs and volunteers.","Cache Layer Anomalies, Database Errors, Service Probes and Health Checks Failures"
88,2023-09-27_Kafka-jumbo_mirror-makers.wikitext,Wikidata Query Service Update Pipeline Incident,2023-09-26 20:00:00,2023-09-26 20:00:00,2023-09-27 14:05:00,Sev(3),"The Wikidata Query Service update pipeline was stuck, causing significant update lag but no data loss. The mirror-maker processes experienced crashes when the number of processes exceeded nine, causing replication issues.",18:05:00,00:00:00,18:05:00,"Wikidata Query Service,Kafka MirrorMaker","Wikidata Query Service, Other","main-eqiad_to_jumbo-eqiad,kafka-main1003,kafka-main1003","Networking Equipment, Databases","Minimal user facing outages. Wikidata Query Service update pipeline was stuck, leading to significant update lag on WDQS and potential delays in downstream pipeline processing, but no data loss expected.","Partial Service Outage, Elevated Response Times and Latencies",A bug in the version of Kafka and/or Mirrormaker that triggers when the number of mirrormaker instances pulling from kafka-main-eqiad to kafka-jumbo exceeds nine.,Software Bugs,"The issue was first detected by Icinga alerts such as 'Kafka MirrorMaker main-eqiad_to_jumbo-eqiad average message produce rate in last 30m on alert1001 is CRITICAL', and crashloop alerts on kafka-jumbo1001. Also observed by engineers on the #wikimedia-analytics IRC channel.","Service Probes and Health Checks Failures, High CPU & Resource Utilization"
89,2024-05-28_wikikube-api.wikitext,Issue with WikiKube Kubernetes Cluster API Servers,2024-05-28 12:23:00,2024-05-28 12:49:12,2024-05-28 17:00:00,Sev(3),"The incident was triggered by a failed scap deployment of MediaWiki, uncovering underlying network and CPU saturation issues in the WikiKube Kubernetes cluster API servers.",04:37:00,00:26:12,04:10:48,MediaWiki,MediaWiki Core Services,"WikiKube Kubernetes cluster,WikiKube Kubernetes cluster,OpenTelemetry collector","Containers & Kubernetes, Monitoring & Telemetry, Cluster Management","No impact for wiki users. Four MediaWiki deployments failed, and a global scap lock was put into place to prevent bad user experience for deployers.",Other,"CPU saturation in Kubernetes control planes, exacerbated by uneven network configurations and increased API traffic due to OpenTelemetry collector.","Capacity Overloads, Configuration Errors, Network Failures","The issue was first detected due to a failed MediaWiki deployment, with alerts firing for failed probes in the kubemaster servers (FIRING: ProbeDown: Service kubemaster1002:6443 has failed probes).",Service Probes and Health Checks Failures
90,2020-05-05_wdqs-deploy.wikitext,Wikidata Query Service Outage Due to Failed Deployment,2020-05-05 07:44:00,2020-05-05 09:43:00,2020-05-05 07:54:00,Sev(2),"On May 5th, 2020, all Wikidata Query Service instances were down for about 5 minutes due to a failed deployment caused by an incorrectly executed deployment procedure and a bug in the code.",00:10:00,01:59:00,22:11:00,"Wikidata Query Service,Wikidata Query Service","Wikidata Query Service, Database and Data Analytics","deployment procedure,WKTSerializer class,integration/unit tests,WDQS metrics",Other,"The exact number of affected queries is unknown, but the metrics did not show significant impact.",Other,"The root cause was a bug where the serialVersionUID field was removed from the WKTSerializer class, which was needed by Blazegraph but was not caught by standard integration/unit tests.",Software Bugs,"The issue was detected by icinga, which posted alerts on #wikimedia-operations indicating several systems were in a CRITICAL state due to HTTP 500 Server Errors and failed systemd state checks.","Appserver Issues, Service Probes and Health Checks Failures"
91,2021-09-29_eqiad-kubernetes.wikitext,Network Disruption in Kubernetes Cluster Due to Rolling Restart of Calico Pods,2021-09-29 13:46:00,2021-09-29 13:46:00,2021-09-29 13:48:00,Sev(3),"Network in the Kubernetes cluster was briefly disrupted due to a rolling restart of Calico pods, making all services in Kubernetes unreachable for 2 minutes.",00:02:00,00:00:00,00:02:00,"mediawiki,sessionstore","MediaWiki Core Services, Caching and Proxy Services","kubernetes,calico pods,calico pods,OCI imagefs,core routers (cr*-eqiad.wikimedia.org)","Containers & Kubernetes, Networking Equipment, Containers & Kubernetes, Other, Networking Equipment","For 2 minutes, fetches to MediaWiki resulted in higher error rates (up to 8.51% for POSTs) and elevated latencies for GETs (p99 at 3.82s). Some 1500 edits failed. Kafka logging lag increased temporarily.","Increased Error Rates, Elevated Response Times and Latencies","The incident occurred due to a race condition during the rolling restart of Calico pods in the kube-system namespace. The Calico Typha pod was restarted and scheduled on a node that didn't have the image, causing a delay. This led to the expiration of the graceful BGP timers on the core routers, making the pod networks unreachable.","Software Bugs, Configuration Errors",The issue was detected through elevated error rates and latencies in MediaWiki. Performance metrics indicated increased error rates (up to 8.51% for POSTs) and elevated latencies for GETs (p99 at 3.82s).,"Latency and Timeout Issues, User Reported Problems"
92,2020-05-28_commons_read-only.wikitext,s4 Primary Database Master Hardware Memory Issue,2023-10-12 20:21:00,2023-10-12 20:24:00,2023-10-12 20:29:00,Sev(2),"s4 primary database master (db1138) had a hardware memory issue, causing the mysqld process to crash, which returned as read-only for 8 minutes.",00:08:00,00:03:00,00:05:00,commonswiki,"MediaWiki Core Services, Media Storage and File Handling","mysqld process,memory DIMM,IDRAC",Other,commonswiki didn't accept writes for 8 minutes. Reads remained unaffected.,Partial Service Outage,Hardware crash on a memory DIMM.,Hardware Failures,"First signs of issues were logged on the host's IDRAC, followed by a MySQL read-only alert labeled as CRITICAL: read_only: True, expected False at 20:24 UTC.",Database Errors
93,2019-10-16_network_eqsin.wikitext,Equinix Singapore IXP Peer Flap Impacting BGP and OSPF Sessions,2019-10-16 17:15:00,2019-10-16 17:28:00,2019-10-16 17:29:00,Sev(2),"An Equinix Singapore IXP peer flapped heavily, overwhelming the routing daemon on cr1-eqsin and causing all its BGP and OSPF sessions to flap or go down.",00:14:00,00:13:00,00:01:00,"Varnish,Nginx,BFD,LVS HTTPS","Caching and Proxy Services, Caching and Proxy Services, Other, Caching and Proxy Services","eqsin PoP,eqsin PoP","Networking Equipment, Other",Estimated ~170k errors surfaced to users of eqsin PoP as local caches could not reach their peers in the main datacenters and served 500 errors.,"Cache Issues, Network Connectivity Problems, Increased Error Rates",The routing daemon on cr1-eqsin was overwhelmed due to heavy flapping of an Equinix Singapore IXP peer.,"Network Failures, Capacity Overloads","Detected via automated alerts including Varnish traffic drop, HTTP availability for Nginx and Varnish, BFD status on cr1-codfw, and LVS HTTPS text-lb.eqsin.wikimedia.org.",Network Connectivity Problems
94,2022-02-22_wdqs_updater_codfw.wikitext,WDQS Update Failure Due to Flink in Codfw,2022-02-22 17:47:00,2022-02-22 19:00:00,2022-02-22 19:27:00,Sev(3),WDQS updaters stopped processing updates in Codfw due to a failure with Flink in Codfw.,01:40:00,01:13:00,00:27:00,"Wikidata,Wikidata","Wikidata Query Service, MediaWiki Core Services","updateQueryServiceLag,updateQueryServiceLag,updateQueryServiceLag",Other,"For about two hours, WDQS updates failed to be processed. As a consequence, bots and tools were unable to edit Wikidata during this time.","API Failures, Partial Service Outage","The root cause was a poor implementation of the swift client protocol, which prevented Flink from starting properly in Kubernetes. Switching to an S3 client resolved the issue.",Software Bugs,"The issue was first detected when the Wikidata maxlag alert was triggered at 19:00, even though Codfw was depooled.","Latency and Timeout Issues, Service Probes and Health Checks Failures"
95,2022-02-06_wdqs_updater.wikitext,WDQS Updater Failure Incident,2022-02-06 23:00:00,2022-02-06 23:00:00,2022-02-07 06:20:00,Sev(2),"The streaming updater stopped functioning due to a misbehaving Kubernetes node, affecting WDQS updates for about 7 hours.",07:20:00,00:00:00,07:20:00,"Wikidata,Wikidata","Wikidata Query Service, MediaWiki Core Services","streaming updater,k8s,flink session cluster,k8s,k8s",Containers & Kubernetes,"For about 7 hours, WDQS updates failed to be processed. As a consequence, bots and tools were unable to edit Wikidata during this time.","Partial Service Outage, API Failures","A Kubernetes node (kubernetes1014) misbehaved, causing resource starvation and preventing the streaming updater from functioning properly.","Hardware Failures, Software Bugs","The issue was first detected through rising WDQS lag in eqiad, which caused edits to be throttled. Metrics from Flink and Kubernetes indicated discrepancies in task slots and active PODs.","Appserver Issues, Database Errors, Latency and Timeout Issues, High CPU & Resource Utilization"
96,2022-08-26_Phabricator_login_issues.wikitext,Phabricator Session Cookie Removal Incident,2022-08-26 09:56:00,2022-08-26 10:07:00,2022-08-26 10:13:00,Sev(2),"On August 26, 2022, a test change in cp6016 prevented session cookies from reaching the service's origin server, causing users to be logged out and unable to log back in for approximately 17 minutes.",00:17:00,00:11:00,00:06:00,"Wikimedia services,Wikimedia services",Other,"session cookies,ATS layer,cp6016",Other,"For approximately 17 minutes, some users accessing Wikimedia services experienced automatic logouts and were unable to log in, affecting logged actions. The most noticeable impact was on Phabricator.","Partial Service Outage, Security Incidents","An ATS bug/misdocumented feature caused session cookies to be wiped during the execution of specific hooks, preventing them from reaching the origin server.",Software Bugs,"The issue was first detected by several people on IRC in the SRE channel who noticed being logged out from Phabricator after any action, with no alerts being sent as it only affected logged-in users.",User Reported Problems
97,2021-06-29_trwikivoyage_primary_db.wikitext,Turkish Wikivoyage Outage due to Database Configuration Error,2021-06-29 14:22:00,2021-06-29 14:26:11,2021-06-29 14:30:00,Sev(2),"Shortly after the June 2021 switch over, the backend appservers were unable to serve requests to tr.wikivoyage.org for about 8 minutes from 14:22 UTC to 14:30 UTC. No other wikis were affected.",00:08:00,00:04:11,00:03:49,Turkish Wikivoyage,Other,"Backend Appservers,Infrastructure,Primary Database,Infrastructure","Web Servers & Application Layers, Caching Servers, Databases, Other","For 8 minutes, registered users on the Turkish Wikivoyage were consistently unable to load any pages or perform any actions. The general public may have noticed it to a lesser extent due to caching at our CDN layer, although any pages absent from the CDN cache would have also been temporarily unavailable.","Complete Service Outage, Cache Issues","The assignment for Codfw datacenter contained a typo, which caused MediaWiki to look for the trwikivoyage database in the s3 cluster instead of s5. This typo was not spotted in code review, and the configuration key was not subject to data validation, leading to unavailability when traffic was switched to Codfw.",Configuration Errors,"The issue was first detected through PHP Fatal Errors for backend requests to tr.wikivoyage.org, found through Logstash and detectable with alerts from the Prometheus metrics for error logs.","Appserver Issues, User Reported Problems"
98,2021-11-18_codfw_ipv6_network.wikitext,2021-11-18 Codfw IPv6 Network Incident,YYYY-MM-DD hh:mm:ss,YYYY-MM-DD hh:mm:ss,YYYY-MM-DD hh:mm:ss,Sev(2),"For 8 minutes, the Codfw cluster experienced partial loss of IPv6 connectivity for upload.wikimedia.org, with no visible user impact due to Happy Eyeballs.",00:00:00,00:00:00,00:00:00,upload.wikimedia.org,Media Storage and File Handling,"Codfw cluster,Codfw cluster,Codfw cluster,Codfw cluster,Codfw cluster,Junos (switch operating system)",Networking Equipment,"For 8 minutes, the Codfw cluster experienced partial loss of IPv6 connectivity for upload.wikimedia.org. The Codfw cluster generally serves Mexico and parts of the US and Canada. The upload.wikimedia.org service serves photos and other media/document files, such as displayed in Wikipedia articles.","Partial Service Outage, Network Connectivity Problems","The new switch installed to replace the one showing signs of disk failure was silently discarding IPv6 traffic, which was later identified as a Junos (switch operating system) bug that was resolved by forcing a virtual-chassis master failover.","Hardware Failures, Software Bugs","Monitoring triggered and the interface between asw-b7-codfw and cr2-codfw was disabled, forcing traffic through the cr1<->asw-b2-codfw link, resolving the upload-lb issue.",Network Connectivity Problems
99,2021-02-26_sudo.wikitext,Incident Involving Syntax Error in /etc/sudoers File Affecting Puppet and Nagios,2021-02-25 09:00:00,2021-02-25 08:51:00,2021-02-25 14:05:00,Sev(2),"A patch was merged and deployed to all hosts containing a syntax error in the /etc/sudoers file, causing sudo to not work and creating numerous alert emails and mail delivery delays.",05:05:00,23:51:00,05:14:00,"Puppet,Nagios,Mail Delivery",Other,"puppet-merge,puppet-merge,root@wikimedia.org Email,mx1001,mx1001",Other,"Users were unable to use sudo, Nagios alert execution was impacted, and there was a significant overload and delay in mail delivery.","Partial Service Outage, Elevated Response Times and Latencies",A syntax error in the /etc/sudoers file introduced by a new patch.,Configuration Errors,The issue was first detected when people reported on IRC that they were unable to use sudo due to a parse error in /etc/sudoers. Hundreds of alert emails started arriving due to sudo failures.,"Security Vulnerabilities, User Reported Problems"
100,2022-07-10_thumbor.wikitext,Thumbor Service Response Time Regression Incident,,,,Sev(2),"For several days, Thumbor p75 service response times gradually regressed by several seconds. Due to an upstream bug introduced in a firejail update, Thumbor constantly restarted itself, leading to increased error rates and delays from HAProxy.",00:00:00,00:00:00,00:00:00,"Thumbor,HAProxy","Media Storage and File Handling, Caching and Proxy Services","Thumbor,firejail",Other,"Thumbor p75 service response times regressed by several seconds, leading to increased error rates and delays, thus affecting the user experience negatively.","Elevated Response Times and Latencies, Increased Error Rates",An upstream bug introduced in a firejail update caused Thumbor to constantly restart itself.,Software Bugs,"The symptom details are not provided in the supplied incident report, but it notes an increased service response time and error rates.","Latency and Timeout Issues, User Reported Problems"
101,2020-07-03_Lilypond_down.wikitext,Lilypond Bypassing Security Restrictions Incident,,,,Sev(2),Lilypond bypassed security restrictions.,00:00:00,00:00:00,00:00:00,,Other,wmf-config settings,Other,Users may have faced vulnerabilities due to the bypassing of security restrictions.,Security Incidents,Lilypond was able to bypass existing security restrictions.,Security Misconfigurations,The issue was detected through internal security audits that identified the bypass.,Security Vulnerabilities
102,2019-09-13_maps.wikitext,Map Servers CPU Saturation Incident on September 13,2019-09-13 04:20:00,2019-09-13 04:26:00,2019-09-13 14:30:00,Sev(3),"On Friday, September 13, map servers experienced CPU saturation due to poorly formed requests, leading to partial unavailability of maps from ~4:30 UTC to ~14:30 UTC. The issue was resolved by validating traffic at the caching layer.",10:10:00,00:06:00,10:04:00,"maps,maps",Other,"maps servers,varnish,maps servers,tilerator,nginx","Web Servers & Application Layers, Caching Servers, Web Servers & Application Layers, Web Servers & Application Layers, Web Servers & Application Layers","Service was degraded for ~9 hours. Approximately 2% of requests were affected, although most users likely experienced some degradation due to the high number of tiles seen during a session.","Partial Service Outage, Elevated Response Times and Latencies","A bug was introduced when fixing linting issues to integrate the CI pipeline, causing a failure in the HTTP error handler. This made Kartotherian unable to validate request parameters, resulting in high CPU usage and timeouts.",Software Bugs,"HTTP availability for Varnish was flapping starting 4:26 UTC and worsening by 6:49 UTC. No page was sent, and there was no direct alert specifically pointing to maps/Kartotherian.","Appserver Issues, Cache Layer Anomalies, Network Connectivity Problems"
103,2021-09-04_appserver_latency.wikitext,Database Server Load Increase Causing Query Delays,2021-09-04 19:30:00,2021-09-04 19:30:00,2021-09-04 20:07:00,Sev(2),"An increase in load on a database server led to slower query responses, causing backend traffic to occupy appserver php-fpm workers longer, resulting in a proportion of requests failing with an error message.",00:37:00,00:00:00,00:37:00,"backend,backend,CDN cache","Other, API and Application Servers, Content Delivery Network (CDN) and Edge Cache","php-fpm,database server","Web Servers & Application Layers, Databases","For 37 minutes, backends were slow (taking several seconds to respond) and 2% of requests failed entirely, affecting logged-in users, most bots/API queries, and some page views from unregistered users for pages that were recently edited or otherwise expired from the CDN cache.","Elevated Response Times and Latencies, Increased Error Rates, Cache Issues",An increase in load on a database server.,Capacity Overloads,"Slowness was first detected when many queries were responding much slower, and failed requests displayed an error page with the message 'upstream connect error or disconnect/reset before headers. reset reason: overflow'.","Database Errors, Latency and Timeout Issues, Network Connectivity Problems"
104,2020-10-06_cloud-vps.wikitext,Sensitive Credentials Leak via Publicly Accessible Memcached Servers,2020-10-09 02:21:00,2020-10-09 02:21:00,2020-10-09 08:43:00,Sev(3),"Sensitive credentials (authentication tokens) were leaked via publicly accessible memcached servers, allowing potential privilege escalation within Cloud VPS/Toolforge.",06:22:00,00:00:00,06:22:00,"Cloud Services,Developer Tools",Cloud Services and Virtual Private Servers (VPS),"Keystone API,memcached servers",Other,"Tokens could be used to escalate privileges, potentially allowing malicious cloud users to manipulate the Keystone API and perform actions like creating and destroying arbitrary VMs within Cloud VPS.",Security Incidents,"Memcached servers were accessible on a default port and without authentication, leading to the exposure of sensitive credentials.",Security Misconfigurations,"The breach was first detected due to tokens being leaked through publicly accessible memcached servers. Access was subsequently blocked in eqiad and codfw, and Keystone auth tokens were manually rotated.",Security Vulnerabilities
105,2019-12-11_MachineVision%2Bcpjobqueue.wikitext,Job Queue Blocked by Image Annotation Requests (MachineVision),2019-12-11 20:39:00,2019-12-12 00:05:00,2019-12-17 01:25:00,Sev(3),"Between 2019-12-11 and 2019-12-17, the job queue was blocked by image annotation request jobs enqueued by the MachineVision extension, causing severe delays in job processing.",124:46:00,03:26:00,121:20:00,"Global renames,Global renames,Echo notifications,File uploads,MassMessages,MassMessages,MassMessages","MediaWiki Core Services, Other","MachineVision extension,job specification interface,job specification interface",Other,"This delayed the execution of a wide variety of tasks that rely on the job queue, including global renames, deleting translatable pages, Echo notifications, file uploads, recent changes processing, MassMessages, and pageview data publication.",Elevated Response Times and Latencies,"The use of the release timestamp feature by the MachineVision extension in the job specification interface, which is not well supported by the Kafka job queue, leading to blocking waits.","Software Bugs, Configuration Errors","The issue was first reported by user 1997kB in Phabricator concerning delayed global renames, followed by additional reports of delayed functionality. No automated alerts detected the issue.","User Reported Problems, Latency and Timeout Issues"
106,2020-07-14_termbox_and_wikifeeds_timeouts.wikitext,Incidents with MediaWiki API Virtual Hosts due to Unconditional HTTPS Redirects,2020-07-13 19:33:00,2020-07-14 06:03:00,2020-07-14 11:25:00,Sev(3),The deployment of unconditional HTTPS redirects led to HTTP 302 redirects causing timeouts in Termbox and Wikifeeds requests due to Kubernetes cluster restrictions.,15:52:00,10:30:00,05:22:00,"Termbox,Wikifeeds",API and Application Servers,"Kubernetes Clusters,Kubernetes Clusters,HTTPS edge","Web Servers & Application Layers, Containers & Kubernetes, Networking Equipment","During a ~15 hour outage, around 12% of Termbox requests failed with HTTP 500 (~186,000 requests) and approximately 1,250,000 Wikifeeds requests timed out, though user impact was minimized due to two layers of caching.","Partial Service Outage, Increased Error Rates, Elevated Response Times and Latencies","Unconditional HTTPS redirects deployed, causing HTTP 302 redirects for requests missing the 'X-Forwarded-Proto: https' header, leading to timeouts when attempting to contact MediaWiki via the edge due to Kubernetes cluster egress restrictions.",Configuration Errors,High latency in restbase recognized by an SRE and Icinga alerts indicating the Wikipedia mobile app main page was not loading.,"Latency and Timeout Issues, Service Probes and Health Checks Failures"
107,2023-01-30_kartotherian.wikitext,Kartotherian Service Outage due to Deployment Rollback Failure,2023-01-30 12:46:00,2023-01-30 12:27:00,2023-01-30 13:31:00,Sev(3),"An upgrade to Kartotherian caused spurious Icinga alerts, leading to a rollback that failed due to non-robust configuration templates, resulting in an outage of the maps service.",00:45:00,23:41:00,01:04:00,Maps,Other,"Kartotherian,deployment configuration templates","Web Servers & Application Layers, Code Deployment & Version Control",No maps were rendered during the outage.,Partial Service Outage,"Non-robustness in the Kartotherian deployment configuration templates, which failed when deploying without the '--env' flag.",Configuration Errors,"Icinga alerts for 'kartotherian endpoints health' in #wikimedia-operations, detecting unusual template variables in URLs and unexpected status codes. For example, an alert in Icinga reported: 'Test Untitled test returned the unexpected status 301 (expecting: 200)', among others.",Service Probes and Health Checks Failures
108,2022-08-16_x2_databases_replication_breakage.wikitext,MediaWiki MainStash Replication Break Incident,2022-08-16 04:07:58,2022-08-16 03:08:12,2022-08-16 04:43:22,Sev(2),"Replication broke between local MainStash databases, causing user-noticeable exceptions during edit-related activities due to a previously undiscovered bug in MediaWiki's handling of replica failures.",00:35:24,23:00:14,01:35:10,"MediaWiki,MainStash","MediaWiki Core Services, Caching and Proxy Services","x2 database section,MediaWiki shutdown handler,x2 database section,ChronologyProtector,cross-dc replication","Databases, Other","For 36 minutes, errors were noticeable for some editors during edit-related actions, although saving edits was unaffected. Approximately 1 hour of volatile cache data was lost.","Partial Service Outage, Cache Issues","A previously undiscovered bug in how MediaWiki handles failures of local MainStash replicas, combined with improper replication and ChronologyProtector configuration, led to user-noticeable errors.","Software Bugs, Configuration Errors","The issue was first detected through 2 alert pages sent as soon as internal replication broke on the primary x2 servers, prompting further pages about the replicas and user reports of errors during edit-related activities.","Database Errors, User Reported Problems"
109,2021-09-13_cirrussearch_restart.wikitext,Impact of Puppet Change on Cirrus Elasticsearch System,2021-09-13 16:29:00,2021-09-13 16:29:00,2021-09-13 18:34:00,Sev(3),"Applying a change to the cirrus elasticsearch systemd unit triggered puppet to restart elasticsearch services, leading to cluster instability and cascading failures. Full recovery took approximately 2 hours and 5 minutes.",02:05:00,00:00:00,02:05:00,"en.wikipedia.org,API opensearch",Other,"Elasticsearch,puppet-agent,Elasticsearch,PyBal,MediaWiki api_appserver,PoolCounter","Databases, Monitoring & Telemetry, Web Servers & Application Layers, Load Balancers",For about 2 hours (from 16:29 until 18:34) search requests on en.wikipedia.org (and likely other wikis) failed with 'An error has occurred while searching: Search is currently too busy. Please try again later.' Search suggestions from API opensearch were absent or delayed. The api_appserver cluster saw higher average latency due to the proportion of search queries.,"Partial Service Outage, Elevated Response Times and Latencies, Increased Error Rates, API Failures","A change in the cirrus elasticsearch systemd unit triggered puppet to restart the elasticsearch services on multiple hosts simultaneously, causing cluster instability and a cascading failure scenario.","Configuration Errors, Software Bugs","The issue was first detected with a 'PyBal backends health check' alert indicating critical issues with elasticsearch hosts, followed by an 'ElasticSearch health check' for shards showing critical status, and the 'production-search-codfw' cluster dropping into red status.","Service Probes and Health Checks Failures, Network Connectivity Problems, Cache Layer Anomalies"
110,2020-05-11_thumbor.wikitext,Thumbor Service Incident due to High Volume of Requests,2020-05-11 12:11:00,2020-05-11 12:47:00,2020-05-11 13:26:00,Sev(3),"An external client issued a large number of requests for nonexistent images on commons, causing the thumbor service to return 503 errors and triggering an alert.",01:15:00,00:36:00,00:39:00,thumbor,Media Storage and File Handling,"eqiad,esams,haproxy,swift,haproxy","Databases, Web Proxies, Caching Servers, Load Balancers, Networking Equipment","Approximately 291,000 image thumbnailing/resizing requests failed and were returned as 503 errors, affecting users primarily in Europe and the East Coast of the American Continents.","Partial Service Outage, Increased Error Rates","The per-IP throttling in Thumbor was inadequate, allowing a single client to overload the service by keeping many workers busy with excessive requests.","Capacity Overloads, Configuration Errors","The issue was detected 40 minutes after it began when icinga paged the team, alerting about the entirety of the service. Graphs showed a spike in 404 and 503 errors, and latencies increased significantly.","Latency and Timeout Issues, Service Probes and Health Checks Failures, User Reported Problems"
111,2020-07-23_wdqs-outage.wikitext,Wikidata Query Service Outage,2020-07-23 17:11:00,2020-07-23 17:24:00,2020-07-23 18:44:00,Sev(2),"Various Wikidata Query Service instances were down due to failing readiness probes, with errors in blazegraph logs that required a restart for resolution.",01:33:00,00:13:00,01:20:00,Wikidata Query Service,Wikidata Query Service,"blazegraph,wdqs_80,wdqs_80,wdqs_80,PyBal","Databases, Web Servers & Application Layers, Load Balancers",There was a period of a few minutes where there was a full outage; all WDQS queries failed to complete during this window. There was a more extended period of degraded service where a subset of queries would fail depending on which instance received the request.,"Complete Service Outage, Partial Service Outage","The incident was due to 'bad queries' that were too taxing on the system, causing individual instances of blazegraph to go offline. Restarts were required to temporarily resolve the issue until problematic queries could be identified and blocked.","Database Issues, Capacity Overloads","The issue was first detected when a critical alert was issued and communicated through IRC, indicating that too many nodes were down, affecting full availability. The initial critical alerts that fired included 'PyBal backends health check on lvs2009', 'PyBal IPVS diff check on lvs2010', and 'PyBal backends health check on lvs2010'.",Service Probes and Health Checks Failures
112,2024-05-31_Image_hotlinking.wikitext,Hotlinking Caused Link Saturation in EQSIN Datacentre,2024-05-31 10:47:04,2024-05-31 10:47:04,2024-05-31 12:39:53,Sev(2),Hotlinking of an image on Commons caused link saturation in the eqsin datacentre.,01:52:49,00:00:00,01:52:49,eqsin,Content Delivery Network (CDN) and Edge Cache,"port utilisation,FastNetMon,varnish frontend rule","Monitoring & Telemetry, Networking Equipment, Caching Servers",Increase in response times for users using eqsin.,Elevated Response Times and Latencies,Hotlinking of an image on Commons caused link saturation in the eqsin datacentre.,Unexpected Traffic Surges,"The issue was first detected via paging for port utilisation in eqsin: Primary outbound port utilisation over 80%. Additionally, FastNetMon detected what it perceived as a DDoS attack.","Network Connectivity Problems, Security Vulnerabilities"
113,2021-10-29_graphite.wikitext,Graphite Metrics Backfill Failure During Bullseye Migration,2021-10-21 10:24:00,2021-10-26 16:50:00,2021-12-13 09:12:00,Sev(3),"The backfill process for Graphite metrics silently failed during the Bullseye migration, causing a subset of metrics to experience data loss for data points before October 11th 2021.",1270:48:00,126:26:00,1144:22:00,"Graphite,Grafana",Monitoring and Logging,"whisper-sync backfill process,Graphite hosts (graphite2003,Graphite hosts (graphite2003","Other, Monitoring & Telemetry",A subset of metrics experienced loss of data points before October 11th 2021 due to the failed backfill process.,Data Corruption or Loss,The whisper-sync backfill process failed undetected during the Bullseye migration.,Software Bugs,"Some Grafana dashboards backed by Graphite showed partial data for a subset of metrics, as reported by Lucas Werkmeister.","Cache Layer Anomalies, User Reported Problems"
114,2023-09-28_mw-page-content-change-enrich.wikitext,Incident with Mediawiki Page Content Change Enrichment,2023-09-28 14:33:00,2023-09-28 14:33:00,2023-10-03 14:14:54,Sev(2),"The mw-page-content-change Flink streaming enrichment job failed to produce messages to Kafka due to ongoing Kafka partition reassignment, causing a backlog of delayed messages over several days.",119:41:54,00:00:00,119:41:54,Mediawiki Event Enrichment,Other,"Kafka,Flink,Flink,Kafka,Kafka",Other,"No messages were lost, but they were delayed by several days.",Elevated Response Times and Latencies,"Copying Kafka partitions to new brokers, which starved broker threads and caused the mw-page-content-change Flink streaming enrichment job to fail in producing messages.",Other,"The first detection of the issue was through the MediawikiPageContentChangeEnrichJobManagerNotRunning alert, indicating that the job manager was not running.",Appserver Issues
115,2023-05-02_Phabricator_outage_during_GitLab_maintenance.wikitext,Phabricator Tasks Unloadable during GitLab Maintenance,2023-05-02 08:14:00,2023-05-02 08:14:00,2023-05-02 08:24:00,Sev(2),"During planned GitLab maintenance, Phabricator tasks failed to load for about 10 minutes, displaying an 'Unhandled Exception ('RuntimeException')' error.",00:10:00,00:00:00,00:10:00,"Version Control Tools,Version Control Tools",Other,"GitLabPatchesCustomField,toolpilot,toolpilot",Other,For approximately 10 minutes Phabricator tasks were not loading when GitLab was in maintenance mode.,Partial Service Outage,A Phabricator widget deployed in T324149 caused the issue during the GitLab maintenance.,Software Bugs,"The issue was first detected by users reporting in the #wikimedia-operations IRC channel, with no automated alerts as pages were loading but displaying an error instead of expected contents.",User Reported Problems
116,2024-03-27_Elasticsearch_Omega_Cluster_Failure.wikitext,"CODFW Omega Cluster Incident on March 27, 2024",2024-03-27 21:45:03,2024-03-27 22:00:00,2024-03-27 23:59:00,Sev(2),"Users connecting to the CODFW omega cluster were unable to search wikis, and edits were not indexed due to operator error during decommissioning.",02:13:57,00:14:57,01:59:00,"Wikis Search,Wikis Search",Search and Indexing,"CODFW Omega Cluster,Puppet Patch,Elastic Search Master Servers","Databases, Code Deployment & Version Control, Databases","From Wed Mar 27 21:34:03 2024 to Wed Mar 27 23:59 2024, users connecting to the CODFW omega cluster (comprised of ~1600 smaller wikis) were unable to search these wikis. Edits to the pages during this time were not added to search indices. Larger wikis were not affected.","Partial Service Outage, Data Corruption or Loss","Operator error during the decommissioning of omega masters, including an unexpected cookbook behavior resulting in firewalling off master hosts.","Configuration Errors, Other","A 'master not discovered exception' (503) from the CODFW omega endpoint was noticed by Ryan Kemper, Search Platform SRE.","Service Probes and Health Checks Failures, Network Connectivity Problems"
117,2022-10-15_s6_master_failure.wikitext,Database Master Outage Due to Bad DIMM,2022-10-15 22:37:00,2022-10-15 22:40:00,2022-10-15 23:01:00,Sev(2),"The s6 master, db1131, went offline due to a bad DIMM and required a reboot, mariadb restart, and failover to db1173, resulting in a read-only state for frwiki, jawiki, ruwiki, and wikitech for 24 minutes.",00:24:00,00:03:00,00:21:00,"frwiki,frwiki,frwiki,wikitech",Other,"s6 master,db1131,db1131,db1131,db1131,db1131",Databases,"frwiki, jawiki, ruwiki, and wikitech were read-only for 24 minutes (starting Sunday 12:37 AM in France, 7:37 AM in Japan, and very early- to mid-morning in Russia).",Partial Service Outage,"The s6 master, db1131, experienced multi-bit memory errors in DIMM A6 causing the database server to go offline.",Hardware Failures,The issue was detected when a paging alert was received for 'Host db1131 #page is DOWN: PING CRITICAL - Packet loss = 100%'. Additional alerts followed for MariaDB replica connection errors.,"Network Connectivity Problems, Database Errors"
118,2019-12-31_search-api-traffic-block.wikitext,Wikimedia Search API Traffic Blocking Incident,2019-12-31 19:42:00,2020-01-01 00:43:00,2020-01-01 01:05:00,Sev(3),An accidental deployment blocked all enwiki search API traffic while attempting to block scraping from a specific User-Agent on AWS hosts.,05:23:00,05:01:00,00:22:00,en.wikipedia.org Search API,"Search and Indexing, API and Application Servers","Puppet,VCL configs,VCL configs","Caching Servers, Code Deployment & Version Control","HTTP 403 errors were returned for 59.1k queries over just under five hours, affecting bots and mobile app users accessing the en.wikipedia.org search API.","Increased Error Rates, API Failures",The change to block excessive scraping was mistakenly configured to block all enwiki search API traffic.,Configuration Errors,The issue was detected through a human report in #wikimedia-operations with no automated detection.,User Reported Problems
119,2023-05-05_prometheus_down_in_ulsfo_and_eqsin.wikitext,Prometheus Downtime Incident in ulsfo and eqsin,2023-05-05 00:27:00,2023-05-05 00:27:00,2023-05-05 08:15:00,Sev(3),"Two Prometheus instances were updated from Buster to Bullseye, resulting in an 8-hour downtime in two data centers due to corrupted files.",07:48:00,00:00:00,07:48:00,Prometheus,Monitoring and Logging,"prometheus4002,prometheus4002,WAL directory,WAL directory","Monitoring & Telemetry, Other","Prometheus was down in ulsfo and eqsin for 8 hours, causing loss of observability in two data centers.",Data Corruption or Loss,"Corrupted WAL and/or 'chunks_heads' directory after synchronization, potentially due to a race condition preventing graceful shutdown in the Buster host.","Software Bugs, Hardware Failures","Automated monitoring detected the alert, but a human noticed the outage and triaged it with the alert.",Service Probes and Health Checks Failures
120,2023-11-15_esams_unreachable.wikitext,ESAMS DC Unreachable Incident,2023-11-15 14:04:00,2023-11-15 14:05:00,2023-11-15 14:21:00,Sev(2),"ESAMS DC was unreachable by users, leading to a significant drop in request rate from ~150k req/s to ~91k req/s. The issue was attributed to a reboot of the wrong router linecard.",00:17:00,00:01:00,00:16:00,ESAMS DC,Content Delivery Network (CDN) and Edge Cache,"esams,ncredir,esams","Networking Equipment, Web Proxies, Load Balancers",Users trying to reach Amsterdam DC experienced network errors and delays.,Network Connectivity Problems,"An engineer accidentally restarted the linecard on the wrong router, causing the outage.",Configuration Errors,The outage was first detected by an SRE due to network errors and confirmed by alerts such as 'PROBLEM - Host ncredir3003 is DOWN: PING CRITICAL - Packet loss'.,Network Connectivity Problems
121,2022-05-02_deployment.wikitext,Incident T307349: Deployment Server Data Loss,2022-05-02 11:13:00,2022-05-02 11:13:00,2022-05-02 15:21:00,Sev(2),"For 4 hours, MediaWiki and other services could not be updated or deployed due to data loss on the active deployment server.",04:08:00,00:00:00,04:08:00,"MediaWiki,Other unspecified services",Other,"Deployment server (deploy1002),Deployment server (deploy1002)",Code Deployment & Version Control,"MediaWiki and other services experienced a halt in deployments for 4 hours, affecting updates and new deployments.",Other,"A wrong command `rm -rf` was executed on the active deployment server, causing loss of parts of the `/srv/deployment` directory.",Configuration Errors,"The issue was detected when deployments were halted, and it was identified that parts of the `/srv/deployment` directory were lost on the active deployment server (deploy1002).",Other
122,2021-10-25_s3_db_recentchanges_replica.wikitext,Incident on 2021-10-25 Involving S3 DB Recentchanges Replica,2021-10-25 18:25:00,2021-10-25 18:30:00,2021-10-25 19:06:00,Sev(2),"For ~30 minutes, average HTTP GET latency for mediawiki backends was higher than usual. For ~12 hours, database replicas of many wikis were stale for Wikimedia Cloud Services such as Toolforge.",00:41:00,00:05:00,00:36:00,"WikimediaCloudServices,WikimediaCloudServices,WikimediaCloudServices","Cloud Services and Virtual Private Servers (VPS), Other, MediaWiki Core Services","db1112.eqiad.wmnet,s3 replica,s3 replica",Databases,"For ~30 minutes, average HTTP GET latency for mediawiki backends was higher than usual. For ~12 hours, database replicas of many wikis were stale for Wikimedia Cloud Services such as Toolforge.","Elevated Response Times and Latencies, Storage and Database Failures","The s3 replica (db1112.eqiad.wmnet) that handles recentchanges/watchlist/contributions queries went down, causing confusion over its role and severity, which delayed the appropriate response.","Database Issues, Configuration Errors","The issue was first detected through an icinga alert for the host being down, and subsequently an alert for increased appserver latency on GET requests.","Service Probes and Health Checks Failures, Latency and Timeout Issues"
123,2022-03-27_wdqs_outage.wikitext,WDQS Service Deadlock Incident,2022-03-27 13:51:00,2022-03-27 14:05:03,2022-03-27 14:24:00,Sev(3),WDQS in Codfw entered a state of deadlock that persisted until service restarts were performed.,00:33:00,00:14:03,00:18:57,WDQS,Wikidata Query Service,"wdqs2002,wdqs2002,wdqs2002,wdqs2002,lvs2009,lvs2009",Other,"For 30 minutes, all WDQS queries failed due to an internal deadlock.",Complete Service Outage,An internal deadlock across multiple servers reduced capacity and caused all Codfw WDQS hosts to become unresponsive.,Software Bugs,"The issue was first detected with a critical alert from icinga-wm at 14:05:03 indicating a socket timeout for the LVS wdqs-ssl codfw port 443/tcp, leading to all WDQS queries failing.","Latency and Timeout Issues, Network Connectivity Problems, Service Probes and Health Checks Failures"
124,2023-02-23_PHP_worker_threads_exhaustion.wikitext,Incident: Database Load Issue on db1127,2023-02-23 17:17:00,2023-02-23 17:17:00,2023-02-23 17:42:00,Sev(2),"db1127 was unable to respond to simple statistics queries due to heavy load, compounded by a MariaDB query killer bug.",00:25:00,00:00:00,00:25:00,"api_appserver,php7.4-fpm.service,MediaWiki",API and Application Servers,"MariaDB,MariaDB,MariaDB,MariaDB","Databases, Monitoring & Telemetry, Web Servers & Application Layers, Databases",Minimal user-facing errors served.,Increased Error Rates,"db1127 was running MariaDB 10.6.10 which has a known bug where queries were not properly killed, exacerbated by the server being heavily loaded.","Database Issues, Software Bugs, Capacity Overloads","Automated alert via VictorOps: Critical: PHPFPMTooBusy api_appserver, indicating a saturation of php7.4-fpm.service workers.",High CPU & Resource Utilization
125,2023-09-20_Elasticsearch_unavailable.wikitext,On-wiki Search Unavailability Incident,2023-09-20 14:02:00,2023-09-20 14:08:00,2023-09-20 14:26:07,Sev(3),On-wiki search for all wikis was delayed or unavailable for a subset of end users due to a data center switch.,00:24:07,00:06:00,00:18:07,On-wiki search,Search and Indexing,"Elasticsearch,PoolCounter,Elasticsearch","Databases, Web Servers & Application Layers","For the duration of the incident, on-wiki search was delayed or unavailable for a subset of end users.","Partial Service Outage, Elevated Response Times and Latencies","The root cause was the restbase requests being sent as POST rather than GET, which forced routing to the primary datacenter (codfw) instead of the non-primary (eqiad).",Configuration Errors,"Monitoring caught the issue when the alert 'CirrusSearch codfw 95th percentile latency - more_like' fired, indicating high latency.",Latency and Timeout Issues
126,2020-02-24_esams.wikitext,Router Hardware Failure Causing Service Disruption,2020-02-24 13:52:02,2020-02-24 13:55:55,2020-02-24 14:36:00,Sev(2),A router hardware failure caused widespread issues across multiple regions due to a pre-existing lowered capacity to absorb failures.,00:43:58,00:03:53,00:40:05,"All services for users in Europe,Parts of Asia and Africa reaching via Amsterdam",Content Delivery Network (CDN) and Edge Cache,"cr2-esams,bast3004,MX480 routers",Networking Equipment,"All services were affected for all types of users for large portions of Europe and everyone reaching our projects via Amsterdam, including parts of Asia and Africa. Approximately 47 million queries were lost over an interval of about 20 minutes.","Complete Service Outage, Data Corruption or Loss, Network Connectivity Problems",The incident was caused by coinciding hardware failures in two different routers.,Hardware Failures,"The issue was first detected by humans, with Icinga alerting 1 minute later about cr2-esams, leading to pages being sent to multiple SREs.",Service Probes and Health Checks Failures
127,2021-10-08_network_provider.wikitext,2021-10-08 Network Provider Incident,2021-10-08 16:11:00,2021-10-08 16:11:00,2021-10-08 17:15:00,Sev(3),"For up to an hour, some regions experienced a partial connectivity outage. This primarily affected the US East Coast for ~13 minutes, and Russia for 1 hour.",01:04:00,00:00:00,01:04:00,"Gerrit Code Review,Gerrit Code Review",Other,"Network,Network,Network",Networking Equipment,A subset of readers and contributors from these regions were unable to reach any wiki projects. Services such as Phabricator and Gerrit Code Review were affected as well.,Network Connectivity Problems,The network malfunction was limited to one of many providers we use in the affected regions.,Network Failures,Around 16:11 UTC our (non-paging) monitoring and users reported connectivity issues to and from our Eqiad location. Traceroutes showed a routing loop in a provider's network.,"Network Connectivity Problems, User Reported Problems"
128,2021-11-10_cirrussearch_commonsfile_outage.wikitext,CirrusSearch Commons File Outage Incident Report,2021-11-10 14:00:00,2021-11-10 15:21:00,2021-11-10 16:32:00,Sev(3),"A high volume of search queries submitted by a developer caused the deletion and recreation of a critical index, leading to search failures on Wikimedia Commons and other wikis.",02:32:00,01:21:00,01:11:00,"Search results page on many wikis (except English Wikipedia),Search results page on many wikis (except English Wikipedia)",Search and Indexing,"Elasticsearch queries,Elasticsearch queries,Elasticsearch queries",Databases,"For about 2.5 hours, the Search results page was unavailable on many wikis (except for English Wikipedia). On Wikimedia Commons, the search suggestions feature was also unresponsive.",Partial Service Outage,"A developer submitted a high volume of search queries against the active production Cirrus cluster and later ran `vagrant provision` without properly closing the tunnel, causing the deletion and recreation of the `commonswiki_file_1623767607` index.",Configuration Errors,"The issue was first detected when users reported that searching for files on Commons was impossible. Multiple tickets were filed, the first at 15:21 UTC.",User Reported Problems
129,2022-05-09_exim-bdat-errors.wikitext,Email Rejection Incident - T307873,2022-05-04 01:28:00,2022-05-08 17:07:00,2022-05-09 16:40:00,Sev(2),"From May 4 to May 9, 2022, approximately 14,000 emails from Gmail users to wikimedia.org were rejected and returned to the senders due to an issue with Exim’s chunking support.",135:12:00,111:39:00,23:33:00,"Email,Email",Other,"SMTP,SMTP,SMTP",Other,"During five days, about 14,000 incoming emails from Gmail users to wikimedia.org were rejected and returned to sender.",Other,"Disabling chunking support in Exim, which caused it to reject emails with a '503 BDAT command used when CHUNKING not advertised' error.",Configuration Errors,"The issue was detected by users reporting bounced emails with a 503 error code. Though the messages were logged, the alerting system did not pick up these bounces.","User Reported Problems, Network Connectivity Problems"
130,2021-11-23_Core_Network_Routing.wikitext,2021-11-23 Core Network Routing Incident,2021-11-23 09:37:00,2021-11-23 09:39:00,2021-11-23 09:51:00,Sev(2),"For about 12 minutes, Eqiad was unable to reach hosts in other data centers (e.g., cache PoPs) via public IP addresses due to a BGP routing error. There was no impact on end-user traffic, and internal traffic generally uses local IP subnets that are routed with OSPF instead of BGP.",00:14:00,00:02:00,00:12:00,"Internal Traffic,Edge Caching Clusters (e.g.,Edge Caching Clusters (e.g.","Caching and Proxy Services, Content Delivery Network (CDN) and Edge Cache","cr1-eqiad,cr1-eqiad,BGP Routing",Networking Equipment,"For about 12 minutes, Eqiad was unable to reach hosts in other data centers (e.g., cache PoPs) via public IP addresses due to a BGP routing error. There was no impact on end-user traffic, and internal traffic generally uses local IP subnets that are routed with OSPF instead of BGP.",Network Connectivity Problems,"A change was made on cr1-eqiad and cr2-eqiad to influence route selection in BGP, which caused a routing loop as the core routers mis-evaluated the preferable routes to remote sites.",Configuration Errors,"Alerts fired at 09:39, the first being due to Icinga in Eqiad failing to reach the public text-lb address in Eqsin (Singapore).","Network Connectivity Problems, Service Probes and Health Checks Failures"
131,2020-01-26_app_server_latency.wikitext,Surge in Requests due to Kobe Bryant's Death Announcement,2020-01-26 19:50:00,2020-01-26 19:52:33,2020-01-26 21:04:00,Sev(3),"News of Kobe Bryant's death caused a surge in edits and page views, leading to high request loads and general unavailability of the MediaWiki application layer for over an hour.",01:14:00,00:02:33,01:11:27,"Application Servers,Application Servers,Cache Servers,Application Servers","MediaWiki Core Services, API and Application Servers, Caching and Proxy Services","Poolcounter,Network link,Mcrouters,Appserver cluster,Appserver cluster,Varnish-fes,Kartotherian","Caching Servers, Networking Equipment, Caching Servers, Cluster Management, Cluster Management, Caching Servers, Web Servers & Application Layers","1 hour of almost complete unavailability of the backend servers. A large number of requests to the sites timed out or got an error response, resulting in almost 90% of backend requests failing.","Complete Service Outage, Increased Error Rates","High contention in editing/parsing and saturation of the network link of two memcached servers due to high edit/reparse activity, leading to a spike in latencies and cache-misses for babel calls from the appservers.","Capacity Overloads, Network Failures","The issue was first detected by monitoring, triggering alerts at 19:52 UTC with problems reported for MediaWiki memcached error rate, high average GET latency for mw requests, and LVS HTTPS IPv4 on text-lb.codfw.wikimedia.org.","Cache Layer Anomalies, Latency and Timeout Issues, Network Connectivity Problems"
132,2020-09-09_mobileapps_config_change.wikitext,Faulty Configuration Deployment Impacting Mobile Apps Services,2023-10-11 08:40:00,2023-10-11 16:40:00,2023-10-11 20:20:00,Sev(2),"A faulty configuration was deployed for mobileapps at 08:40, causing mobile-html content to have broken CSS and JS links for pages regenerated during the day. The issue was reported at 16:40 and swiftly reverted, with all affected pages purged by 20:20.",11:40:00,08:00:00,03:40:00,mobileapps,API and Application Servers,mobile-html content,Web Servers & Application Layers,Users experienced broken CSS and JS links in mobile-html content for pages regenerated during the day.,Cache Issues,A faulty configuration was deployed during the reconfiguration of services to use the service proxy middleware for remote procedure calls.,Configuration Errors,"The issue was first detected and reported at 16:40, identified by broken CSS and JS links in mobile-html content for affected pages.","Cache Layer Anomalies, User Reported Problems"
133,2020-09-08_Rack_D_hosts_outage.wikitext,Power Cable Issue in ASW2-D3-Eqiad During PDU Upgrade,2023-10-09 13:34:00,2023-10-09 13:34:00,2023-10-09 15:02:00,Sev(3),"A power cable issue in asw2-d3-eqiad during a scheduled PDU upgrade caused hosts in racks D1, D3, and D4 to be unavailable from 13:34 UTC to 15:02 UTC. The switch rebooted into a different firmware image which then had to be upgraded to restore normal operations.",01:28:00,00:00:00,01:28:00,"Wikifeeds,Horizon,dns1002,stat1005,stat1005",Other,"webrequest,EventLogging,bacula,ferm,EX switches","Monitoring & Telemetry, Monitoring & Telemetry, Other, Networking Equipment, Networking Equipment",Wikifeeds was not working properly; Horizon unavailability broke the creation and updating of WMCS images; dns1002 flapped causing sporadic DNS lookup failures for eqiad hosts; logins failed to stat1005 and stat1006. Kafka-jumbo1006 unavailability led to some data loss for webrequest and EventLogging topics; errors and failures in some bacula full backups; ferm was broken on about 30 hosts causing manual restarts.,"Complete Service Outage, Partial Service Outage, Network Connectivity Problems, Data Corruption or Loss, Storage and Database Failures","Asw-d3-eqiad rebooted into a different boot partition with a different JunOS version, causing it to be 'inactive' in the VCF.","Configuration Errors, Hardware Failures","Hosts in racks D1, D3, and D4 became unavailable; alerted by host unavailability and multiple service disruptions including DNS lookup failures, login failures, and data loss.","Network Connectivity Problems, Service Probes and Health Checks Failures, User Reported Problems"
134,2022-11-15_asw1-eqsin.wikitext,Outage Report for APAC Region Due to Juniper Bug,2022-11-15 04:51:00,,2022-11-15 04:56:00,Sev(2),"A new server connection to eqsin top of rack switches triggered a Juniper bug, causing a process to be killed and interrupting traffic, including a Virtual-Chassis master switchover, leading to a 5-minute outage.",00:05:00,00:00:00,00:00:00,eqsin,"Caching and Proxy Services, Content Delivery Network (CDN) and Edge Cache","Juniper switch,Juniper switch",Networking Equipment,"For about 5 minutes, users in the APAC region (using eqsin) could have been served 5xx errors instead of the requested page.","Partial Service Outage, Increased Error Rates",Connecting a new server to the eqsin top of rack switches triggered a Juniper bug which caused traffic interruptions and a Virtual-Chassis master switchover.,"Network Failures, Hardware Failures, Software Bugs","Automated monitoring detected an incident due to 5xx error rates. No pages were made despite user impact, and no task was documented during the outage.","Appserver Issues, Database Errors, Latency and Timeout Issues"
135,2022-11-04_Swift_issues.wikitext,Swift Service Availability Issues Affecting Commons/Multimedia,2022-11-04 14:32:00,2022-11-04 14:32:00,2022-11-04 15:21:00,Sev(3),"For 49 minutes, the Swift/mediawiki file backend returned errors for both reads and new uploads.",00:49:00,00:00:00,00:49:00,"Application Frameworks,Utilities,File Handling & Transfer",Media Storage and File Handling,"file backend,thumbor,swift-proxy,ms-fe1010,ms-fe1010,ms-fe1010,ms-fe1010","Caching Servers, Web Servers & Application Layers",Swift has reduced service availability affecting Commons/multimedia.,Partial Service Outage,Incomplete or outdated documentation for Swift led to delayed problem resolution.,Other,The issue was detected automatically and the engineers On Call received a page from Splunk on Call. Alerts such as ProbeDown firing and Swift https backend on ms-fe1010 being CRITICAL due to a socket timeout were observed.,"Service Probes and Health Checks Failures, Latency and Timeout Issues, Network Connectivity Problems"
136,2019-08-20_logstash.wikitext,Logstash Outage Incident Review,2019-01-01 23:10:00,2019-01-01 23:24:00,2019-01-01 23:40:00,Sev(2),"For about 30 minutes, Logstash was not receiving any messages from the MediaWiki servers.",00:30:00,00:14:00,00:16:00,"MediaWiki,MediaWiki","MediaWiki Core Services, Monitoring and Logging","Logstash-Kafka consumer,Logstash-Kafka consumer,Logstash-Kafka consumer",Monitoring & Telemetry,"During the Logstash outage, operational monitoring was impaired, and developers were unable to use WikimediaDebug or deploy new code for MediaWiki and most other services. There was no direct impact on end-users of public services. Logs were recovered when Logstash was restarted.","Partial Service Outage, Elevated Response Times and Latencies, Increased Error Rates",Logstash consumer failed and does not recover by itself.,Software Bugs,The issue was first detected by Icinga alerts indicating too many messages in Kafka logging-eqiad.,"Service Probes and Health Checks Failures, High CPU & Resource Utilization"
137,2023-01-17_MediaWiki.wikitext,MediaWiki Crash During Backport Deployment,2023-01-17 18:31:00,2023-01-17 18:34:51,2023-01-17 18:48:00,Sev(2),An issue with inconsistent state of deployed code during a backport deployment caused MediaWiki to crash for all logged-in page views.,00:17:00,00:03:51,00:13:09,All wikis,MediaWiki Core Services,"MediaWiki,php-fpm,MediaWiki,MediaWiki",Web Servers & Application Layers,"For roughly 15 minutes, all wikis were unreachable for logged-in users and non-cached pages.",Complete Service Outage,MediaWiki re-reading and applying changes to extension.json before a php-fpm restart could pick up changes to the PHP code.,"Configuration Errors, Software Bugs",First automated alert: <+icinga-wm> PROBLEM - MediaWiki exceptions and fatals per minute for appserver on alert1001 is CRITICAL: 2.651e+04 gt 100,Appserver Issues
138,2022-02-22_vrts.wikitext,Stuck VRTS Aliases Generating Process on mx2001 Resulted in Email Rejects,2022-02-22 08:00:00,2022-02-22 13:29:00,2022-02-22 16:47:00,Sev(2),"A stuck vrts aliases generating process on mx2001 resulted in rejects for dcw@wikimedia.org, a new VRTS queue, causing incoming emails to bounce with an SMTP 550 Error for 12 hours.",08:47:00,05:29:00,03:18:00,VRTS Queue Email Processing,Other,"generate_otrs_aliases.service,mx1001,mx1001",Other,"For 12 hours, incoming emails to a specific new VRTS queue were not processed with senders receiving a bounce with an SMTP 550 Error. It is estimated no 'useful' emails were lost.","Partial Service Outage, Increased Error Rates",The generate_otrs_aliases.service systemd timer job was stuck and was not updating VRTS mailing lists/queues on mx2001.,Software Bugs,An SRE received an email from a VRTS admin stating that the dcw@wikimedia.org queue returned SMTP 550 errors. The error message was '208.80.153.45 does not like recipient. Remote host said: 550 Previous (cached) callout verification failure'.,"User Reported Problems, Cache Layer Anomalies"
139,2020-09-25_s5_replication_lag.wikitext,Database Replication Issue in Multiple Wikis,2020-09-25 11:55:00,2020-09-25 11:42:00,2020-09-25 12:25:00,Sev(3),"A number of s5 database replicas in the Eqiad and Codfw data centres alerted about replication being stopped due to a duplicate key, leading to watchlist and recentchanges delays.",00:30:00,23:47:00,00:43:00,"watchlist,recentchanges",MediaWiki Core Services,"s5 database,ipblocks",Databases,Editors of wikis hosted in the s5 section saw stale data in recentchanges and watchlist interfaces.,Cache Issues,"The incident was likely caused by a drift that was either there and touched by an inserted row or generated by MediaWiki through an unsafe statement like INSERT IGNORE, leading to duplicate key errors.","Database Issues, Software Bugs",Automated monitoring caught the issue and SRE was paged about stopped replication in a number of s5 replicas. The alerts were appropriate and pointed out the issue accurately.,Database Errors
140,2019-10-31_wikidata.wikitext,Wikidata API Call Timeouts Due to DB Read Load,2023-10-17 16:00:00,2023-10-17 16:38:00,2023-10-17 17:29:00,Sev(2),Wikidata API calls were getting timeouts due to increased DB read load caused by backported changes intended to reduce deadlocks around writing to the new terms store for Wikibase.,01:29:00,00:38:00,00:51:00,"Wikidata API,Wikidata API","API and Application Servers, Other","Wikibase,DB read operations,Wikibase",Databases,"Wikidata editors experienced timeouts to API requests, and API response times for writes increased significantly. Most edits initiated from API calls were made successfully, but clients did not receive confirmation responses.","Elevated Response Times and Latencies, API Failures","Backported changes intended to reduce deadlocks around writing to the new terms store caused an increased DB read load, resulting in timeouts for Wikidata API calls.","Database Issues, Configuration Errors",The issue was first detected by humans in a Telegram chat and was later confirmed by Addshore.,User Reported Problems
141,2023-01-17_asw-b2-codfw_failure_redux.wikitext,Codfw Row B Network Connectivity Incident,2023-01-17 13:00:00,2023-01-17 12:49:00,2023-01-17 17:17:00,Sev(3),All hosts in codfw row B lost network connectivity due to issues triggered by the prep work to bring a replacement switch online and a subsequent operator error.,04:17:00,23:49:00,04:28:00,"geoip,generic-map,restbase-async,mobileapps,swift,swift,thanos-query,recommendation-api,etcd,cassandra","Other, Other, API and Application Servers, API and Application Servers, Media Storage and File Handling, Media Storage and File Handling, Database and Data Analytics, API and Application Servers, Caching and Proxy Services, Database and Data Analytics","network switches,IPv6 connectivity,network switches,core routers",Networking Equipment,All hosts in codfw row B lost network connectivity. End-user impact is to be determined.,Network Connectivity Problems,"Two separate Junos bugs: one triggered by the prep work for a replacement switch, causing instability in connectivity between different switches, and another triggered by an operator error breaking IPv6 connectivity for the whole row.","Network Failures, Software Bugs, Configuration Errors",The issue was first detected at 12:49 UTC when LibreNMS alerts fired for a virtual-chassis crash on device asw-b-codfw.mgmt.codfw.wmnet.,"Network Connectivity Problems, Service Probes and Health Checks Failures"
142,2022-05-24_Failed_Apache_restart.wikitext,Incident Involving Puppet Change and Apache Restart,2023-01-01 11:45:00,2023-01-01 11:45:00,2023-01-01 12:16:00,Sev(2),"A Puppet change caused an Apache restart issue, leading to a small number of 502 HTTP errors for users and multiple IRC alerts.",00:31:00,00:00:00,00:31:00,"kibana7,Apache HTTP,network monitoring,MW app servers","Monitoring and Logging, API and Application Servers","apache2.service,apache2.service,puppet,cumin","Web Servers & Application Layers, Web Servers & Application Layers, Code Deployment & Version Control, Cluster Management",A very small amount of 502 HTTP errors for users (predominantly logged-in users). Plus some 140 IRC alerts for a subset of hosts running Apache.,"Increased Error Rates, API Failures",A Puppet change made Apache listen on port 443 regardless of whether mod_ssl was enabled.,Configuration Errors,"The issue was first detected by the engineer who merged the patch, followed by multiple IRC alerts and pager notifications.",User Reported Problems
143,2022-06-10_overload_varnish_haproxy.wikitext,Disruption in Wiki Traffic Across Multiple Regions,2022-06-10 14:57:00,2022-06-10 14:57:00,2022-06-10 15:00:00,Sev(2),"For 3 minutes, wiki traffic was disrupted in multiple regions at the CDN layer, leading to HTTP 5xx error responses for both cached and logged-in responses.",00:03:00,00:00:00,00:03:00,wiki traffic,Content Delivery Network (CDN) and Edge Cache,"CDN layer,CDN layer,HAProxy","Web Proxies, Caching Servers, Load Balancers",Users experienced service disruptions and HTTP 5xx error responses while trying to access wiki pages.,"Partial Service Outage, Increased Error Rates","Ongoing rate-limiting caused lots of HTTP 429 from Varnish, which resulted in an overload at the HAProxy level and general service disruption.",Capacity Overloads,"HTTP 503 errors were reported on Phabricator, with alerts triggered due to Varnish overload.","Cache Layer Anomalies, High CPU & Resource Utilization"
144,2022-08-24_swift.wikitext,Media Request Failures Resulting in Numerous HTTP Errors,TODO 00:00:00,TODO 00:00:00,TODO 00:06:00,Sev(2),"For approximately 17 hours, media requests (mostly thumbnail reads, but also uploads) failed at a rate of about 2000 HTTP errors per minute, causing significant disruption.",00:00:00,00:00:00,00:00:00,"upload.wikimedia.org,upload.wikimedia.org","Media Storage and File Handling, Other","media,media",Other,"Approximately 1.9 million uncached media requests to upload.wikimedia.org failed along with 609 unsuccessful upload attempts, which was particularly noticeable to Commons contributors.","Cache Issues, API Failures",The root cause is not specified in the provided incident report draft.,Other,The issue was detected due to media requests failing at a high rate of 2000 HTTP errors per minute.,"Latency and Timeout Issues, Network Connectivity Problems"
145,2020-01-08_mw-api.wikitext,MediaWiki EventBus TLS Configuration Incident,2020-01-08 15:06:00,2020-01-08 15:08:00,2020-01-08 16:00:00,Sev(3),"A configuration change making EventBus use TLS for eventgate-analytics led to POST requests from MediaWiki application servers timing out, causing HTTP server errors and widespread disruptions in all data centers.",00:54:00,00:02:00,00:52:00,"MediaWiki,EventBus,ATS,Varnish","MediaWiki Core Services, API and Application Servers, Caching and Proxy Services, Caching and Proxy Services","mediawiki.api-request,mediawiki.api-request,ATS-tls,varnish-fe","Web Servers & Application Layers, Web Servers & Application Layers, Web Proxies, Caching Servers","Between 15:06 and 15:32, ATS backend requests to application servers resulted in 3K to 7K 5xx errors per second. Many user requests received 502 error responses or no response at all. Additionally, analytics events in the mediawiki.api-request and mediawiki.cirrussearch-request streams were lost.","Partial Service Outage, Increased Error Rates, Data Corruption or Loss","The change to make EventBus use TLS for eventgate-analytics led to application server requests timing out, causing cascading failures across the stack.","TLS/SSL Issues, Configuration Errors","The issue was first detected by various Icinga PHP7 rendering alerts on IRC, followed by multiple pages about api.svc.eqiad.wmnet socket timeouts, ATS TLS, and Varnish reduced availability.","Latency and Timeout Issues, Service Probes and Health Checks Failures, Cache Layer Anomalies"
146,2023-06-06 dawiktionary dawiki svwiktionary partial outage.wikitext,"Partial Outage Impacting dawiktionary, dawiki, and svwiktionary",2023-06-06 00:30:00,2023-06-06 01:35:00,2023-06-06 03:49:00,Sev(3),"All users of dawiktionary, dawiki, and svwiktionary sometimes received fatal exceptions while browsing due to a data issue related to a data migration implementation bug.",03:19:00,01:05:00,02:14:00,"dawiktionary,dawiktionary,dawiktionary",Other,"object cache,object cache,data migration",Other,"All users of dawiktionary, dawiki, and svwiktionary sometimes received fatal exceptions while browsing.",Increased Error Rates,"A data migration implementation bug in how bash interprets double-quotes ("") characters caused a data issue that entered the object cache.",Software Bugs,The issue was first detected by a volunteer who noticed errors and reported them via IRC. Another volunteer escalated the issue to SRE via Klaxon.,User Reported Problems
147,2022-03-04_esams_availability_banner_sampling.wikitext,Wiki Unreachability Due to CentralNotice Banner Deployment,2022-03-04 09:18:00,2022-03-04 09:18:00,2022-03-04 10:47:53,Sev(3),"A banner deployed via CentralNotice with a 100% sampling rate caused instabilities in the traffic layer, making wikis largely unreachable in Europe and intermittently affecting other global regions.",01:29:53,00:00:00,01:29:53,"wikis,eventgate-analytics-external",Other,"Varnish,CentralNotice,Esams datacenter,Esams datacenter","Web Proxies, Caching Servers, Other, Other","For 1.5 hours, wikis were largely unreachable from Europe (via Esams) with shorter and more limited impact across the globe via other data centers.","Partial Service Outage, Network Connectivity Problems","A banner was deployed with a 100% sampling rate for event instrumentation, causing a large amount of incoming traffic for event beacons that overwhelmed the Varnish system.",Capacity Overloads,"The issue was first detected through instabilities at the traffic layer, where connections for event beacons piled up, leading to Varnish being unable to handle traffic. Initial impact was observed in the Esams datacenter.","Network Connectivity Problems, Cache Layer Anomalies"
148,2022-12-18_World_Cup.wikitext,API Outage Due to DiscussionTools Extension and Increased Traffic,2022-12-18 17:58:00,2022-12-18 17:59:00,2022-12-18 18:22:00,Sev(3),"For approximately 24 minutes, uncached calls to the API on the eqiad datacenter overloaded the application servers, causing unreasonable latency or failing to respond to requests. Elevated latencies persisted for the following 3 hours as traffic load organically went down.",00:24:00,00:01:00,00:23:00,"API,Parsoid,Frontend (HAProxy,Frontend (HAProxy","API and Application Servers, Caching and Proxy Services","MediaWiki Application Servers,DiscussionTools MediaWiki Extension,DiscussionTools MediaWiki Extension",Web Servers & Application Layers,"All API users experienced 5XX errors (11-12 thousand errors per second) or unreasonable latencies for 24 minutes. After that, there was degraded performance with increased but more reasonable latency for around 3 hours.","API Failures, Elevated Response Times and Latencies, Increased Error Rates","The DiscussionTools MediaWiki extension added a ResourceLoader module on almost all page views, creating an API call. Combined with a significant 50% increase in overall traffic, this led to an overload and increased latencies on the API cluster.","Capacity Overloads, Unexpected Traffic Surges","Monitoring and paging worked as intended, triggering alerts such as PHPFPMTooBusy and FrontendUnavailable at around 17:59 UTC.","Appserver Issues, High CPU & Resource Utilization"
149,2020-04-07_Wikidata%27s_wb_items_per_site_table_dropped.wikitext,Wikidata wb_items_per_site Table Incident Review,2020-04-06 23:02:00,2020-04-06 23:03:57,2020-04-06 23:26:00,Sev(3),"A misconfigured cron script led to the dropping of Wikidata's `wb_items_per_site` table, causing significant disruptions to page renders and edits for 20 minutes, and ongoing metadata issues for 19 hours.",00:24:00,00:01:57,00:22:03,"MediaWiki,MediaWiki","MediaWiki Core Services, Wikidata Query Service","wb_items_per_site table,update.php,update.php,cron/wikibase/dumpwikibaserdf.sh,wb_items_per_site table","Databases, Code Deployment & Version Control","Almost all content page parses were broken for 20 minutes, showing database error messages. This disruption affected many page views for logged-in users and many edits. For 19 hours, users encountered incorrect metadata, and hundreds of duplicate items in Wikidata.","Complete Service Outage, Data Corruption or Loss","Wikidata's `wb_items_per_site` table was dropped by a misconfigured weekly cron script running `update.php`, which had been misconfigured for eight years to drop this table.",Configuration Errors,First detected by a user report indicating a database error at 23:03:09 UTC and followed by an automated alert at 23:05:33 UTC reporting MediaWiki exceptions and fatals per minute reached critical levels.,"User Reported Problems, Database Errors"
150,2023-03-09_mailman.wikitext,Mailman Outage Due to Network Maintenance,2023-03-07 14:35:00,2023-03-07 14:43:00,2023-03-09 23:34:00,Sev(2),"During network maintenance, the Mailman runner process for delivering outgoing emails crashed and was not automatically restarted. Mailman queued outgoing emails for two days until the issue was resolved.",56:59:00,00:08:00,56:51:00,"Mailing Lists,Mailing Lists,Mailing Lists,Mailing Lists",Other,"Mailman runner process,MariaDB database server,Mailman runner process,network switches at eqiad row A,Mailman runner process","Other, Databases, Other, Networking Equipment, Other","For 2 days, Mailman did not deliver any mail, affecting public and private community, affiliate, and WMF mailing lists.","Partial Service Outage, Elevated Response Times and Latencies",The Mailman runner process crashed due to being unable to connect to the MariaDB database server and was not automatically restarted.,"Database Issues, Software Bugs","Automated monitoring through icinga detected the issue with an alert: '<+icinga-wm> PROBLEM - mailman3_runners on lists1001 is CRITICAL: PROCS CRITICAL: 13 processes with UID = 38 (list), regex args /usr/lib/mailman3/bin/runner' but it was not noticed by humans until a Gerrit Reviewer Bot report almost 2 days later.",Service Probes and Health Checks Failures
151,2023-05-30_Unintentional_%2B2_on_a_config_patch_without_deployment.wikitext,Incident: Accidental +2 on mediawiki-config Patch,2023-05-30 10:00:00,2023-05-30 10:00:00,2023-05-30 10:01:00,Sev(2),"A patch to mediawiki-config was accidentally +2'd and not immediately deployed, which could have led to confusion for a future deployer.",00:01:00,00:00:00,00:01:00,mediawiki-config,MediaWiki Core Services,"Patch Deployment,Gerrit",Other,A patch to mediawiki-config was accidentally +2'd and not immediately deployed.,Other,"A patch was accidentally +2'd without intent to deploy, similar to how one might on a software repo rather than a config repo.",Configuration Errors,The issue was first detected when https://gerrit.wikimedia.org/r/c/operations/mediawiki-config/+/923650 was +2'd and then reverted by an observer within a minute.,Other
152,2019-09-23_s3_primary_db_master_crash.wikitext,S3 Primary Database Master Outage,2019-09-22 18:40:38,2019-09-22 18:42:45,2019-09-22 19:16:00,Sev(3),s3 primary database master experienced a RAID backup battery failure causing the host to crash and requiring a power cycle from the iDRAC.,00:35:22,00:02:07,00:33:15,s3 wikis,Cloud Services and Virtual Private Servers (VPS),"db1075,RAID backup battery","Databases, Other",All the s3 wikis went read-only as the master wasn't available for writes. Reads were not affected as all the replicas were available.,Partial Service Outage,"The master lost its BBU, resulting in a complete host crash, an issue previously encountered with HP hosts.",Hardware Failures,"The issue was first detected when db1075 reported HOST DOWN. Alerts were sent to IRC and pages were issued, and users reported issues on #wikimedia-operations.","Database Errors, User Reported Problems"
153,2020-06-04_cloud-private-repo.wikitext,Incident: Change Deployed to Puppet Causes Secrets Deletion,2020-06-04 10:12:00,2020-06-04 12:37:00,2020-06-04 16:40:00,Sev(2),"A change deployed to puppet inadvertently deleted the private repo from all puppet backend and standalone servers, causing the deletion of all secrets and necessitating manual restoration.",06:28:00,02:25:00,04:03:00,"Cloud Services,Puppet Backend Servers,Puppet Backend Servers","Cloud Services and Virtual Private Servers (VPS), Other","labs/private repo,tools-puppetmaster-02,integration/deployment-prep,labs/private repo,tools-puppetmaster-02,docker,acme-chief,k8s/kubeadm","Monitoring & Telemetry, Caching Servers, Cluster Management, Code Deployment & Version Control, Databases, Containers & Kubernetes",Any cloud environments that had added private secrets reverted to using the dummy secrets in the labs/private repo.,Security Incidents,"The change was merged without realizing it would remove the private repo from the puppet backend and standalone servers, and the engineer performing the change was unaware of the importance of the private repository.","Configuration Errors, Other",The issue was first detected by a member of the Cloud Services team when they noticed missing data in the tools private repo and inquired about recent changes.,User Reported Problems
154,2023-02-22_wiki_outage.wikitext,HAProxy Upgrade Issue on Cache Hosts,2023-02-22 09:18:00,,2023-02-22 09:45:00,Sev(3),"During routine maintenance to upgrade HAProxy on cache hosts, an accidental depooling of backends caused significant traffic to fail, resulting in errors for clients.",00:27:00,00:00:00,00:00:00,"Phabricator,Phabricator,Grafana",Other,"varnish-text,varnish-text,varnish-text,varnish-text,varnish-text",Caching Servers,"For approximately 18 minutes, around 17% of incoming non-multimedia Wikimedia traffic received 503 or 500 errors or were missing, primarily affecting users in Europe, Africa, and Asia.","Partial Service Outage, Increased Error Rates, Network Connectivity Problems",A mismatch during the maintenance run between depooling hosts individually and pooling back the CDN led to all ATS backends in the text cache cluster being accidentally depooled.,Configuration Errors,"Automated alerts/page fired (FrontendUnavailable), including alerts for cache_text, varnish-text, and multiple ProbeDown incidents.","Service Probes and Health Checks Failures, Cache Layer Anomalies"
155,2022-07-11_FrontendUnavailable_cache_text.wikitext,MediaWiki API Cluster Latency and Error Incident,2022-07-11 19:30:00,2022-07-11 19:30:00,2022-07-11 19:36:00,Sev(2),"For 5 minutes, the MediaWiki API cluster in eqiad responded with higher latencies or errors to clients.",00:06:00,00:00:00,00:06:00,MediaWiki API,API and Application Servers,"PHP workers,database host (db1132),database host (db1132)","Databases, Web Servers & Application Layers",Users experienced increased latency and errors when accessing the MediaWiki API cluster.,"Elevated Response Times and Latencies, Increased Error Rates, API Failures","An increase in requests to the API cluster reduced the available PHP workers and exhausted connections on a MariaDB 10.6 database host (db1132), which is sensitive to high load.","Unexpected Traffic Surges, Capacity Overloads",The issue was first detected through an increase in latency and errors returned to clients. No specific alert details were provided.,"Latency and Timeout Issues, User Reported Problems"
156,2022-05-31_Analytics_Hadoop_failure.wikitext,Hadoop HDFS NameNode Failure Incident,2022-05-31 17:09:10,2022-05-31 17:09:10,2022-05-31 18:00:00,Sev(2),A failure in the Hadoop HDFS NameNode on both the primary and standby servers caused all HDFS writes and reads to fail. This incident was resolved without any lasting impact or data loss.,00:50:50,00:00:00,00:50:50,"Big Data Services,Big Data Services,Big Data Services,Data Visualization Tools,Data Visualization Tools,Big Data Services,Ingestion Services",Database and Data Analytics,"Hadoop HDFS NameNode,Hadoop HDFS NameNode,Hadoop HDFS NameNode,Hadoop HDFS NameNode",Databases,"There was no public-facing impact. Users within the WMF data teams who use tools such as Hive, Spark, Jupyter, Superset, etc., were affected. Several scheduled ingestion pipelines that read from or write to HDFS were also disrupted during the incident.","Partial Service Outage, Storage and Database Failures","The incident was caused by the primary and standby NameNodes crashing due to the `/var/lib/hadoop/journal` directories on all journal nodes becoming completely filled. This was traced back to a backup job issue where the standby NameNode was unable to save its image, causing journal nodes to fill up their partitions over time.","Capacity Overloads, Software Bugs",The issue was first detected through an email alert received at Tue May 31 17:09:10 UTC 2022 with the subject 'At least one Hadoop HDFS NameNode is active is CRITICAL'.,Service Probes and Health Checks Failures
157,2022-07-12_codfw_A5_powercycle.wikitext,Power Outage During PDU Maintenance in Rack A5,2022-07-12 15:45:00,2022-07-12 15:45:00,2022-07-12 16:00:00,Sev(2),"During the scheduled maintenance to upgrade the PDUs in rack A5, CyrusOne mistakenly flipped the incorrect breaker, resulting in all servers in rack A5 losing power to both primary and secondary power feeds. The issue was resolved once the breaker was flipped back on.",00:15:00,00:00:00,00:15:00,"Ganeti VMs,MariaDB,Graphite","Cloud Services and Virtual Private Servers (VPS), Database and Data Analytics, Monitoring and Logging","PDU,graphite2003,maps2005,db2079,db2079","Other, Monitoring & Telemetry, Web Servers & Application Layers, Databases, Databases","No apparent user-facing impact, but lots of internal clean up, e.g. for Ganeti VMs.",Other,"CyrusOne flipped the incorrect breaker on the breaker panel during scheduled maintenance to upgrade the PDUs, causing a loss of power to all servers in rack A5.",Hardware Failures,"The issue was first detected by multiple alerts indicating critical packet loss and database connectivity issues:
* 15:45 <+icinga-wm> PROBLEM - Host graphite2003 #page is DOWN: PING CRITICAL - Packet loss = 100%
* 15:45 <+icinga-wm> PROBLEM - Host maps2005 is DOWN: PING CRITICAL - Packet loss = 100%
* 15:55 <+icinga-wm> PROBLEM - MariaDB read only s8 #page on db2079 is CRITICAL: Could not connect to localhost:3306
* 15:56 <+icinga-wm> PROBLEM - MariaDB read only m1 #page on db2132 is CRITICAL: Could not connect to localhost:3306
* 16:00 <+icinga-wm> RECOVERY - MariaDB read only s8 #page on db2079 is OK","Network Connectivity Problems, Database Errors"
158,2021-09-18_appserver_latency.wikitext,Slow Database Queries Resulting in php-fpm Worker Exhaustion,2021-09-05 00:00:00,2021-09-05 00:00:00,2021-09-05 00:10:00,Sev(2),Slow database queries resulted in php-fpm worker exhaustion.,00:10:00,00:00:00,00:10:00,"General Services,General Services,General Services",Other,"php-fpm,CDN Cache","Web Servers & Application Layers, Caching Servers","For about 10 minutes, backends were slow or unavailable for all wikis. This affected logged-in users, most bots/API queries, and some page views from unregistered users (pages that were recently edited or otherwise expired from the CDN cache).","Partial Service Outage, Elevated Response Times and Latencies",Slow database queries,Database Issues,Issue was first detected due to php-fpm worker exhaustion.,Appserver Issues
159,2019-08-02_api-overload-parsoid.wikitext,"API Appserver Cluster Parsoid-Batch Calls Overwhelm (Aug 5, 2019)",2019-08-05 07:00:00,2019-08-05 07:00:00,2019-08-05 15:34:00,Sev(2),The api-appserver cluster experienced brownouts and frontend availability dropped to ~97% due to expensive parsoid-batch calls from certain pages.,08:34:00,00:00:00,08:34:00,"API,services,API,API,services,services","API and Application Servers, Other","api-appserver cluster,parsoid-batch requests,mw12[23].*",Web Servers & Application Layers,"The mediawiki API was heavily impacted, affecting internal services and external API users. A notable slowdown led to a decrease in request rates.","Elevated Response Times and Latencies, API Failures","API appservers were overwhelmed by a storm of long-running parsoid-batch requests, particularly from external requests, which exhausted the cluster's resources.","Capacity Overloads, Unexpected Traffic Surges","High CPU load alerts for appservers were triggered on IRC during brownouts from 6-12 UTC, followed by widespread alerts for affected internal systems starting around 15 UTC.",High CPU & Resource Utilization
160,2021-04-29_db_and_memc_load.wikitext,Incident: mediawiki-BagOStuff Refactor Resulting in Increased DB Load,2021-04-29 13:00:00,2021-04-29 13:41:00,2021-04-29 18:00:00,Sev(2),"On 14 April, a bug was introduced during a refactor of mediawiki-BagOStuff that caused revision text blobs to no longer be cached in Memcached, leading to an increased load on ExternalStore DB servers. This led to numerous HTTP 5xx errors during a spike in backend traffic on 29 April.",05:00:00,00:41:00,04:19:00,"Appserver,ExternalStore DB,Memcached,MediaWiki","API and Application Servers, Database and Data Analytics, Caching and Proxy Services, MediaWiki Core Services","mediawiki,sql,cache,appserver,mediawiki",Caching Servers,"During a period of five hours (13:00-18:00 UTC) on 29 April, there were 18 distinct minutes where the error rate was severely elevated to 1-10% of backend requests responding with errors.","Elevated Response Times and Latencies, Increased Error Rates","A bug in the refactored mediawiki-BagOStuff code caused the revision text service to stop caching data in Memcached, resulting in increased load on ExternalStore DB servers.","Software Bugs, Database Issues",The issue was first detected as numerous brief spikes where many requests were served HTTP 50x errors due to MediaWiki encountering DBConnectionError: Too many connections.,Database Errors
161,2022-08-10_confd_all_hosts.wikitext,Puppet Patch Incident leading to Confd Installations,2022-08-10 10:32:00,2022-08-10 10:44:00,2022-08-10 13:31:00,Sev(2),"A Puppet patch was merged which inadvertently installed confd on numerous production hosts, causing provisioning failures and triggering an Icinga alert.",02:59:00,00:12:00,02:47:00,"Puppet,Icinga",Monitoring and Logging,"confd,cumin",Other,No external impact.,Other,"The Puppet provisioning for confd installations failed due to missing corresponding Icinga checks, which was caused by the accidental merge of a Puppet patch.",Configuration Errors,The issue was detected by an Icinga alert indicating widespread Puppet agent failures: '10:44 <icinga-wm> PROBLEM - Widespread puppet agent failures on alert1001 is CRITICAL: 0.1057 ge 0.01'.,Service Probes and Health Checks Failures
162,2021-12-03_mx.wikitext,Delayed Email Delivery Incident on 2021-12-03,2021-12-03 21:11:00,2021-12-02 10:24:02,2021-12-04 01:32:00,Sev(3),"For about 24 hours, a portion of outgoing email from wikimedia.org was delayed in delivery affecting staff Gmail and Znuny/OTRS/Phabricator notifications. No mail was lost; it was eventually delivered.",04:21:00,-11:13:02,39:07:58,"staff Gmail,OTRS,OTRS,Phabricator notifications",Other,"mail servers,Linux kernel,iptables conntrack module,monitoring alerts,iptables conntrack module",Other,A portion of outgoing email from wikimedia.org was delayed in delivery affecting staff Gmail and Znuny/OTRS/Phabricator notifications. No mail was lost; it was eventually delivered.,"Partial Service Outage, Elevated Response Times and Latencies","Due to a kernel bug in the iptables conntrack module, packets from our mail server towards Google mail servers started to be dropped, causing outgoing mail to become stuck in the outgoing queue.","Software Bugs, Network Failures",Icinga detected an increased mail queue size growing over the alerting threshold and notified IRC. Users also reported issues to ITS.,"Service Probes and Health Checks Failures, User Reported Problems"
163,2022-03-31_api_errors.wikitext,API Server Latency and Errors Due to Exhaustive DB Queries,2022-03-31 05:18:00,2022-03-31 05:18:00,2022-03-31 05:40:00,Sev(2),"A code change caused the CentralAuth GlobalUsersPager class to produce expensive DB queries, leading to resource exhaustion on s7 database replicas. The issue was resolved by manually killing the slow queries.",00:22:00,00:00:00,00:22:00,"API server,API server",API and Application Servers,"API Gateway,GlobalUsersPager class,CentralAuth,s7 database replicas,PHP-FPM workers","Load Balancers, Web Servers & Application Layers, Other, Databases, Web Servers & Application Layers","For 22 minutes, API server and app server availability were slightly decreased (~0.1% errors, all for s7-hosted wikis such as Spanish Wikipedia), and the latency of API servers was elevated.","Partial Service Outage, Elevated Response Times and Latencies, Increased Error Rates","After a code change, the GlobalUsersPager class in CentralAuth produced expensive DB queries that exhausted resources on s7 database replicas.","Database Issues, Software Bugs","Backpressure from the databases tied up PHP-FPM workers on the API servers, triggering a paging alert for worker saturation.","Database Errors, High CPU & Resource Utilization"
164,2020-06-11_sessionstore%2Bkubernetes.wikitext,Sessionstore Service Outage - Impact on MediaWiki Edits,2020-06-11 18:36:00,2020-06-11 18:41:00,2020-06-11 19:20:00,Sev(4),"The sessionstore service experienced an outage due to insufficient capacity to handle a sudden surge in requests to MediaWiki, affecting users' ability to submit edits while logged in.",00:44:00,00:05:00,00:39:00,"sessionstore,mediawiki,eventgate_analytics_cluster,eventgate_analytics_cluster,eventgate_main_cluster,mathoid_cluster,cxserver,restbase,echo store,wikifeeds,citoid","MediaWiki Core Services,Other,API and Application Servers,Other,API and Application Servers,Other,Other,API and Application Servers,Database and Data Analytics,API and Application Servers,API and Application Servers","kubernetes1001,kubernetes1001,kubernetes1001,kubernetes1001,Liotash,PyBal,helmfile,logstash,Prometheus,icinga,grafana,LVS,LVS,linux/systemd","Containers & Kubernetes, Containers & Kubernetes, Containers & Kubernetes, Containers & Kubernetes, Other, Load Balancers, Containers & Kubernetes, Monitoring & Telemetry, Monitoring & Telemetry, Monitoring & Telemetry, Load Balancers, Load Balancers, Other","Logged-in users were unable to edit, login, or logout of all wikis across all geographic regions. Readers were unaffected.",Complete Service Outage,Insufficient capacity in the sessionstore service to handle a sudden increase in requests.,"Capacity Overloads, Unexpected Traffic Surges","Automated monitoring detected the issue, with the first alert indicating reduced availability for several clusters and a paging alert for sessionstore.svc.eqiad.wmnet (LVS socket timeout).","Latency and Timeout Issues, Service Probes and Health Checks Failures"
165,2020-01-27_app_server_latency.wikitext,External Demand Impact on MW API Query and Recommendation API Service,2023-10-17 10:30:00,2023-10-17 10:35:00,2023-10-17 11:10:00,Sev(2),"External demand for an expensive MW API query caused slower response times for the MW API web servers, leading to timeouts. The Recommendation API service was partially unavailable for 35-40 minutes.",00:40:00,00:05:00,00:35:00,"Recommendation API service,MediaWiki API",API and Application Servers,"MW API web servers,RESTBase backend errors,MW API web servers","Web Servers & Application Layers, Caching Servers, Monitoring & Telemetry",Users experienced partial unavailability of the Recommendation API service for about 35-40 minutes.,"Partial Service Outage, API Failures",High external demand for an expensive MW API query caused overall slower responses from MW API web servers.,Unexpected Traffic Surges,The issue was first detected through high RESTBase backend errors and MediaWiki errors & timeouts alerts observed in Grafana and Logstash.,"Latency and Timeout Issues, Appserver Issues"
166,2022-03-01_ulsfo_network.wikitext,Network Connectivity Loss at San Francisco Datacenter,2022-03-01 22:35:25,2022-03-01 22:35:25,2022-03-01 22:55:00,Sev(2),"Multiple of our redundant network providers for the San Francisco datacenter simultaneously experienced connectivity loss. After 20 minutes, clients were rerouted to other datacenters.",00:19:35,00:00:00,00:19:35,"New Zealand client routing,New Zealand client routing,New Zealand client routing",Content Delivery Network (CDN) and Edge Cache,"Network providers,San Francisco datacenter,Network providers",Other,"For 20 minutes, clients normally routed to Ulsfo were unable to reach any of our projects. This includes New Zealand, parts of Canada, and the United States west coast.",Network Connectivity Problems,Multiple of our redundant network providers for the San Francisco datacenter simultaneously experienced connectivity loss.,Network Failures,"The issue was first detected when clients normally routed to Ulsfo were unable to reach any of our projects, which triggered alerts related to connectivity loss.","Network Connectivity Problems, Service Probes and Health Checks Failures"
167,2023-08-09_mw-on-k8s_outage_due_to_wrong_tls_cert.wikitext,Mediawiki Kubernetes Deployment Certificate Issue,2023-08-09 11:41:00,2023-08-09 11:46:00,2023-08-09 12:16:00,Sev(2),"A change to the global k8s defaults was merged that caused the next MediaWiki on Kubernetes deployment to pick up a wrong certificate for TLS termination, leading to an outage.",00:35:00,00:05:00,00:30:00,"Mediawiki web,Mediawiki web","MediaWiki Core Services, API and Application Servers","k8s,cert-manager,mw-api,mw-api","Containers & Kubernetes, Web Servers & Application Layers",For approximately 30 minutes 1% of traffic received errors (120 requests/s).,"Increased Error Rates, Partial Service Outage",A change to the global k8s defaults caused MediaWiki deployments to use a wrong certificate for TLS termination.,"Configuration Errors, TLS/SSL Issues",The issue was first detected by 'ProbeDown' alerts indicating that Service mw-api-ext:4447 and mw-api-int:4446 had failed probes.,Service Probes and Health Checks Failures
168,2021-09-26_appserver_latency.wikitext,Increased DB Load Resulting in Slower Responses and PHP-FPM Worker Limits Reached,2023-10-12 14:10:00,2023-10-12 14:15:00,2023-10-12 14:25:00,Sev(2),"Increased db load for enwiki (s1) resulted in slower responses, leading to php-fpm worker limits being reached, affecting requests for all wikis.",00:15:00,00:05:00,00:10:00,"all wikis,all wikis",Other,"db,php-fpm,backend appservers,CDN cache","Databases, Web Servers & Application Layers, Web Servers & Application Layers, Caching Servers","For about 15 minutes, backend appservers were slower or unable to respond for all wikis, mainly affecting logged-in users and most bot/API queries. Some page views from unregistered users were affected for pages that were recently edited or otherwise expired from the CDN cache.","Partial Service Outage, Elevated Response Times and Latencies, Increased Error Rates","Increased db load for enwiki (s1) led to slower responses, resulting in php-fpm worker limits being reached.","Database Issues, Capacity Overloads",The issue was first detected through errors stating 'upstream connect error or disconnect/reset before headers. reset reason: overflow'.,"Network Connectivity Problems, Service Probes and Health Checks Failures"
169,2021-10-22_eqiad_return_path_timeouts.wikitext,Eqiad Return Path Timeouts,2021-10-22 20:06:00,2021-10-22 20:09:00,2021-10-22 20:57:00,Sev(3),"For ~40 minutes (from 20:06 to 20:48 UTC), clients users who geographically reach us via Eqiad had trouble connecting and received timeout errors. Recovery included temporarily depooling Eqiad to route clients to a different datacenter and re-pooling after issue resolution.",00:51:00,00:03:00,00:48:00,"wikis,CDN,Phabricator,Gerrit","MediaWiki Core Services, Content Delivery Network (CDN) and Edge Cache, Other, Other","Eqiad datacenter,Network routing,IRC alerts,VictorOps pager",Other,"For ~40 minutes (from 20:06 to 20:48 UTC), clients users who geographically reach us via Eqiad had trouble connecting and received timeout errors. Based on traffic dashboards, we lost about 7K requests/second.","Partial Service Outage, Network Connectivity Problems","A return path issue after datacenter maintenance at Eqiad, compounded by concurrent networking maintenance, led to connectivity problems. Only partial alerts were received due to outbound path issues affecting our external alert vendor.","Network Failures, External Service Failures","Detected by Icinga reporting interfaces down and timeouts reported by users on Discord. IRC alerts indicated too high incoming rate of browser-reported Network Error Logging events, but VictorOps pager did not receive the alert due to outbound path issues.","Network Connectivity Problems, Latency and Timeout Issues, User Reported Problems"
170,2024-04-26_Search_unavailable_for_some_eqiad_users.wikitext,Search Platform Service Outage - Incomplete Search Results,2024-04-24 19:27:00,2024-04-25 03:13:00,2024-04-26 13:31:00,Sev(3),"During the outage window, users connected to the primary (eqiad) datacenter may have had incomplete search results, messages saying 'try your search again,' and/or a lack of autocomplete.",42:04:00,07:46:00,34:18:00,"Search Platform,Search Platform",Search and Indexing,"elastic1105,elastic1105,LVS load-balancers,mwmaint1002,eqiad Datacenter","Databases, Load Balancers, Web Servers & Application Layers, Other","Users connected to the primary (eqiad) datacenter experienced incomplete search results, error messages, and missing autocomplete functionality.","Partial Service Outage, Increased Error Rates","An omission in the VLAN configuration for newly provisioned racks in the eqiad datacenter. Specifically, the new VLANs assigned for the racks were not configured on the LVS load balancers, causing connectivity issues to new Elasticsearch hosts.",Configuration Errors,Humans reported an error via a Phabricator task (phab:T363516).,User Reported Problems
171,2020-02-07_mediawiki_API_down.wikitext,Incident with Bot Scraping zhwiki Causing API Unresponsiveness,2020-02-07 14:06:00,2020-02-07 14:06:40,2020-02-07 14:17:00,Sev(3),"A bot scraping zhwiki started making more expensive requests more aggressively, bypassing parser cache and using slow templates, causing significant issues.",00:11:00,00:00:40,00:10:20,"API,Application Servers",API and Application Servers,"Parsercache,Parsercache,Templates","Web Servers & Application Layers, Caching Servers",API became almost unresponsive for about 10 minutes. Application servers were unresponsive for another 10 minutes shortly after.,"Elevated Response Times and Latencies, API Failures","A bot making expensive requests bypassed parser cache and used slow templates, leading to high server load.","Capacity Overloads, Unexpected Traffic Surges","Detected by a series of critical alerts in Icinga: MediaWiki exceptions and fatals per minute, high average POST and GET latency, and Apache HTTP being critical.","Appserver Issues, Latency and Timeout Issues, Service Probes and Health Checks Failures"
172,2021-11-04_large_file_upload_timeouts.wikitext,Incident Report: MediaWiki Large File Upload Failure,2021-10-28 00:00:00,2021-10-28 00:00:00,2021-11-02 00:00:00,Sev(3),"For 9 months, editors were unable to upload large files on Wikimedia Commons due to timeouts caused by HTTP/2 usage in libcurl. Forcing HTTP/1 resolved the issue.",120:00:00,00:00:00,120:00:00,"MediaWiki,MediaWiki,MediaWiki","MediaWiki Core Services, Media Storage and File Handling","MediaWiki appservers,Swift,libcurl,Nginx","Web Servers & Application Layers, Caching Servers, Other, Web Servers & Application Layers","Editors were entirely unable to upload large files, leading to user frustration and a poorer experience as they gave up or reduced upload quality. Additional sysadmin time was required for manual server-side upload requests.",Partial Service Outage,"The libcurl upgrade enabled HTTP/2 by default, which is slower for large file uploads than HTTP/1.1 or HTTP/1.0. This caused timeouts when uploading files to Swift cross-datacenter.",Software Bugs,The issue was first detected due to database query errors on the MediaWiki side caused by timeouts in MediaWiki/Swift communication. Initial manual investigation confirmed the timeouts.,"Latency and Timeout Issues, Database Errors"
173,2020-02-07_wikidata.wikitext,Wikibase Client Configuration Incident,2020-02-07 14:38:00,2020-02-07 14:40:00,2020-02-07 14:45:00,Sev(3),"A configuration typo in Wikibase clients caused group 0 and 1 wikis to read from unmigrated rows, leading to missing data. Fixing the typo resulted in an inordinate load on servers and an outage.",00:07:00,00:02:00,00:05:00,Wikis,MediaWiki Core Services,"Wikibase Client,Wikibase Client,InitializeSettings.php,wb term store",Other,"Wikis were unavailable for users via eqiad for about 8 minutes, although cached pages accessed via other sites should have been available.","Complete Service Outage, Cache Issues","A typo in the configuration setting for Wikibase clients prevented it from being passed through from InitialiseSettings.php, causing the wrong data store to be read from.",Configuration Errors,"Icinga alerted immediately, starting with MediaWiki exceptions and fatals per minute. The icinga-wm bot flooded out of the IRC channel due to too many reports.","Service Probes and Health Checks Failures, User Reported Problems"
174,2020-03-19_parsercache.wikitext,Parsercache Database Overload Incident,2020-03-16 18:00:00,2020-03-16 18:43:14,2020-03-16 19:24:00,Sev(2),"Parsercache databases were overloaded due to a malfunctioning host, causing increased latency and connection spikes across the active hosts, impacting mwapps server performance.",01:24:00,00:43:14,00:40:46,"latency,mwapps","Monitoring and Logging, API and Application Servers","parsercache databases,mw app servers,pc1008,pc1008,pc1008,pc1008",Caching Servers,"Query latency increased, mw app servers' workers were saturated, and higher than usual response times were observed.",Elevated Response Times and Latencies,"A malfunctioning host (pc1008) caused connections to pile up, leading to increased latency and performance degradation in other active hosts due to parsercache's 'double write' behavior.",Hardware Failures,Icinga paged for pc1008 host performance degradation at 18:43:14 with a 'CRITICAL slave_sql_state could not connect' alert.,"Database Errors, Network Connectivity Problems"
175,2022-09-08_codfw_appservers_degradation.wikitext,Nginx Server Restart Causing Etcdmirror Outage,2022-09-08 15:18:18,2022-09-08 15:18:18,2022-09-08 15:51:18,Sev(3),"An Nginx server restart triggered an etcdmirror outage that affected end-users during a MediaWiki deployment, causing various servers in Codfw to be in a degraded state.",00:33:00,00:00:00,00:33:00,"appserver,appserver,parsoid,php-fpm",API and Application Servers,"Others,nginx,scap,pybal,poolcounter","Containers & Kubernetes, Web Servers & Application Layers, Code Deployment & Version Control, Load Balancers, Caching Servers","For 2 minutes, appserver and api_appserver in Codfw were in a degraded state. For 16 minutes, parsoid in Codfw was in a degraded state.","Partial Service Outage, Elevated Response Times and Latencies","An Nginx server restart caused etcdmirror to crash, leading to php-fpm being unable to contact its configuration server, which in turn failed to restart during the MediaWiki deployment. This caused the affected servers to be depooled.",Configuration Errors,"Errors were reported during scap sync-file by claime, and icinga-wm alerted that the etcdmirror-conftool-eqiad-wmnet service on conf2005 was in a CRITICAL state.","Appserver Issues, Service Probes and Health Checks Failures"
176,2022-08-16_Beta_Cluster_502.wikitext,Outage in Beta Cluster Affecting All Sites,2022-08-16 17:57:00,2022-08-16 17:57:00,2022-08-17 00:57:00,Sev(2),"An inadvertent restart of some WMCS cloudvirts and their associated VMs caused all sites within the Beta Cluster to fail to load, displaying 'Error: 502, Next Hop Connection Failed'. Restarting trafficserver resolved the issue.",07:00:00,00:00:00,07:00:00,"Beta Cluster,Selenium Test Jobs",Other,"Virtual Machines (VMs),Virtual Machines (VMs),Trafficserver,Apache Config,Puppet","Other, Other, Caching Servers, Web Servers & Application Layers, Code Deployment & Version Control","For 7 hours, all Beta Cluster sites were unavailable, affecting daily Selenium test jobs.",Complete Service Outage,"Inadvertent restart of some WMCS cloudvirts and their associated VMs, likely compounded by an Apache config/puppet failure.","Configuration Errors, Other","Detected by an uptime monitor (https://uptime.theresnotime.io/status/wmf-beta) that paged the responder, user reports, and CI errors from beta sync.","Service Probes and Health Checks Failures, User Reported Problems, Other"
177,2023-05-05_wdqs_not_updating_in_codfw.wikitext,Stale Data Issue in WDQS Due to RDF Streaming Updater Instability,2023-05-04 10:00:00,2023-05-05 19:08:00,2023-05-10 10:30:00,Sev(3),"The rdf-streaming-updater application in CODFW became unstable and stopped sending updates, resulting in stale data for users connecting through CODFW.",144:30:00,33:08:00,111:22:00,"WCQS,WCQS",Wikidata Query Service,"rdf-streaming-updater,taskmanager,k8s cluster,rocksdb,taskmanager","Databases, Containers & Kubernetes, Cluster Management",End users accessing WDQS from the CODFW region received stale results.,Cache Issues,"The streaming updater flink job stopped functioning in CODFW due to potential resource constraints, specifically caused by the issues mentioned in FLINK-22597 and improper RocksDB resource compaction.","Software Bugs, Capacity Overloads","The issue was first detected by Prometheus alerts for the WCQS cluster, which fired starting at 2023-05-04T1030. WDQS cluster alerts started a bit later at 2023-05-05T1908, with alert subjects including 'RdfStreamingUpdaterFlinkJobUnstable' and 'RdfStreamingUpdaterHighConsumerUpdateLag'.",Service Probes and Health Checks Failures
178,2022-04-06_esams_network.wikitext,Wikis Slow or Unreachable Due to AMS-IX Outage,2023-01-15 08:20:00,,2023-01-15 08:50:00,Sev(2),"For 30 minutes, wikis were slow or unreachable for a portion of clients to the Esams data center due to an issue at the Amsterdam Internet Exchange.",00:30:00,00:00:00,00:00:00,wikis,MediaWiki Core Services,"Esams data center,Esams data center",Networking Equipment,"Users in Europe, Middle-East, and Africa experienced slow or unreachable wikis while accessing the Esams data center.","Partial Service Outage, Network Connectivity Problems",An issue at the Amsterdam Internet Exchange (AMS-IX).,"Network Failures, External Service Failures",Users reported slowness or inability to reach wikis for a duration of 30 minutes.,"Latency and Timeout Issues, User Reported Problems"
179,2023-02-22_read_only.wikitext,Editing Disabled Site-Wide Due to Read-Only Mode Activation,2023-02-22 11:03:25,2023-02-22 11:39:00,2023-02-22 12:18:48,Sev(3),"During a live switchover test, an existing logical bug set the secondary datacenter into read-only mode which led to an editing outage for users in codfw and mobile users. An attempt to fix it caused all datacenters to be set in read-only mode, leading to a major editing outage.",01:15:23,00:35:35,00:39:48,"MediaWiki,MediaWiki",MediaWiki Core Services,"datacenter,datacenter",Other,"For approximately 2 minutes, editing was disabled site-wide. For approximately 54 minutes, editing failed for some users in the codfw datacenter (around 1-2% of all edits).","Complete Service Outage, Partial Service Outage","A logical bug in the switchover test script unexpectedly set the secondary datacenter to read-only mode, and subsequent manual attempts to fix the bug used incorrect data types causing all datacenters to enter read-only mode.","Software Bugs, Configuration Errors",The issue was first detected through user reports on #wikimedia-tech channel and a MediaWikiHighErrorRate alert when the full read-only mode issue occurred.,"User Reported Problems, Service Probes and Health Checks Failures"
180,2023-01-14_asw-b2-codfw_failure.wikitext,Switch Failure and Reduced Redundancy (codfw),2023-01-14 08:18:00,2023-01-14 08:19:00,2023-01-14 10:38:00,Sev(2),"A switch (asw-b2-codfw) failure led to a failover to another switch, causing all systems in B2 to go offline. The incident was detected by automated monitoring and responded to by volunteers and SREs.",02:20:00,00:01:00,02:19:00,"swift,thanos,mediawiki,PyBal","Media Storage and File Handling, Monitoring and Logging, MediaWiki Core Services, Caching and Proxy Services","asw-b-codfw,asw-b-codfw,lvs2008,ms-fe-2010,thanos-fe2001,ms-be2046,cp2031,elastic2041,kafka-logging2002,mc2043,thanos-fe2001,elastic2041,cp2031,elastic2041,elastic2041,elastic2041,elastic2041,mc2043,ms-be2046,ml-cache2002,elastic2041,cr1-codfw,cr1-codfw","Networking Equipment, Load Balancers, Web Servers & Application Layers, Caching Servers, Monitoring & Telemetry, Caching Servers, Databases",No user-facing impact; reduced redundancy (and inability to make some changes).,Other,"Switch asw-b2-codfw failed; asw-b-codfw master failed over to b7, leaving all systems in B2 offline.","Hardware Failures, Network Failures",Automated monitoring detected the outage with multiple hosts showing 100% packet loss. Initial DOWN alerts were triggered before the switch failure was detected by monitoring systems.,"Network Connectivity Problems, Service Probes and Health Checks Failures"
181,2019-08-23_network_codfw.wikitext,Provider Outage on Primary Transport Link Between eqiad and codfw,2019-08-23 21:20:00,2019-08-23 21:20:00,2019-08-23 21:55:00,Sev(3),"A provider outage on the primary transport link between eqiad and codfw caused constant flapping, leading to routing re-convergence churn and packet loss, resulting in elevated 5xx errors from Varnish across multiple sites.",00:35:00,00:00:00,00:35:00,"Monitoring,Varnish","Monitoring and Logging, Caching and Proxy Services","Monitoring,Monitoring,Network Components,Network Components","Monitoring & Telemetry, Monitoring & Telemetry, Networking Equipment, Networking Equipment","Surfaced a bit more than 52,000 5xx responses. Many Varnish requests returned 'No backend', and Icinga checks were flapping 'TTL exceeded' and 'No route to host'.","Partial Service Outage, Increased Error Rates, Cache Issues, Network Connectivity Problems","Provider outage on the primary transport link between eqiad and codfw, leading to constant flapping of the link.","Network Failures, External Service Failures",Monitoring reported the issue via SmokePing and Icinga. Icinga displayed host checks flapping between 'TTL exceeded' and 'No route to host'.,Network Connectivity Problems
182,2022-06-12_appserver_latency.wikitext,Incident Report: T310431 Backend Slowness and Unresponsiveness,2022-06-12 08:14:00,2022-06-12 08:14:00,2022-06-12 08:44:00,Sev(2),"For about 30 minutes on June 12, 2022, backends were intermittently slow or partially unresponsive, affecting a portion of logged-in clients and uncached page views.",00:30:00,00:00:00,00:30:00,"Backend Services,Logged-in Client Services,Uncached Page Views",Other,Backend Systems,Other,Intermittent slowness and partial unresponsiveness of backend services affected a portion of logged-in clients and uncached page views for about 30 minutes.,"Partial Service Outage, Elevated Response Times and Latencies, Cache Issues",Detailed root cause analysis is not provided in the given incident report.,Other,Intermittent slowness and partial unresponsiveness of backend services were noticed. Specific alert details are not provided in the given incident report.,"Latency and Timeout Issues, High CPU & Resource Utilization"
183,2022-07-20_network_interruption.wikitext,Network Partition and Service Outage Incident,2022-07-20 03:16:00,2022-07-20 03:16:00,2022-07-20 03:25:00,Sev(2),"A network partition incident occurred when the top-of-rack switch asw2-c-eqiad virtual chassis lost connectivity to FPC5, causing multiple service disruptions and increased error rates.",00:09:00,00:00:00,00:09:00,"varnish-frontend,appservers,Phabricator,Kubernetes API","Caching and Proxy Services, API and Application Servers, Other, Cloud Services and Virtual Private Servers (VPS)","asw2-c-eqiad virtual chassis,FPC5,HAProxy,dbproxy1018,dbproxy1018,dbproxy1018,dbproxy1018,dbproxy1018,kube-apiserver","Networking Equipment, Networking Equipment, Load Balancers, Web Proxies, Web Proxies, Web Proxies, Web Proxies, Web Proxies, Cluster Management","The network partition caused 970,000 external requests to fail, 1.2 million app server requests to fail, a spike in CDN HTTP 5xx errors, Phabricator unavailability for 32 minutes, and partial unavailability of the Kubernetes API for 52 minutes.","Partial Service Outage, Increased Error Rates, Network Connectivity Problems, API Failures","The top-of-rack switch asw2-c-eqiad virtual chassis lost connectivity to FPC5 due to incorrect cabling, leading to network partition and host failures in rack C5.",Network Failures,"The issue was detected through a burst of both paging and non-paging alerts, including Icinga reporting hosts down, BGP status alerts, application-level errors, and MariaDB replica alerts. A user also reported access issues via IRC.","Network Connectivity Problems, Database Errors, User Reported Problems"
184,2021-04-27_Commons_wiki_primary_db.wikitext,Wikimedia Commons Database Contention Incident,2021-04-27 10:29:00,2021-04-27 10:33:00,2021-04-27 11:45:00,Sev(2),"From 10:28 to 11:43, Wikimedia Commons was unavailable and/or slow. Additionally, from 10:41 to 11:10, Commons was in unscheduled read-only mode due to InnoDB contention for read and write queries on the primary s4 database.",01:16:00,00:04:00,01:12:00,"Wikimedia Commons,Wikimedia Commons,Wikimedia Commons",Media Storage and File Handling,"Primary s4 Database,Primary s4 Database,GlobalUsage Extension","Databases, Other","Users were unable to create new accounts, upload files, or edit pages on Wikimedia Commons. Additionally, high latency was experienced across the MediaWiki API.","Partial Service Outage, Elevated Response Times and Latencies","The high contention on InnoDB indexes caused by long-running queries from the GlobalUsage extension was identified, but it is ambiguous whether it was the primary cause or a consequence of another underlying issue.",Database Issues,"The issue was first detected by alerts relating to MySQL replication on s4, including a critical alert about the MariaDB Replica IO and threadpool blockage warnings in the database error log.",Database Errors
185,2021-11-05_TOC_language_converter.wikitext,Table of Contents Language Conversion Incident - November 2021,2021-11-05 20:01:00,2021-11-05 17:56:00,2021-11-06 01:43:00,Sev(3),The Table of Contents (ToC) on multiple language variant wikis displayed inconsistently or disappeared entirely due to train rollback and parser cache issues.,05:42:00,21:55:00,07:47:00,"Wikis (including Chinese Wikipedia,Wikis (including Chinese Wikipedia,Wikis (including Chinese Wikipedia",MediaWiki Core Services,"Contributor,Content,Contributor,Contributor",Other,"For 6 hours, wikis had blank or missing ToCs. For up to 3 days prior, language variant wikis (like Chinese Wikipedia) displayed ToCs in incorrect or inconsistent language variants, making them unreadable for some users.","Partial Service Outage, Data Corruption or Loss","Incorrect handling of Table of Contents by LanguageConverter in parser cache contents across different MediaWiki versions, exacerbated by train rollback.",Software Bugs,The issue was first detected by users of Chinese Wikipedia observing incorrect language conversion in the ToC. There was no automated monitoring.,User Reported Problems
186,2021-06-15_Eqsin_network.wikitext,Connectivity Issues and Recovery at Eqsin Cluster in Singapore,2021-06-15 09:23:00,2021-06-15 09:23:00,2021-06-15 19:00:00,Sev(2),"At 09:23 UTC, alerts indicated connectivity issues to the Eqsin cluster in Singapore, which were resolved by depooling the cluster and redirecting the traffic to other data centers. The issue was fully resolved by 19:00 UTC.",09:37:00,00:00:00,09:37:00,wikis,Other,"Eqsin cluster,Eqsin cluster,Esams,DNS",Other,"For about 35 minutes from 09:20 to 09:45 UTC, the wikis were largely unreachable from countries normally served by the Singapore DC (including India, Hong Kong, and Japan).","Partial Service Outage, Network Connectivity Problems","Connectivity issues to the Eqsin cluster in Singapore, specifics of which were not detailed in the report.",Network Failures,Connectivity issues were first detected as alerts at 09:23 UTC.,Network Connectivity Problems
187,2023-09-29_CloudVPS_vms_losing_network_connectivity.wikitext,Incident Report for Package Cleanup Affecting CloudVPS Services,2023-09-28 11:07:00,2023-09-29 07:06:00,2023-09-29 11:46:00,Sev(3),"A package cleanup caused bullseye VMs in the cloud realm to remove isc-dhcp-client, leading to network connectivity loss for CloudVPS hosted external services.",24:39:00,19:59:00,04:40:00,"cloud-vps,toolforge,toolforge,metricsinfra,metricsinfra","Cloud Services and Virtual Private Servers (VPS), Database and Data Analytics, Monitoring and Logging","VMs,isc-dhcp-client,nfs servers,proxies,monitoring and alerting systems",Other,"Any cloud-vps hosted external service was down (including Toolforge, PAWS, Quarry, and others). Some of the VMs became unreachable through SSH.","Complete Service Outage, Network Connectivity Problems",The removal of isc-dhcp-client packages during a cleanup caused VMs to lose network connectivity as their IP leases expired.,Configuration Errors,The issue was first detected by users reporting connectivity issues on IRC and Phabricator. Admins later noticed an alertmanager down.,"Network Connectivity Problems, User Reported Problems"
188,2021-11-25_eventgate-main_outage.wikitext,2021-11-25 eventgate-main outage,2021-11-25 07:32:00,2021-11-25 07:32:00,2021-11-25 07:35:00,Sev(2),"For about 3 minutes (from 7:32 to 7:35 UTC), eventgate-main was unavailable, resulting in 25,000 unrecoverable MediaWiki backend errors and about 1,000 user-facing web requests and API requests failing with an HTTP 500 error.",00:03:00,00:00:00,00:03:00,"MediaWiki,Eventgate","MediaWiki Core Services, API and Application Servers","backend,API requests,API requests,event intake processing",Other,"For about 3 minutes (from 7:32 to 7:35 UTC), eventgate-main was unavailable. This resulted in 25,000 unrecoverable MediaWiki backend errors due to inability to queue new jobs. About 1,000 user-facing web requests and API requests failed with an HTTP 500 error. Event intake processing rate measured by eventgate briefly dropped from ~3000/second to 0/second during the outage.","Complete Service Outage, Increased Error Rates, API Failures","During the Helm3 migration of the eqiad Kubernetes cluster, the service eventgate-main experienced an outage because it was falsely assumed to be served by the Codfw cluster but was still pooled in the Eqiad cluster. Thus, during the time of removing and re-creating the pods, no traffic could be served for this service.",Configuration Errors,"The issue was first detected through a drop in the event intake processing rate measured by eventgate from ~3000/second to 0/second during the outage. Additionally, MediaWiki backend errors and HTTP 500 errors for web and API requests were observed.","Appserver Issues, Database Errors, Latency and Timeout Issues, User Reported Problems"
189,2019-10-07_wmcs-network.wikitext,WMCS OpenStack Control Plane Upgrade Incident,2019-10-07 15:45:00,2019-10-07 15:50:00,2019-10-07 16:30:00,Sev(2),"During the upgrade of the WMCS OpenStack control plane, all cloud VMs presented with the wrong originating IP for outbound traffic, causing several services to break, including DNS, LDAP, and NFS.",00:45:00,00:05:00,00:40:00,"Infrastructure,Infrastructure,Infrastructure,CI Tools,CI Tools,Infrastructure",Other,"cloudcontrol1003,cloudcontrol1003,cloudnet1003,cloudnet1003,cloudvirt hosts,Horizon,cloud-recursor0,Neutron",Other,"The largest impact was on NFS access from Toolforge, which prevented many grid and k8s jobs from running properly and generated a large volume of alert emails. CI tests also produced incorrect failure messages due to DNS failures. SSH access to most VMs was disrupted for about an hour.","Partial Service Outage, Increased Error Rates, Network Connectivity Problems","Neutron network misconfiguration caused VMs to present with the wrong IP address, leading to service access issues.",Configuration Errors,"The issue was immediately evident due to shinken and icinga alerts, but the team did not respond immediately as some alerts were expected during the upgrade process.",Service Probes and Health Checks Failures
190,2020-03-25_codfw-network.wikitext,Unexpected Loss of Internal Connectivity to codfw Hosts,2023-03-25 11:50:08,2023-03-25 11:50:17,2023-03-25 11:56:54,Sev(3),"Unexpected loss of internal connectivity to codfw hosts and services for 5 minutes caused user-visible failed queries for users with traffic through eqsin and ulsfo edges. The issue was due to maintenance requiring a linecard restart on cr1-codfw, exposing a flaw in codfw's network design.",00:06:46,00:00:09,00:06:37,"Swift,Maps,Restbase API,upload-lb,Kafka mirrormaker","Media Storage and File Handling, API and Application Servers, API and Application Servers, Caching and Proxy Services, Database and Data Analytics","cr1-codfw,VRRP,OSPF,linecard,upload-lb.eqsin.wikimedia.org,upload-lb.eqsin.wikimedia.org,upload-lb.eqsin.wikimedia.org","Networking Equipment, Load Balancers","~28k queries were lost, impacting <1% of global HTTP traffic. Upload-lb requests in ulsfo and eqsin were affected, with ~10% request failures in ulsfo and 1.5% in eqsin. Kafka mirrormaker delays also occurred.","Data Corruption or Loss, Increased Error Rates",Maintenance that required a linecard restart on cr1-codfw exposed a design flaw. The restarted linecard held VRRP mastership and acted as a black hole for routing outside the cluster.,"Network Failures, Configuration Errors",Automated alerts from Icinga for service IPs in codfw and socket timeouts against many hosts were detected. Numerous alert notifications were received in #wikimedia-operations.,"Network Connectivity Problems, Latency and Timeout Issues"
191,2021-09-12_Esams_upload.wikitext,Esams Upload Cache Cluster Outage on 2021-09-12,2021-09-12 18:13:00,2021-09-12 18:13:00,2021-09-12 18:43:00,Sev(3),"On 2021-09-12, the cache upload cluster at esams experienced an outage due to a large image being hotlinked by multiple Romanian news organizations, causing a sudden surge in traffic and subsequent server failures.",00:30:00,00:00:00,00:30:00,upload-lb.esams.wikimedia.org,Media Storage and File Handling,"ATS backend instance,Varnish frontend instances",Caching Servers,"For 20 minutes, images and other media files were unavailable for many clients in countries routed to Esams, with up to 15,000 failed requests at peak. This affected all wikis, resulting in gaps in articles where an image should be.","Partial Service Outage, Increased Error Rates","A large image was hotlinked by multiple Romanian news organizations, causing a sudden surge in traffic to a single ATS backend instance, saturating its 10G NIC and leading to anomalous behavior in Varnish frontend instances.",Unexpected Traffic Surges,"The issue was first detected by an 'ATS TLS has reduced HTTP availability' page, as well as IRC and email notifications.",SSL Certificate Expirations
192,2022-06-21_asw-a2-codfw_accidental_power_cycle.wikitext,Codfw Server Rack Network Connectivity Loss Incident,2022-06-21 14:32:00,2022-06-21 14:32:00,2022-06-21 14:43:00,Sev(2),"One of the Codfw server racks lost network connectivity for 11 minutes during regular maintenance, affecting wiki traffic temporarily due to an automatic switch-over in load balancing.",00:11:00,00:00:00,00:11:00,"wiki traffic,CDN,DNS","Content Delivery Network (CDN) and Edge Cache, Caching and Proxy Services","Codfw server rack,Codfw server rack,ASW network switch,ASW network switch,App servers","Databases, Networking Equipment, Web Servers & Application Layers","Users in regions served by Codfw (Mexico, and parts of US/Canada) experienced a brief increase in latency while the backup LVS host took over.",Elevated Response Times and Latencies,"The second power cable for the ASW network switch was not plugged in properly, causing a full loss of network connectivity for the server rack.",Hardware Failures,Increased latency for internal health check monitoring with alerts from most A2 servers about loss of power redundancy.,"Network Connectivity Problems, Service Probes and Health Checks Failures"
193,2021-07-26_ruwikinews_DynamicPageList.wikitext,DynamicPageList Extension Outage on Wikimedia,2021-07-26 10:30:00,2021-07-26 10:33:00,2021-07-26 10:59:00,Sev(3),"A large bot import to Russian Wikinews caused slow queries from the DynamicPageList extension to overload the s3 database cluster, resulting in php-fpm processes hanging and a site-wide outage. The incident was resolved by disabling the extension on ruwikinews and aggressively killing queries on s3 replicas.",00:29:00,00:03:00,00:26:00,"all wikis,all wikis",Other,"DynamicPageList (DPL) extension,s3 cluster,PHP-FPM workers,MariaDB,MariaDB","Databases, Web Servers & Application Layers","For 30 minutes, 15% of requests from contributors on all wikis were responding either slowly, with an error, or not at all. There were also brief moments during which no readers could load recently modified or uncached pages.","Partial Service Outage, Elevated Response Times and Latencies, Increased Error Rates","The DynamicPageList extension's database queries scaled inefficiently with the categories on ruwikinews, causing the server to execute a full table scan that overloaded the s3 DB cluster.","Database Issues, Capacity Overloads","The issue was first detected by Icinga sending two pages at 10:33 for 'Not enough idle PHP-FPM workers for Mediawiki' on the appserver and api_appserver clusters, followed by an IRC alert from a user and flooding alerts in #wikimedia-operations.","Appserver Issues, User Reported Problems"
194,2022-05-01_etcd.wikitext,TLS Certificate Expiry Impacting Etcd Sync,2023-10-04 04:48:00,2023-10-04 04:48:00,2023-10-04 06:38:00,Sev(2),"The TLS certificate for etcd.eqiad.wmnet expired, causing Conftool to be unable to sync Etcd configuration data between core data centers for 2 hours.",01:50:00,00:00:00,01:50:00,"Conftool,Etcd,Puppet-merge,wikitech/labweb hosts",Other,"etcd main cluster,systemd timers/jobs on labweb hosts,etcd main cluster,cergen,systemd timers/jobs on labweb hosts","Other, Web Servers & Application Layers, Databases, Other, Other","Conftool could not sync Etcd configuration data, which led to data centers getting out of sync, puppet-merge being inoperational, and failed systemd timers on wikitech/labweb hosts. No noticeable impact on public services.","Partial Service Outage, Data Corruption or Loss","The TLS certificate for etcd.eqiad.wmnet expired, and there was complexity in renewing it due to different certificate management methods in the core data centers.",TLS/SSL Issues,"Monitoring alerted us to 'Etcd replication lag,' and wikitech/labweb hosts alerted due to failed timers/jobs.","Database Errors, Service Probes and Health Checks Failures"
195,2020-02-11_caching-proxies.wikitext,CDN Cache Layer Maintenance Outage in Eqiad,2020-02-11 21:02:00,2020-02-11 21:04:00,2020-02-11 21:17:00,Sev(3),"During maintenance on our CDN edge cache layer in eqiad, all caching servers were accidentally depooled due to insufficient safeguards in tooling and errors in following the maintenance procedure, causing a 15-minute outage.",00:15:00,00:02:00,00:13:00,"Wikimedia sites,Wikimedia sites,grafana,logstash,phabricator","Other, Monitoring and Logging, Monitoring and Logging, Other","CDN edge cache layer,cp1075,cp1075,cp1075,cp1075,cp1075,cp1075,cp1075",Caching Servers,"All users of all services behind the eqiad caching layer were affected for about 15 minutes, resulting in the loss of approximately 30k requests per second, totaling around 27 million requests lost, which is nearly 20% of total traffic at the time.","Complete Service Outage, Cache Issues",Insufficient safeguards in tooling and errors in following the maintenance procedure led to the accidental depooling of all caching servers in eqiad.,Configuration Errors,"The issue was detected by both humans and automated monitoring systems (Icinga). Multiple users reported issues on IRC, and Icinga alerted several SREs with critical alerts indicating downtime and socket timeouts.","Latency and Timeout Issues, User Reported Problems, Service Probes and Health Checks Failures"
196,2022-05-09_confctl.wikitext,Codfw Web Traffic Incident - 2022-05-09,2022-05-09 07:44:00,2022-05-09 07:46:00,2022-05-09 07:51:00,Sev(4),"A 5-minute outage occurred due to a misconfigured command that depooled servers in Codfw, causing all web traffic to receive error responses.",00:07:00,00:02:00,00:05:00,"CDN edge cache,Swift media files,Elasticsearch,WDQS","Content Delivery Network (CDN) and Edge Cache, Media Storage and File Handling, Search and Indexing, Wikidata Query Service","appservers,confctl","Web Servers & Application Layers, Cluster Management","For 5 minutes, all web traffic routed to Codfw received error responses, affecting users in Central USA and South America.",Complete Service Outage,"The confctl command was executed with an invalid selection parameter, causing it to depool all servers in Codfw.",Configuration Errors,"The issue was first detected by monitoring systems with 15 alerts and by the engineer executing the change, who noticed the change was affecting more servers than expected.",Service Probes and Health Checks Failures
197,2023-01-24_sessionstore_quorum_issues.wikitext,"Wikipedia Session Storage Outage - January 24, 2023",2023-01-24 20:54:00,2023-01-24 20:57:00,2023-01-24 21:10:00,Sev(3),"Wikipedia's session storage suffered an outage of about 15 minutes (eqiad), causing users to be unable to log in or edit pages.",00:16:00,00:03:00,00:13:00,Wikipedia,Other,"Group1,Group1,Group1,Group1","Caching Servers, Databases, Databases, Databases",Users were unable to log in or edit pages on Wikipedia during the outage.,"Partial Service Outage, Increased Error Rates",Reboot of a Cassandra host (sessionstore1001) led to clients encountering errors and an inability to achieve LOCAL_QUORUM consistency.,"Hardware Failures, Database Issues",Monitoring did not alert; manual page issued when responders noticed a significant drop in successful wiki edits and users started reporting session data loss.,"User Reported Problems, Service Probes and Health Checks Failures"
198,2022-06-16_MariaDB_password_leak.wikitext,Database Password Exposure Incident,2023-10-05 13:00:00,2023-10-05 14:00:00,2023-10-05 15:00:00,Sev(2),"A current production database password was exposed publicly for about 2 hours, but no user-facing impact or data compromise occurred.",02:00:00,01:00:00,01:00:00,labtestwikitech,Cloud Services and Virtual Private Servers (VPS),"database,debugging tools,firewalls,database","Databases, Other, Networking Equipment, Other","No user-facing impact and no data was compromised. The incident broke an important security boundary, but other boundaries prevented data compromise.",Security Incidents,Database credentials were inadvertently exposed through PHP print statements which output them to web clients during troubleshooting.,"Configuration Errors, Security Misconfigurations","The issue was first detected by Sam Reed who noticed the leakage, prompting a quick response and password rotation.",Security Vulnerabilities
